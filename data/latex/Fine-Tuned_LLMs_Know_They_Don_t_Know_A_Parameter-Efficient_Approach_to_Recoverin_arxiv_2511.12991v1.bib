@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}

@article{2021A,
  title={A General Language Assistant as a Laboratory for Alignment},
  author={ Askell, Amanda  and  Bai, Yuntao  and  Chen, Anna  and  Drain, Dawn  and  Ganguli, Deep  and  Henighan, Tom  and  Jones, Andy  and  Joseph, Nicholas  and  Mann, Ben  and  Dassarma, Nova },
  year={2021},
}

@article{li2024survey,
  title={A survey on the honesty of large language models},
  author={Li, Siheng and Yang, Cheng and Wu, Taiqiang and Shi, Chufan and Zhang, Yuji and Zhu, Xinyu and Cheng, Zesen and Cai, Deng and Yu, Mo and Liu, Lemao and others},
  journal={arXiv preprint arXiv:2409.18786},
  year={2024}
}

@article{sun2024trustllm,
  title={Trustllm: Trustworthiness in large language models},
  author={Sun, Lichao and Huang, Yue and Wang, Haoran and Wu, Siyuan and Zhang, Qihui and Gao, Chujie and Huang, Yixin and Lyu, Wenhan and Zhang, Yixuan and Li, Xiner and others},
  journal={arXiv preprint arXiv:2401.05561},
  volume={3},
  year={2024}
}


@article{ren2025mask,
  title={The MASK Benchmark: Disentangling Honesty From Accuracy in AI Systems},
  author={Ren, Richard and Agarwal, Arunim and Mazeika, Mantas and Menghini, Cristina and Vacareanu, Robert and Kenstler, Brad and Yang, Mick and Barrass, Isabelle and Gatti, Alice and Yin, Xuwang and others},
  journal={arXiv preprint arXiv:2503.03750},
  year={2025}
}

@article{gekhman2024does,
  title={Does fine-tuning LLMs on new knowledge encourage hallucinations?},
  author={Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
  journal={arXiv preprint arXiv:2405.05904},
  year={2024}
}

@article{kang2024unfamiliar,
  title={Unfamiliar finetuning examples control how language models hallucinate},
  author={Kang, Katie and Wallace, Eric and Tomlin, Claire and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2403.05612},
  year={2024}
}


@article{zhu2018clinical,
  title={Clinical concept extraction with contextual word embedding},
  author={Zhu, Henghui and Paschalidis, Ioannis Ch and Tahmasebi, Amir},
  journal={arXiv preprint arXiv:1810.10566},
  year={2018}
}

@article{li2019fine,
  title={Fine-tuning bidirectional encoder representations from transformers (BERT)--based models on large-scale electronic health record notes: an empirical study},
  author={Li, Fei and Jin, Yonghao and Liu, Weisong and Rawat, Bhanu Pratap Singh and Cai, Pengshan and Yu, Hong and others},
  journal={JMIR medical informatics},
  volume={7},
  number={3},
  pages={e14830},
  year={2019},
  publisher={JMIR Publications Inc., Toronto, Canada}
}


@article{mai2024fine,
  title={Fine-tuning is fine, if calibrated},
  author={Mai, Zheda and Chowdhury, Arpita and Zhang, Ping and Tu, Cheng-Hao and Chen, Hong-You and Pahuja, Vardaan and Berger-Wolf, Tanya and Gao, Song and Stewart, Charles and Su, Yu and others},
  journal={arXiv preprint arXiv:2409.16223},
  year={2024}
}

@article{askell2021general,
  title={A general language assistant as a laboratory for alignment},
  author={Askell, Amanda and Bai, Yuntao and Chen, Anna and Drain, Dawn and Ganguli, Deep and Henighan, Tom and Jones, Andy and Joseph, Nicholas and Mann, Ben and DasSarma, Nova and others},
  journal={arXiv preprint arXiv:2112.00861},
  year={2021}
}

@inproceedings{
zhang2024dissecting,
title={Dissecting learning and forgetting in language model finetuning},
author={Xiao Zhang and Ji Wu},
booktitle={The Twelfth International Conference on Learning Representations},
year={2024},
url={https://openreview.net/forum?id=tmsqb6WpLz}
}

@article{zhang2023r,
  title={R-tuning: Teaching large language models to refuse unknown questions},
  author={Zhang, Hanning and Diao, Shizhe and Lin, Yong and Fung, Yi R and Lian, Qing and Wang, Xingyao and Chen, Yangyi and Ji, Heng and Zhang, Tong},
  journal={arXiv preprint arXiv:2311.09677},
  year={2023}
}

@inproceedings{10.5555/3692070.3692393,
author = {Cheng, Qinyuan and Sun, Tianxiang and Liu, Xiangyang and Zhang, Wenwei and Yin, Zhangyue and Li, Shimin and Li, Linyang and He, Zhengfu and Chen, Kai and Qiu, Xipeng},
title = {Can AI assistants know what they don't know?},
year = {2024},
booktitle = {Proceedings of the 41st International Conference on Machine Learning},
articleno = {323},
numpages = {19},
}

@inproceedings{NEURIPS2024_9c20f16b,
 author = {Kapoor, Sanyam and Gruver, Nate and Roberts, Manley and Collins, Katherine and Pal, Arka and Bhatt, Umang and Weller, Adrian and Dooley, Samuel and Goldblum, Micah and Wilson, Andrew Gordon},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {A. Globerson and L. Mackey and D. Belgrave and A. Fan and U. Paquet and J. Tomczak and C. Zhang},
 pages = {85932--85972},
 publisher = {Curran Associates, Inc.},
 title = {Large Language Models Must Be Taught to Know What They Don’t Know},
 volume = {37},
 year = {2024}
}

@article{zhu2025grait,
  title={GRAIT: Gradient-Driven Refusal-Aware Instruction Tuning for Effective Hallucination Mitigation},
  author={Zhu, Runchuan and Jiang, Zinco and Wu, Jiang and Ma, Zhipeng and Song, Jiahe and Bai, Fengshuo and Lin, Dahua and Wu, Lijun and He, Conghui},
  journal={arXiv preprint arXiv:2502.05911},
  year={2025}
}

@inproceedings{ulmer-etal-2024-calibrating,
    title = "Calibrating Large Language Models Using Their Generations Only",
    author = "Ulmer, Dennis  and
      Gubri, Martin  and
      Lee, Hwaran  and
      Yun, Sangdoo  and
      Oh, Seong",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.824/",
    doi = "10.18653/v1/2024.acl-long.824",
    pages = "15440--15459",
}

@article{ji2025calibrating,
  title={Calibrating Verbal Uncertainty as a Linear Feature to Reduce Hallucinations},
  author={Ji, Ziwei and Yu, Lei and Koishekenov, Yeskendir and Bang, Yejin and Hartshorn, Anthony and Schelten, Alan and Zhang, Cheng and Fung, Pascale and Cancedda, Nicola},
  journal={arXiv preprint arXiv:2503.14477},
  year={2025}
}

@inproceedings{ghosh2019study,
  title={A study on support vector machine based linear and non-linear pattern classification},
  author={Ghosh, Sourish and Dasgupta, Anasuya and Swetapadma, Aleena},
  booktitle={2019 International conference on intelligent sustainable systems (ICISS)},
  pages={24--28},
  year={2019},
  organization={IEEE}
}




@article{gao2024best,
  title={The best of both worlds: Toward an honest and helpful large language model},
  author={Gao, Chujie and Zhang, Qihui and Chen, Dongping and Huang, Yue and Wu, Siyuan and Fu, Zhengyan and Wan, Yao and Zhang, Xiangliang and Sun, Lichao},
  journal={arXiv preprint arXiv:2406.00380},
  year={2024}
}

@article{hu2023won,
  title={Won't get fooled again: Answering questions with false premises},
  author={Hu, Shengding and Luo, Yifan and Wang, Huadong and Cheng, Xingyi and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.02394},
  year={2023}
}


@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{wan2024knowledge,
  title={Knowledge verification to nip hallucination in the bud},
  author={Wan, Fanqi and Huang, Xinting and Cui, Leyang and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2401.10768},
  year={2024}
}


@inproceedings{zhu2025utilize,
  title={Utilize the flow before stepping into the same river twice: Certainty represented knowledge flow for refusal-aware instruction tuning},
  author={Zhu, Runchuan and Ma, Zhipeng and Wu, Jiang and Gao, Junyuan and Wang, Jiaqi and Lin, Dahua and He, Conghui},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={24},
  pages={26157--26165},
  year={2025}
}

@article{ulmer2024calibrating,
  title={Calibrating large language models using their generations only},
  author={Ulmer, Dennis and Gubri, Martin and Lee, Hwaran and Yun, Sangdoo and Oh, Seong Joon},
  journal={arXiv preprint arXiv:2403.05973},
  year={2024}
}


@article{ji2024llm,
  title={Llm internal states reveal hallucination risk faced with a query},
  author={Ji, Ziwei and Chen, Delong and Ishii, Etsuko and Cahyawijaya, Samuel and Bang, Yejin and Wilie, Bryan and Fung, Pascale},
  journal={arXiv preprint arXiv:2407.03282},
  year={2024}
}

@article{luo2023empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}
@article{lyu2024keeping,
  title={Keeping llms aligned after fine-tuning: The crucial role of prompt templates},
  author={Lyu, Kaifeng and Zhao, Haoyu and Gu, Xinran and Yu, Dingli and Goyal, Anirudh and Arora, Sanjeev},
  journal={arXiv preprint arXiv:2402.18540},
  year={2024}
}


@article{wang2024loss,
  title={On the loss of context-awareness in general instruction fine-tuning},
  author={Wang, Yihan and Bai, Andrew and Peng, Nanyun and Hsieh, Cho-Jui},
  journal={arXiv preprint arXiv:2411.02688},
  year={2024}
}

@article{goyal2024context,
  title={Context-Parametric Inversion: Why Instruction Finetuning May Not Actually Improve Context Reliance},
  author={Goyal, Sachin and Baek, Christina and Kolter, J Zico and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2410.10796},
  year={2024}
}

@article{yang2024alignment,
  title={Alignment for honesty},
  author={Yang, Yuqing and Chern, Ethan and Qiu, Xipeng and Neubig, Graham and Liu, Pengfei},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={63565--63598},
  year={2024}
}

@article{ren2025keeping,
  title={Keeping an eye on llm unlearning: The hidden risk and remedy},
  author={Ren, Jie and Dai, Zhenwei and Tang, Xianfeng and Xing, Yue and Zeng, Shenglai and Liu, Hui and Zeng, Jingying and Peng, Qiankun and Varshney, Samarth and Wang, Suhang and others},
  journal={arXiv preprint arXiv:2506.00359},
  year={2025}
}


@article{xu2025unlearning,
  title={Unlearning Isn't Deletion: Investigating Reversibility of Machine Unlearning in LLMs},
  author={Xu, Xiaoyu and Yue, Xiang and Liu, Yang and Ye, Qingqing and Hu, Haibo and Du, Minxin},
  journal={arXiv preprint arXiv:2505.16831},
  year={2025}
}

@article{barez2501open,
  title={Open problems in machine unlearning for ai safety},
  author={Barez, Fazl and Fu, Tingchen and Prabhu, Ameya and Casper, Stephen and Sanyal, Amartya and Bibi, Adel and O’Gara, Aidan and Kirk, Robert and Bucknall, Ben and Fist, Tim and others},
  journal={URL https://arxiv. org/abs/2501.04952},
  year={2025}
}


@article{lo2024large,
  title={Large language models relearn removed concepts},
  author={Lo, Michelle and Cohen, Shay B and Barez, Fazl},
  journal={arXiv preprint arXiv:2401.01814},
  year={2024}
}

@article{lucki2409adversarial,
  title={An adversarial perspective on machine unlearning for ai safety},
  author={{\L}ucki, Jakub and Wei, Boyi and Huang, Yangsibo and Henderson, Peter and Tramer, Florian and Rando, Javier},
  journal={URL https://arxiv. org/abs/2409.18025},
  year={2024}
}

@article{zhang2024catastrophic,
  title={Catastrophic Failure of LLM Unlearning via Quantization},
  author={Zhang, Zhiwei and Wang, Fali and Li, Xiaomin and Wu, Zongyu and Tang, Xianfeng and Liu, Hui and He, Qi and Yin, Wenpeng and Wang, Suhang},
  journal={arXiv preprint arXiv:2410.16454},
  year={2024}
}

@article{han2024enhancing,
  title={Enhancing Confidence Expression in Large Language Models Through Learning from Past Experience},
  author={Han, Haixia and Li, Tingyun and Chen, Shisong and Shi, Jie and Du, Chengyu and Xiao, Yanghua and Liang, Jiaqing and Lin, Xin},
  journal={arXiv preprint arXiv:2404.10315},
  year={2024}
}

@article{wan2024knowledge,
  title={Knowledge verification to nip hallucination in the bud},
  author={Wan, Fanqi and Huang, Xinting and Cui, Leyang and Quan, Xiaojun and Bi, Wei and Shi, Shuming},
  journal={arXiv preprint arXiv:2401.10768},
  year={2024}
}

@article{cheng2024can,
  title={Can AI assistants know what they don't know?},
  author={Cheng, Qinyuan and Sun, Tianxiang and Liu, Xiangyang and Zhang, Wenwei and Yin, Zhangyue and Li, Shimin and Li, Linyang and He, Zhengfu and Chen, Kai and Qiu, Xipeng},
  journal={arXiv preprint arXiv:2401.13275},
  year={2024}
}

@inproceedings{xu2024sayself,
  title={Sayself: Teaching llms to express confidence with self-reflective rationales},
  author={Xu, Tianyang and Wu, Shujin and Diao, Shizhe and Liu, Xiaoze and Wang, Xingyao and Chen, Yangyi and Gao, Jing},
  booktitle={Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing},
  pages={5985--5998},
  year={2024}
}

@article{ulmer2024calibrating,
  title={Calibrating large language models using their generations only},
  author={Ulmer, Dennis and Gubri, Martin and Lee, Hwaran and Yun, Sangdoo and Oh, Seong Joon},
  journal={arXiv preprint arXiv:2403.05973},
  year={2024}
}


@misc{monosemanticity,
  title        = {Towards monosemanticity: Decomposing language models with dictionary learning},
  author       = {Trenton Bricken and Adly Templeton and Joshua Batson and Brian Chen and Adam Jermyn and Tom Conerly and Nicholas L. Turner and Cem Anil and Carson Denison and Amanda Askell and Robert Lasenby and Yifan Wu and Shauna Kravec and Nicholas Schiefer and Tim Maxwell and Nicholas Joseph and Alex Tamkin and Karina Nguyen and Brayden McLean and Josiah E. Burke and Tristan Hume and Shan Carter and Tom Henighan and Chris Olah},
  howpublished = {\url{https://transformer-circuits.pub/2023/monosemantic-features/index.html}},
  year         = {2023},

}


@article{niu2024does,
  title={What does the Knowledge Neuron Thesis Have to do with Knowledge?},
  author={Niu, Jingcheng and Liu, Andrew and Zhu, Zining and Penn, Gerald},
  journal={arXiv preprint arXiv:2405.02421},
  year={2024}
}

@inproceedings{yi2025nlsr,
  title={Nlsr: Neuron-level safety realignment of large language models against harmful fine-tuning},
  author={Yi, Xin and Zheng, Shunfan and Wang, Linlin and de Melo, Gerard and Wang, Xiaoling and He, Liang},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={24},
  pages={25706--25714},
  year={2025}
}

@article{ferrando2024primer,
  title={A primer on the inner workings of transformer-based language models},
  author={Ferrando, Javier and Sarti, Gabriele and Bisazza, Arianna and Costa-Juss{\`a}, Marta R},
  journal={arXiv preprint arXiv:2405.00208},
  year={2024}
}

@article{dai2021knowledge,
  title={Knowledge neurons in pretrained transformers},
  author={Dai, Damai and Dong, Li and Hao, Yaru and Sui, Zhifang and Chang, Baobao and Wei, Furu},
  journal={arXiv preprint arXiv:2104.08696},
  year={2021}
}

@article{shi2024ircan,
  title={Ircan: Mitigating knowledge conflicts in llm generation via identifying and reweighting context-aware neurons},
  author={Shi, Dan and Jin, Renren and Shen, Tianhao and Dong, Weilong and Wu, Xinwei and Xiong, Deyi},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={4997--5024},
  year={2024}
}

@article{wang2022finding,
  title={Finding skill neurons in pre-trained transformer-based language models},
  author={Wang, Xiaozhi and Wen, Kaiyue and Zhang, Zhengyan and Hou, Lei and Liu, Zhiyuan and Li, Juanzi},
  journal={arXiv preprint arXiv:2211.07349},
  year={2022}
}

@article{stolfo2024confidence,
  title={Confidence regulation neurons in language models},
  author={Stolfo, Alessandro and Wu, Ben and Gurnee, Wes and Belinkov, Yonatan and Song, Xingyi and Sachan, Mrinmaya and Nanda, Neel},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={125019--125049},
  year={2024}
}


@article{cunningham2023sparse,
  title={Sparse autoencoders find highly interpretable features in language models},
  author={Cunningham, Hoagy and Ewart, Aidan and Riggs, Logan and Huben, Robert and Sharkey, Lee},
  journal={arXiv preprint arXiv:2309.08600},
  year={2023}
}


@inproceedings{frantar2023sparsegpt,
  title={Sparsegpt: Massive language models can be accurately pruned in one-shot},
  author={Frantar, Elias and Alistarh, Dan},
  booktitle={International Conference on Machine Learning},
  pages={10323--10337},
  year={2023},
  organization={PMLR}
}

@article{yang2018hotpotqa,
  title={HotpotQA: A dataset for diverse, explainable multi-hop question answering},
  author={Yang, Zhilin and Qi, Peng and Zhang, Saizheng and Bengio, Yoshua and Cohen, William W and Salakhutdinov, Ruslan and Manning, Christopher D},
  journal={arXiv preprint arXiv:1809.09600},
  year={2018}
}

@article{kwiatkowski2019natural,
  title={Natural questions: a benchmark for question answering research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and others},
  journal={Transactions of the Association for Computational Linguistics},
  volume={7},
  pages={453--466},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~…}
}

@inproceedings{pal2022medmcqa,
  title={Medmcqa: A large-scale multi-subject multi-choice dataset for medical domain question answering},
  author={Pal, Ankit and Umapathi, Logesh Kumar and Sankarasubbu, Malaikannan},
  booktitle={Conference on health, inference, and learning},
  pages={248--260},
  year={2022},
  organization={PMLR}
}


@article{krithara2023bioasq,
  title={BioASQ-QA: A manually curated corpus for Biomedical Question Answering},
  author={Krithara, Anastasia and Nentidis, Anastasios and Bougiatiotis, Konstantinos and Paliouras, Georgios},
  journal={Scientific Data},
  volume={10},
  number={1},
  pages={170},
  year={2023},
  publisher={Nature Publishing Group UK London}
}
@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{yin2023large,
  title={Do large language models know what they don't know?},
  author={Yin, Zhangyue and Sun, Qiushi and Guo, Qipeng and Wu, Jiawen and Qiu, Xipeng and Huang, Xuanjing},
  journal={arXiv preprint arXiv:2305.18153},
  year={2023}
}
@article{amayuelas2023knowledge,
  title={Knowledge of knowledge: Exploring known-unknowns uncertainty with large language models},
  author={Amayuelas, Alfonso and Wong, Kyle and Pan, Liangming and Chen, Wenhu and Wang, William},
  journal={arXiv preprint arXiv:2305.13712},
  year={2023}
}

@article{liu2023examining,
  title={Examining LLMs' Uncertainty Expression Towards Questions Outside Parametric Knowledge},
  author={Liu, Genglin and Wang, Xingyao and Yuan, Lifan and Chen, Yangyi and Peng, Hao},
  journal={arXiv preprint arXiv:2311.09731},
  year={2023}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}

@article{hong2024orpo,
  title={Orpo: Monolithic preference optimization without reference model},
  author={Hong, Jiwoo and Lee, Noah and Thorne, James},
  journal={arXiv preprint arXiv:2403.07691},
  year={2024}
}

@article{hu2023won,
  title={Won't get fooled again: Answering questions with false premises},
  author={Hu, Shengding and Luo, Yifan and Wang, Huadong and Cheng, Xingyi and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2307.02394},
  year={2023}
}

@article{zhuang2023toolqa,
  title={Toolqa: A dataset for llm question answering with external tools},
  author={Zhuang, Yuchen and Yu, Yue and Wang, Kuan and Sun, Haotian and Zhang, Chao},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={50117--50143},
  year={2023}
}

@misc{imani2024mathematical,
  title={Mathematical reasoning using large language models},
  author={IMANI, Shima and Shrivastava, Harsh and Du, Liang},
  year={2024},
  month=sep # "~5",
  publisher={Google Patents},
  note={US Patent App. 18/144,802}
}


@article{yang2024drhouse,
  title={Drhouse: An llm-empowered diagnostic reasoning system through harnessing outcomes from sensor data and expert knowledge},
  author={Yang, Bufang and Jiang, Siyang and Xu, Lilin and Liu, Kaiwei and Li, Hai and Xing, Guoliang and Chen, Hongkai and Jiang, Xiaofan and Yan, Zhenyu},
  journal={Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
  volume={8},
  number={4},
  pages={1--29},
  year={2024},
  publisher={ACM New York, NY, USA}
}

@article{gekhman2024does,
  title={Does fine-tuning llms on new knowledge encourage hallucinations?},
  author={Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
  journal={arXiv preprint arXiv:2405.05904},
  year={2024}
}

@article{kang2024unfamiliar,
  title={Unfamiliar finetuning examples control how language models hallucinate},
  author={Kang, Katie and Wallace, Eric and Tomlin, Claire and Kumar, Aviral and Levine, Sergey},
  journal={arXiv preprint arXiv:2403.05612},
  year={2024}
}

@article{zhang2024multi,
  title={A multi-stream network for retrosynthesis prediction},
  author={Zhang, Qiang and Liu, Juan and Zhang, Wen and Yang, Feng and Yang, Zhihui and Zhang, Xiaolei},
  journal={Frontiers of Computer Science},
  volume={18},
  number={2},
  pages={182906},
  year={2024},
  publisher={Springer Nature BV}
}

@article{wei2024dmfvae,
  title={DMFVAE: miRNA-disease associations prediction based on deep matrix factorization method with variational autoencoder},
  author={Wei, Pijing and Wang, Qianqian and Gao, Zhen and Cao, Ruifen and Zheng, Chunhou},
  journal={Frontiers of Computer Science},
  volume={18},
  number={6},
  pages={186912},
  year={2024},
  publisher={Springer}
}

@inproceedings{sarabadani2019detection,
  title={Detection of adverse drug reaction mentions in tweets using ELMo},
  author={Sarabadani, Sarah},
  booktitle={Proceedings of the fourth social media mining for health applications (\# smm4h) workshop \& shared task},
  pages={120--122},
  year={2019}
}


@article{lecun1989optimal,
  title={Optimal brain damage},
  author={LeCun, Yann and Denker, John and Solla, Sara},
  journal={Advances in neural information processing systems},
  volume={2},
  year={1989}
}

@inproceedings{hassibi1993optimal,
  title={Optimal brain surgeon and general network pruning},
  author={Hassibi, Babak and Stork, David G and Wolff, Gregory J},
  booktitle={IEEE international conference on neural networks},
  pages={293--299},
  year={1993},
  organization={IEEE}
}

@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}



@article{kim2025medical,
  title={Medical hallucinations in foundation models and their impact on healthcare},
  author={Kim, Yubin and Jeong, Hyewon and Chen, Shan and Li, Shuyue Stella and Lu, Mingyu and Alhamoud, Kumail and Mun, Jimin and Grau, Cristina and Jung, Minseok and Gameiro, Rodrigo and others},
  journal={arXiv preprint arXiv:2503.05777},
  year={2025}
}

@article{dahl2024large,
  title={Large legal fictions: Profiling legal hallucinations in large language models},
  author={Dahl, Matthew and Magesh, Varun and Suzgun, Mirac and Ho, Daniel E},
  journal={Journal of Legal Analysis},
  volume={16},
  number={1},
  pages={64--93},
  year={2024},
  publisher={Oxford University Press UK}
}




@article{nguyen2025smoothing,
  title={Smoothing Out Hallucinations: Mitigating LLM Hallucination with Smoothed Knowledge Distillation},
  author={Nguyen, Hieu and He, Zihao and Gandre, Shoumik Atul and Pasupulety, Ujjwal and Shivakumar, Sharanya Kumari and Lerman, Kristina},
  journal={arXiv preprint arXiv:2502.11306},
  year={2025}
}

@article{manish2023constitutional,
  title={Constitutional AI: An Expanded Overview of Anthropic’s Alignment Approach},
  author={Manish, Sanwal},
  journal={Information Horizons: American Journal of Library and Information Science Innovation},
  volume={1},
  number={7},
  pages={36--39},
  year={2023},
  publisher={grnjournal}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

% fix there
@article{glaese2022improving,
  title={Improving alignment of dialogue agents via targeted human judgements},
  author={Glaese, Amelia and McAleese, Nat and Tr{\'e}bacz, Maja and Aslanides, John and Firoiu, Vlad and Ewalds, Timo and Rauh, Maribeth and Weidinger, Laura and Chadwick, Martin and Thacker, Phoebe and others},
  journal={arXiv preprint arXiv:2209.14375},
  year={2022}
}