@article{khaki2021deepcorn,
  title={Deepcorn: A Semi-Supervised Deep Learning Method for High-Throughput Image-Based Corn Kernel Counting and Yield Estimation},
  author={Khaki, Saeed and Pham, Hieu and Han, Ye and Kuhl, Andy and Kent, Wade and Wang, Lizhi},
  journal={Knowledge-Based Systems},
  volume={218},
  pages={106874},
  year={2021},
  publisher={Elsevier}
}

@article{li2022maize,
  title={Maize Yield Estimation in Intercropped Smallholder Fields using Satellite Data in Southern Malawi},
  author={Li, Chengxiu and Chimimba, Ellasy Gulule and Kambombe, Oscar and Brown, Luke A and Chibarabada, Tendai Polite and Lu, Yang and Anghileri, Daniela and Ngongondo, Cosmo and Sheffield, Justin and Dash, Jadunandan},
  journal={Remote Sensing},
  volume={14},
  number={10},
  pages={2458},
  year={2022},
  publisher={MDPI}
}

@article{darra2023can,
  title={Can Yield Prediction Be Fully Digitilized? A Systematic Review},
  author={Darra, Nicoleta and Anastasiou, Evangelos and Kriezi, Olga and Lazarou, Erato and Kalivas, Dionissios and Fountas, Spyros},
  journal={Agronomy},
  volume={13},
  number={9},
  pages={2441},
  year={2023},
  publisher={MDPI}
}

@article{zrnic2024cross,
  title={Cross-Prediction-Powered Inference},
  author={Zrnic, Tijana and Cand{\`e}s, Emmanuel J},
  journal={Proceedings of the National Academy of Sciences},
  volume={121},
  number={15},
  pages={e2322083121},
  year={2024},
  publisher={National Academy of Sciences}
}
@misc{ji2025predictions,
      title={Predictions As Surrogates: Revisiting Surrogate Outcomes in the Age of AI}, 
      author={Wenlong Ji and Lihua Lei and Tijana Zrnic},
      year={2025},
      eprint={2501.09731},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2501.09731}, 
}

@article{tibshirani1996regression,
  title={Regression Shrinkage and Selection via the Lasso},
  author={Tibshirani, Robert},
  journal={Journal of the Royal Statistical Society Series B: Statistical Methodology},
  volume={58},
  number={1},
  pages={267--288},
  year={1996},
  publisher={Oxford University Press}
}
@article{friedman2010regularization,
  title={Regularization Paths for Generalized Linear Models via Coordinate Descent},
  author={Friedman, Jerome H and Hastie, Trevor and Tibshirani, Rob},
  journal={Journal of Statistical Software},
  volume={33},
  pages={1--22},
  year={2010}
}
@misc{angelopoulos2023ppi++,
      title={PPI++: Efficient Prediction-Powered Inference}, 
      author={Anastasios N. Angelopoulos and John C. Duchi and Tijana Zrnic},
      year={2024},
      eprint={2311.01453},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2311.01453}, 
}
@misc{zrnic2024note,
      title={A Note on the Prediction-Powered Bootstrap}, 
      author={Tijana Zrnic},
      year={2025},
      eprint={2405.18379},
      archivePrefix={arXiv},
      primaryClass={stat.ML},
      url={https://arxiv.org/abs/2405.18379}, 
}

@misc{vit,
      title={An Image Is Worth 16x16 Words: Transformers for Image Recognition at Scale}, 
      author={Alexey Dosovitskiy and Lucas Beyer and Alexander Kolesnikov and Dirk Weissenborn and Xiaohua Zhai and Thomas Unterthiner and Mostafa Dehghani and Matthias Minderer and Georg Heigold and Sylvain Gelly and Jakob Uszkoreit and Neil Houlsby},
      year={2021},
      eprint={2010.11929},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2010.11929}, 
}
@article{efron1987better,
  title={Better Bootstrap Confidence Intervals},
  author={Efron, Bradley},
  journal={Journal of the American Statistical Association},
  volume={82},
  number={397},
  pages={171--185},
  year={1987},
  publisher={Taylor \& Francis}
}
@article{diciccio1996bootstrap,
  title={Bootstrap Confidence Intervals},
  author={DiCiccio, Thomas J and Efron, Bradley},
  journal={Statistical Science},
  volume={11},
  number={3},
  pages={189--228},
  year={1996},
  publisher={Institute of Mathematical Statistics}
}
@article{robins1994estimation,
  title={Estimation of Regression Coefficients When Some Regressors Are Not Always Observed},
  author={Robins, James M and Rotnitzky, Andrea and Zhao, Lue Ping},
  journal={Journal of the American Statistical Association},
  volume={89},
  number={427},
  pages={846--866},
  year={1994},
  publisher={Taylor \& Francis}
}
@article{angelopoulos2023prediction,
  title={Prediction-Powered Inference},
  author={Angelopoulos, Anastasios N and Bates, Stephen and Fannjiang, Clara and Jordan, Michael I and Zrnic, Tijana},
  journal={Science},
  volume={382},
  number={6671},
  pages={669--674},
  year={2023},
  publisher={American Association for the Advancement of Science}
}
@book{efron2021computer,
  title={Computer Age Statistical Inference, Student Edition: Algorithms, Evidence, and Data Science},
  author={Efron, Bradley and Hastie, Trevor},
  volume={6},
  year={2021},
  publisher={Cambridge University Press}
}
@article{friedman1991multivariate,
  title={Multivariate Adaptive Regression Splines},
  author={Friedman, Jerome H},
  journal={The Annals of Statistics},
  volume={19},
  number={1},
  pages={1--67},
  year={1991},
  publisher={Institute of Mathematical Statistics}
}
@article{breiman2001random,
  title={Random Forests},
  author={Breiman, Leo},
  journal={Machine Learning},
  volume={45},
  pages={5--32},
  year={2001},
  publisher={Springer}
}
@misc{li2024efficient,
      title={Efficient Estimation and Data Fusion Under General Semiparametric Restrictions on Outcome Mean Functions}, 
      author={Harrison H. Li},
      year={2025},
      eprint={2406.06941},
      archivePrefix={arXiv},
      primaryClass={stat.ME},
      url={https://arxiv.org/abs/2406.06941}, 
}

@article{rothe_value_2016,
	author = {Rothe, Christoph},
	file = {10.1515_jci-2022-0019 (1).pdf:/Users/harrisonli/Zotero/storage/GC4A2X49/10.1515_jci-2022-0019 (1).pdf:application/pdf;Rothe - 2016 - The Value of Knowing the Propensity Score for Esti.pdf:/Users/harrisonli/Zotero/storage/DM6WKZBC/Rothe - 2016 - The Value of Knowing the Propensity Score for Esti.pdf:application/pdf},
	journal = {SSRN Electronic Journal},
	language = {en},
	title = {The Value of Knowing the Propensity Score for Estimating Average Treatment Effects},
	url = {https://www.ssrn.com/abstract=2797560},
	urldate = {2023-11-07},
	year = {2016},
	Bdsk-Url-1 = {https://www.ssrn.com/abstract=2797560},
	Bdsk-Url-2 = {https://doi.org/10.2139/ssrn.2797560}}
@article{chernozhukov2018double,
    author = {Chernozhukov, Victor and Chetverikov, Denis and Demirer, Mert and Duflo, Esther and Hansen, Christian and Newey, Whitney and Robins, James},
    title = {Double/Debiased Machine Learning for Treatment and Structural Parameters},
    journal = {The Econometrics Journal},
    volume = {21},
    number = {1},
    pages = {C1-C68},
    year = {2018},
    abstract = {We revisit the classic semi‐parametric problem of inference on a low‐dimensional parameter θ0 in the presence of high‐dimensional nuisance parameters η0. We depart from the classical setting by allowing for η0 to be so high‐dimensional that the traditional assumptions (e.g. Donsker properties) that limit complexity of the parameter space for this object break down. To estimate η0, we consider the use of statistical or machine learning (ML) methods, which are particularly well suited to estimation in modern, very high‐dimensional cases. ML methods perform well by employing regularization to reduce variance and trading off regularization bias with overfitting in practice. However, both regularization bias and overfitting in estimating η0 cause a heavy bias in estimators of θ0 that are obtained by naively plugging ML estimators of η0 into estimating equations for θ0. This bias results in the naive estimator failing to be N−1/2 consistent, where N is the sample size. We show that the impact of regularization bias and overfitting on estimation of the parameter of interest θ0 can be removed by using two simple, yet critical, ingredients: (1) using Neyman‐orthogonal moments/scores that have reduced sensitivity with respect to nuisance parameters to estimate θ0; (2) making use of cross‐fitting, which provides an efficient form of data‐splitting. We call the resulting set of methods double or debiased ML (DML). We verify that DML delivers point estimators that concentrate in an N−1/2‐neighbourhood of the true parameter values and are approximately unbiased and normally distributed, which allows construction of valid confidence statements. The generic statistical theory of DML is elementary and simultaneously relies on only weak theoretical requirements, which will admit the use of a broad array of modern ML methods for estimating the nuisance parameters, such as random forests, lasso, ridge, deep neural nets, boosted trees, and various hybrids and ensembles of these methods. We illustrate the general theory by applying it to provide theoretical properties of the following: DML applied to learn the main regression parameter in a partially linear regression model; DML applied to learn the coefficient on an endogenous variable in a partially linear instrumental variables model; DML applied to learn the average treatment effect and the average treatment effect on the treated under unconfoundedness; DML applied to learn the local average treatment effect in an instrumental variables setting. In addition to these theoretical applications, we also illustrate the use of DML in three empirical examples.},
    eprint = {https://academic.oup.com/ectj/article-pdf/21/1/C1/27684918/ectj00c1.pdf},
}
@inproceedings{benkeser2016highly,
  title={The Highly Adaptive Lasso Estimator},
  author={Benkeser, David and van der Laan, Mark},
  booktitle={2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA)},
  pages={689--696},
  year={2016},
  organization={IEEE}
}
@article{van2017generally,
  title={A Generally Efficient Targeted Minimum Loss Based Estimator Based on the Highly Adaptive Lasso},
  author={van der Laan, Mark},
  journal={The International Journal of Biostatistics},
  volume={13},
  number={2},
  year={2017},
  publisher={De Gruyter}
}
@misc{he2015deepresiduallearningimage,
      title={Deep Residual Learning for Image Recognition}, 
      author={Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun},
      year={2015},
      eprint={1512.03385},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1512.03385}, 
}
@misc{russakovsky2015imagenetlargescalevisual,
      title={ImageNet Large Scale Visual Recognition Challenge}, 
      author={Olga Russakovsky and Jia Deng and Hao Su and Jonathan Krause and Sanjeev Satheesh and Sean Ma and Zhiheng Huang and Andrej Karpathy and Aditya Khosla and Michael Bernstein and Alexander C. Berg and Li Fei-Fei},
      year={2015},
      eprint={1409.0575},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/1409.0575}, 
}
@article{tibshirani2018package,
  title={Package ‘grf’},
  author={Tibshirani, Julie and Athey, Susan and Friedberg, Rina and Hadad, Vitor and Hirshberg, David and Miner, Luke and Sverdrup, Erik and Wager, Stefan and Wright, Marvin and Tibshirani, Maintainer Julie},
  journal={Comprehensive R Archive Network},
  year={2018}
}
@article{lobell2020eyes,
  title={Eyes in the Sky, Boots on the Ground: Assessing Satellite–and Ground-Based Approaches to Crop Yield Measurement and Analysis},
  author={Lobell, David B and Azzari, George and Burke, Marshall and Gourlay, Sydney and Jin, Zhenong and Kilic, Talip and Murray, Siobhan},
  journal={American Journal of Agricultural Economics},
  volume={102},
  number={1},
  pages={202--219},
  year={2020},
  publisher={Wiley Online Library}
}
@article{chen2022debiased,
  title={Debiased Machine Learning Without Sample-Splitting for Stable Estimators},
  author={Chen, Qizhao and Syrgkanis, Vasilis and Austern, Morgane},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3096--3109},
  year={2022}
}
@misc{okasa2022meta,
      title={Meta-Learners for Estimation of Causal Effects: Finite Sample Cross-Fit Performance}, 
      author={Gabriel Okasa},
      year={2022},
      eprint={2201.12692},
      archivePrefix={arXiv},
      primaryClass={econ.EM},
      url={https://arxiv.org/abs/2201.12692}, 
}
