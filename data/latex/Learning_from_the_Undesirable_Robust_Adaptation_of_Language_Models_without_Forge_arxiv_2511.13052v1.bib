@misc{foret2020sharpness,
  title={Sharpness-aware minimization for efficiently improving generalization},
  author={Foret, Pierre and Kleiner, Ariel and Mobahi, Hossein and Neyshabur, Behnam},
  journal={arXiv preprint arXiv:2010.01412},
  year={2020}
}

@article{ghosh2024closer,
  title={A closer look at the limitations of instruction tuning},
  author={Ghosh, Sreyan and Evuru, Chandra Kiran Reddy and Kumar, Sonal and Aneja, Deepali and Jin, Zeyu and Duraiswami, Ramani and Manocha, Dinesh and others},
  journal={arXiv preprint arXiv:2402.05119},
  year={2024}
}

@inproceedings{li2025preserving,
  title={Preserving diversity in supervised fine-tuning of large language models},
  author={Li, Ziniu and Chen, Congliang and Xu, Tian and Qin, Zeyu and Xiao, Jiancong and Luo, Zhi-Quan and Sun, Ruoyu},
  booktitle={ICLR},
  year={2025}
}

@misc{oikonomou2025sharpness,
  title={Sharpness-Aware Minimization: General Analysis and Improved Rates},
  author={Oikonomou, Dimitris and Loizou, Nicolas},
  journal={arXiv preprint arXiv:2503.02225},
  year={2025}
}

@misc{tahmasebi2024universal,
  title={A universal class of sharpness-aware minimization algorithms},
  author={Tahmasebi, Behrooz and Soleymani, Ashkan and Bahri, Dara and Jegelka, Stefanie and Jaillet, Patrick},
  journal={arXiv preprint arXiv:2406.03682},
  year={2024}
}

@article{xie2024sampa,
  title={SAMPa: Sharpness-aware Minimization Parallelized},
  author={Xie, Wanyun and Pethick, Thomas and Cevher, Volkan},
  journal={arXiv preprint arXiv:2410.10683},
  year={2024}
}
@article{andriushchenko2023sharpness,
  title={Sharpness-aware minimization leads to low-rank features},
  author={Andriushchenko, Maksym and Bahri, Dara and Mobahi, Hossein and Flammarion, Nicolas},
  journal={NeurIPS},
  volume={36},
  pages={47032--47051},
  year={2023}
}

@article{wu2020adversarial,
  title={Adversarial weight perturbation helps robust generalization},
  author={Wu, Dongxian and Xia, Shu-Tao and Wang, Yisen},
  journal={NeurIPS},
  volume={33},
  pages={2958--2969},
  year={2020}
}

@inproceedings{zheng2021regularizing,
  title={Regularizing neural networks via adversarial model perturbation},
  author={Zheng, Yaowei and Zhang, Richong and Mao, Yongyi},
  booktitle={CVPR},
  pages={8156--8165},
  year={2021}
}

@article{goodfellow2014explaining,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv preprint arXiv:1412.6572},
  year={2014}
}

@misc{wei2019eda,
  title={Eda: Easy data augmentation techniques for boosting performance on text classification tasks},
  author={Wei, Jason and Zou, Kai},
  journal={arXiv preprint arXiv:1901.11196},
  year={2019}
}

@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@article{shumailov2024ai,
  title={AI models collapse when trained on recursively generated data},
  author={Shumailov, Ilia and Shumaylov, Zakhar and Zhao, Yiren and Papernot, Nicolas and Anderson, Ross and Gal, Yarin},
  journal={Nature},
  volume={631},
  number={8022},
  pages={755--759},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@misc{rogulsky2024effects,
  title={The Effects of Hallucinations in Synthetic Training Data for Relation Extraction},
  author={Rogulsky, Steven and Popovic, Nicholas and F{\"a}rber, Michael},
  journal={arXiv preprint arXiv:2410.08393},
  year={2024}
}

@misc{guo2023curious,
  title={The curious decline of linguistic diversity: Training language models on synthetic text},
  author={Guo, Yanzhu and Shang, Guokan and Vazirgiannis, Michalis and Clavel, Chlo{\'e}},
  journal={arXiv preprint arXiv:2311.09807},
  year={2023}
}

@article{fang2024bias,
  title={Bias of AI-generated content: an examination of news produced by large language models},
  author={Fang, Xiao and Che, Shangkun and Mao, Minjia and Zhang, Hongzhe and others},
  journal={Scientific Reports},
  volume={14},
  number={1},
  pages={5224},
  year={2024},
  publisher={Nature Publishing Group UK London}
}

@misc{jain2023neftune,
  title={Neftune: Noisy embeddings improve instruction finetuning},
  author={Jain, Neel and Chiang, Ping-yeh and Wen, Yuxin and Kirchenbauer, John and Chu, Hong-Min and Somepalli, Gowthami and Bartoldson, Brian R and Kailkhura, Bhavya and Schwarzschild, Avi and Saha, Aniruddha and others},
  journal={arXiv preprint arXiv:2310.05914},
  year={2023}
}

@misc{qi2024safety,
  title={Safety alignment should be made more than just a few tokens deep},
  author={Qi, Xiangyu and Panda, Ashwinee and Lyu, Kaifeng and Ma, Xiao and Roy, Subhrajit and Beirami, Ahmad and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2406.05946},
  year={2024}
}



@article{kang2024latent,
  title={Latent Paraphrasing: Perturbation on Layers Improves Knowledge Injection in Language Models},
  author={Kang, Minki and Hwang, Sung Ju and Lee, Gibbeum and Cho, Jaewoong},
  journal={NeurIPS},
  volume={37},
  pages={119689--119716},
  year={2024}
}   


@article{shi2024instruction,
  title={Instruction tuning with loss over instructions},
  author={Shi, Zhengxiang and Yang, Adam and Wu, Bin and Aitchison, Laurence and Yilmaz, Emine and Lipani, Aldo},
  journal={NeurIPS},
  volume={37},
  pages={69176--69205},
  year={2024}
}


@misc{yadav2023symnoise,
  title={SymNoise: Advancing Language Model Fine-tuning with Symmetric Noise},
  author={Yadav, Abhay Kumar and Singh, Arjun},
  journal={arXiv preprint arXiv:2312.01523},
  year={2023}
}




@misc{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{mann2020language,
  title={Language models are few-shot learners},
  author={Mann, Ben and Ryder, N and Subbiah, M and Kaplan, J and Dhariwal, P and Neelakantan, A and Shyam, P and Sastry, G and Askell, A and Agarwal, S and others},
  journal={arXiv preprint arXiv:2005.14165},
  volume={1},
  pages={3},
  year={2020}
}

@misc{chatgpt,
  author = {OpenAI},
  title = {{Introducing ChatGPT}},
  howpublished = "\url{https://openai.com/blog/chatgpt}",
  year = {2022}
}

@misc{openai2023gpt4,
      title={GPT-4 Technical Report}, 
      author={OpenAI},
      year={2023},
      eprint={2303.08774},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}

@misc{claude,
  author = {Anthropic},
  title = {{Introducing Claude}},
  howpublished = "\url{https://www.anthropic.com/index/introducing-claude}",
  year = {2023}
}

@misc{geminiteam2023gemini,
      title={Gemini: A Family of Highly Capable Multimodal Models}, 
      author={{Gemini Team}},
    journal={arXiv preprint arXiv:2312.11805},
   year={2023}
}

@article{hendryckstest2021,
  title={Measuring Massive Multitask Language Understanding},
  author={Dan Hendrycks and Collin Burns and Steven Basart and Andy Zou and Mantas Mazeika and Dawn Song and Jacob Steinhardt},
  journal={ICLR},
  year={2021}
}

@misc{cobbe2021training,
      title={Training Verifiers to Solve Math Word Problems},
      author={Karl Cobbe and Vineet Kosaraju and Mohammad Bavarian and Jacob Hilton and Reiichiro Nakano and Christopher Hesse and John Schulman},
      year={2021},
      eprint={2110.14168},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@inproceedings{zellers2019hellaswag,
    title={HellaSwag: Can a Machine Really Finish Your Sentence?},
    author={Zellers, Rowan and Holtzman, Ari and Bisk, Yonatan and Farhadi, Ali and Choi, Yejin},
    booktitle ={ACL},
    year={2019}
}

@article{sakaguchi2019winogrande,
    title={WinoGrande: An Adversarial Winograd Schema Challenge at Scale},
    author={Sakaguchi, Keisuke and Bras, Ronan Le and Bhagavatula, Chandra and Choi, Yejin},
    journal={arXiv preprint arXiv:1907.10641},
    year={2019}
}

@inproceedings{lin-etal-2022-truthfulqa,
    title = "{T}ruthful{QA}: Measuring How Models Mimic Human Falsehoods",
    author = "Lin, Stephanie  and
      Hilton, Jacob  and
      Evans, Owain",
    booktitle = "ACL",
    month = may,
    year = "2022",
    address = "Dublin, Ireland",
    url = "https://aclanthology.org/2022.acl-long.229",
    doi = "10.18653/v1/2022.acl-long.229",
    pages = "3214--3252",
}

@article{greenblatt2024alignment,
  title={Alignment faking in large language models},
  author={Greenblatt, Ryan and Denison, Carson and Wright, Benjamin and Roger, Fabien and MacDiarmid, Monte and Marks, Sam and Treutlein, Johannes and Belonax, Tim and Chen, Jack and Duvenaud, David and others},
  journal={arXiv preprint arXiv:2412.14093},
  year={2024}
}

@inproceedings{li2024entropic,
  title={Entropic distribution matching for supervised fine-tuning of LLMs: Less overfitting and better diversity},
  author={Li, Ziniu and Chen, Congliang and Xu, Tian and Qin, Zeyu and Xiao, Jiancong and Sun, Ruoyu and Luo, Zhi-Quan},
  booktitle={NeurIPS 2024 Workshop on Fine-Tuning in Modern Machine Learning: Principles and Scalability},
  year={2024}
}

@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={NeurIPS},
  volume={36},
  pages={53728--53741},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={NeurIPS},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{bai2022training,
  title={Training a helpful and harmless assistant with reinforcement learning from human feedback},
  author={Bai, Yuntao and Jones, Andy and Ndousse, Kamal and Askell, Amanda and Chen, Anna and DasSarma, Nova and Drain, Dawn and Fort, Stanislav and Ganguli, Deep and Henighan, Tom and others},
  journal={arXiv preprint arXiv:2204.05862},
  year={2022}
}

@article{wei2021finetuned,
  title={Finetuned language models are zero-shot learners},
  author={Wei, Jason and Bosma, Maarten and Zhao, Vincent Y and Guu, Kelvin and Yu, Adams Wei and Lester, Brian and Du, Nan and Dai, Andrew M and Le, Quoc V},
  journal={arXiv preprint arXiv:2109.01652},
  year={2021}
}
@inproceedings{finn2017model,
  title={Model-agnostic meta-learning for fast adaptation of deep networks},
  author={Finn, Chelsea and Abbeel, Pieter and Levine, Sergey},
  booktitle={ICML},
  pages={1126--1135},
  year={2017},
  organization={PMLR}
}

@article{rajeswaran2019meta,
  title={Meta-learning with implicit gradients},
  author={Rajeswaran, Aravind and Finn, Chelsea and Kakade, Sham M and Levine, Sergey},
  journal={NeurIPS},
  volume={32},
  year={2019}
}

@misc{luo2023empirical,
  title={An empirical study of catastrophic forgetting in large language models during continual fine-tuning},
  author={Luo, Yun and Yang, Zhen and Meng, Fandong and Li, Yafu and Zhou, Jie and Zhang, Yue},
  journal={arXiv preprint arXiv:2308.08747},
  year={2023}
}

@misc{jiang2025unlocking,
  title={Unlocking the power of function vectors for characterizing and mitigating catastrophic forgetting in continual instruction tuning},
  author={Jiang, Gangwei and Jiang, Caigao and Li, Zhaoyi and Xue, Siqiao and Zhou, Jun and Song, Linqi and Lian, Defu and Wei, Yin},
  journal={arXiv preprint arXiv:2502.11019},
  year={2025}
}

@misc{qi2023fine,
  title={Fine-tuning aligned language models compromises safety, even when users do not intend to!},
  author={Qi, Xiangyu and Zeng, Yi and Xie, Tinghao and Chen, Pin-Yu and Jia, Ruoxi and Mittal, Prateek and Henderson, Peter},
  journal={arXiv preprint arXiv:2310.03693},
  year={2023}
}

@article{xu2023wizardlm,
  title={Wizardlm: Empowering large language models to follow complex instructions},
  author={Xu, Can and Sun, Qingfeng and Zheng, Kai and Geng, Xiubo and Zhao, Pu and Feng, Jiazhan and Tao, Chongyang and Jiang, Daxin},
  journal={arXiv preprint arXiv:2304.12244},
  year={2023}
}


@article{xue2023repeat,
  title={To repeat or not to repeat: Insights from scaling llm under token-crisis},
  author={Xue, Fuzhao and Fu, Yao and Zhou, Wangchunshu and Zheng, Zangwei and You, Yang},
  journal={NeurIPS},
  volume={36},
  pages={59304--59322},
  year={2023}
}

@article{yang2023bayesian,
  title={Bayesian low-rank adaptation for large language models},
  author={Yang, Adam X and Robeyns, Maxime and Wang, Xi and Aitchison, Laurence},
  journal={arXiv preprint arXiv:2308.13111},
  year={2023}
}

@misc{howard2023singleexample,
  author       = {Jeremy Howard and Jonathan Whitaker},
  title        = {Can LLMs Learn from a Single Example?},
  year         = {2023},
  month        = {September},
  url          = {https://www.fast.ai/posts/2023-09-04-learning-jumps/},
  note         = {fast.ai Technical Report}
}

@misc{gekhman2024does,
  title={Does fine-tuning LLMs on new knowledge encourage hallucinations?},
  author={Gekhman, Zorik and Yona, Gal and Aharoni, Roee and Eyal, Matan and Feder, Amir and Reichart, Roi and Herzig, Jonathan},
  journal={arXiv preprint arXiv:2405.05904},
  year={2024}
}

@misc{lin2024flame,
  title={Flame: Factuality-aware alignment for large language models},
  author={Lin, Sheng-Chieh and Gao, Luyu and Oguz, Barlas and Xiong, Wenhan and Lin, Jimmy and Yih, Wen-tau and Chen, Xilun},
  journal={arXiv preprint arXiv:2405.01525},
  year={2024}
}

@article{huang2025survey,
  title={A survey on hallucination in large language models: Principles, taxonomy, challenges, and open questions},
  author={Huang, Lei and Yu, Weijiang and Ma, Weitao and Zhong, Weihong and Feng, Zhangyin and Wang, Haotian and Chen, Qianglong and Peng, Weihua and Feng, Xiaocheng and Qin, Bing and others},
  journal={ACM Transactions on Information Systems},
  volume={43},
  number={2},
  pages={1--55},
  year={2025},
  publisher={ACM New York, NY}
}

@article{adler2024nemotron,
  title={Nemotron-4 340b technical report},
  author={Adler, Bo and Agarwal, Niket and Aithal, Ashwath and Anh, Dong H and Bhattacharya, Pallab and Brundyn, Annika and Casper, Jared and Catanzaro, Bryan and Clay, Sharon and Cohen, Jonathan and others},
  journal={arXiv preprint arXiv:2406.11704},
  year={2024}
}

@article{snell2024scaling,
  title={Scaling llm test-time compute optimally can be more effective than scaling model parameters},
  author={Snell, Charlie and Lee, Jaehoon and Xu, Kelvin and Kumar, Aviral},
  journal={arXiv preprint arXiv:2408.03314},
  year={2024}
}

@misc{alpaca,
  author = {Rohan Taori and Ishaan Gulrajani and Tianyi Zhang and Yann Dubois and Xuechen Li and Carlos Guestrin and Percy Liang and Tatsunori B. Hashimoto },
  title = {Stanford Alpaca: An Instruction-following LLaMA model},
  year = {2023},
  publisher = {GitHub},
  journal = {GitHub repository},
  howpublished = {\url{https://github.com/tatsu-lab/stanford_alpaca}},
}

@article{sajjadi2016regularization,
  title={Regularization with stochastic transformations and perturbations for deep semi-supervised learning},
  author={Sajjadi, Mehdi and Javanmardi, Mehran and Tasdizen, Tolga},
  journal={NeurIPS},
  volume={29},
  year={2016}
}

@misc{inan2023llama,
  title={Llama guard: Llm-based input-output safeguard for human-ai conversations},
  author={Inan, Hakan and Upasani, Kartikeya and Chi, Jianfeng and Rungta, Rashi and Iyer, Krithika and Mao, Yuning and Tontchev, Michael and Hu, Qing and Fuller, Brian and Testuggine, Davide and others},
  journal={arXiv preprint arXiv:2312.06674},
  year={2023}
}

@misc{metallamaguard2,
  author =       {Llama Team},
  title =        {Meta Llama Guard 2},
  howpublished = {\url{https://github.com/meta-llama/PurpleLlama/blob/main/Llama-Guard2/MODEL_CARD.md}},
  year =         {2024}
}

@misc{dubey2024llama3herdmodels,
  title =         {The Llama 3 Herd of Models},
  author =        {Llama Team, AI @ Meta},
  year =          {2024},
  eprint =        {2407.21783},
  archivePrefix = {arXiv},
  primaryClass =  {cs.AI},
  url =           {https://arxiv.org/abs/2407.21783}
}


@article{han2024wildguard,
  title={Wildguard: Open one-stop moderation tools for safety risks, jailbreaks, and refusals of llms},
  author={Han, Seungju and Rao, Kavel and Ettinger, Allyson and Jiang, Liwei and Lin, Bill Yuchen and Lambert, Nathan and Choi, Yejin and Dziri, Nouha},
  journal={arXiv preprint arXiv:2406.18495},
  year={2024}
}

@article{ghosh2025aegis2,
  title={AEGIS2. 0: A Diverse AI Safety Dataset and Risks Taxonomy for Alignment of LLM Guardrails},
  author={Ghosh, Shaona and Varshney, Prasoon and Sreedhar, Makesh Narsimhan and Padmakumar, Aishwarya and Rebedea, Traian and Varghese, Jibin Rajan and Parisien, Christopher},
  journal={arXiv preprint arXiv:2501.09004},
  year={2025}
}

@article{zeng2024shieldgemma,
  title={Shieldgemma: Generative ai content moderation based on gemma},
  author={Zeng, Wenjun and Liu, Yuchi and Mullins, Ryan and Peran, Ludovic and Fernandez, Joe and Harkous, Hamza and Narasimhan, Karthik and Proud, Drew and Kumar, Piyush and Radharapu, Bhaktipriya and others},
  journal={arXiv preprint arXiv:2407.21772},
  year={2024}
}

@article{liu2025guardreasoner,
  title={GuardReasoner: Towards Reasoning-based LLM Safeguards},
  author={Liu, Yue and Gao, Hongcheng and Zhai, Shengfang and Xia, Jun and Wu, Tianyi and Xue, Zhiwei and Chen, Yulin and Kawaguchi, Kenji and Zhang, Jiaheng and Hooi, Bryan},
  journal={arXiv preprint arXiv:2501.18492},
  year={2025}
}

@inproceedings{markov2023holistic,
  title={A holistic approach to undesired content detection in the real world},
  author={Markov, Todor and Zhang, Chong and Agarwal, Sandhini and Nekoul, Florentine Eloundou and Lee, Theodore and Adler, Steven and Jiang, Angela and Weng, Lilian},
  booktitle={AAAI},
  volume={37},
  number={12},
  pages={15009--15018},
  year={2023}
}

@article{lee2024harmaug,
  title={HarmAug: Effective Data Augmentation for Knowledge Distillation of Safety Guard Models},
  author={Lee, Seanie and Seong, Haebin and Lee, Dong Bok and Kang, Minki and Chen, Xiaoyin and Wagner, Dominik and Bengio, Yoshua and Lee, Juho and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2410.01524},
  year={2024}
}


@misc{yang2024self,
  title={Self-distillation bridges distribution gap in language model fine-tuning},
  author={Yang, Zhaorui and Pang, Tianyu and Feng, Haozhe and Wang, Han and Chen, Wei and Zhu, Minfeng and Liu, Qian},
  journal={arXiv preprint arXiv:2402.13669},
  year={2024}
}

@article{gupta2025selective,
  title={Selective Self-to-Supervised Fine-Tuning for Generalization in Large Language Models},
  author={Gupta, Sonam and Nandwani, Yatin and Yehudai, Asaf and Khandelwal, Dinesh and Raghu, Dinesh and Joshi, Sachindra},
  journal={arXiv preprint arXiv:2502.08130},
  year={2025}
}

@misc{roziere2023code,
  title={Code llama: Open foundation models for code},
  author={Roziere, Baptiste and Gehring, Jonas and Gloeckle, Fabian and Sootla, Sten and Gat, Itai and Tan, Xiaoqing Ellen and Adi, Yossi and Liu, Jingyu and Sauvestre, Romain and Remez, Tal and others},
  journal={arXiv preprint arXiv:2308.12950},
  year={2023}
}

@article{luo2023wizardcoder,
  title={Wizardcoder: Empowering code large language models with evol-instruct},
  author={Luo, Ziyang and Xu, Can and Zhao, Pu and Sun, Qingfeng and Geng, Xiubo and Hu, Wenxiang and Tao, Chongyang and Ma, Jing and Lin, Qingwei and Jiang, Daxin},
  journal={arXiv preprint arXiv:2306.08568},
  year={2023}
}

@article{jin2024genegpt,
  title={Genegpt: Augmenting large language models with domain tools for improved access to biomedical information},
  author={Jin, Qiao and Yang, Yifan and Chen, Qingyu and Lu, Zhiyong},
  journal={Bioinformatics},
  volume={40},
  number={2},
  pages={btae075},
  year={2024},
  publisher={Oxford University Press}
}
@article{wu2023bloomberggpt,
  title={Bloomberggpt: A large language model for finance},
  author={Wu, Shijie and Irsoy, Ozan and Lu, Steven and Dabravolski, Vadim and Dredze, Mark and Gehrmann, Sebastian and Kambadur, Prabhanjan and Rosenberg, David and Mann, Gideon},
  journal={arXiv preprint arXiv:2303.17564},
  year={2023}
}

@article{huang2025virus,
  title={Virus: Harmful Fine-tuning Attack for Large Language Models Bypassing Guardrail Moderation},
  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
  journal={arXiv preprint arXiv:2501.17433},
  year={2025}
}

@article{patil2024gorilla,
  title={Gorilla: Large language model connected with massive apis},
  author={Patil, Shishir G and Zhang, Tianjun and Wang, Xin and Gonzalez, Joseph E},
  journal={NeurIPS},
  volume={37},
  pages={126544--126565},
  year={2024}
}

@article{chen2021evaluating,
  title={Evaluating large language models trained on code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde De Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}


@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive {NLP} tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and others},
  journal={NeurIPS},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{zou2310representation,
  title={Representation engineering: A top-down approach to ai transparency, 2023},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={URL https://arxiv. org/abs/2310.01405}
}

@inproceedings{liu2025reducing,
  title={Reducing hallucinations in large vision-language models via latent space steering},
  author={Liu, Sheng and Ye, Haotian and Zou, James},
  booktitle={ICLR},
  year={2025}
}

@article{liu2311context,
  title={In-context vectors: Making in context learning more effective and controllable through latent space steering, 2024},
  author={Liu, Sheng and Ye, Haotian and Xing, Lei and Zou, James},
  journal={URL https://arxiv. org/abs/2311.06668}
}

@inproceedings{cao2025scans,
  title={SCANS: Mitigating the Exaggerated Safety for {LLM}s via Safety-Conscious Activation Steering},
  author={Cao, Zouying and Yang, Yifei and Zhao, Hai},
  booktitle={AAAI},
  volume={39},
  number={22},
  pages={23523--23531},
  year={2025}
}

@inproceedings{luo2024pace,
  title={Pace: Parsimonious concept engineering for large language models},
  author={Luo, Jinqi and Ding, Tianjiao and Chan, Kwan Ho Ryan and Thaker, Darshan and Chattopadhyay, Aditya and Callison-Burch, Chris and Vidal, Ren{\'e}},
  booktitle={NeurIPS},
  year={2024}
}


@inproceedings{reynolds2021prompt,
  title={Prompt programming for large language models: Beyond the few-shot paradigm},
  author={Reynolds, Laria and McDonell, Kyle},
  booktitle={CHI},
  pages={1--7},
  year={2021}
}

@misc{zhu2023promptbench,
  title={Promptbench: Towards evaluating the robustness of large language models on adversarial prompts},
  author={Zhu, Kaijie and Wang, Jindong and Zhou, Jiaheng and Wang, Zichen and Chen, Hao and others},
  journal={arXiv e-prints},
  pages={arXiv--2306},
  year={2023}
}

@article{wei2025paft,
  title={PAFT: Prompt-Agnostic Fine-Tuning},
  author={Wei, Chenxing and Shu, Yao and Ou, Mingwen and He, Ying Tiffany and Yu, Fei Richard},
  journal={arXiv preprint arXiv:2502.12859},
  year={2025}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{gemma_2025,
    title={Gemma 3},
    url={https://goo.gle/Gemma3Report},
    publisher={Kaggle},
    author={Gemma Team},
    year={2025}
}

@misc{qwen3technicalreport,
      title={Qwen3 Technical Report}, 
      author={{Qwen Team}},
      year={2025},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2505.09388}, 
}

@misc{amini2019mathqa,
    title={MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms},
    author={Aida Amini and Saadia Gabriel and Peter Lin and Rik Koncel-Kedziorski and others},
    year={2019}
}

@misc{miao2021diverse,
    title={A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers},
    author={Shen-Yun Miao and Chao-Chun Liang and Keh-Yih Su},
    year={2021},
    eprint={2106.15772},
    archivePrefix={arXiv},
    primaryClass={cs.AI}
}


@article{chen2021codex,
  title={Evaluating Large Language Models Trained on Code},
  author={Mark Chen and Jerry Tworek and Heewoo Jun and Qiming Yuan and Henrique Ponde de Oliveira Pinto and Jared Kaplan and Harri Edwards and Yuri Burda and Nicholas Joseph and Greg Brockman and Alex Ray and Raul Puri and Gretchen Krueger and Michael Petrov and Heidy Khlaaf and Girish Sastry and Pamela Mishkin and Brooke Chan and Scott Gray and Nick Ryder and Mikhail Pavlov and Alethea Power and Lukasz Kaiser and Mohammad Bavarian and Clemens Winter and Philippe Tillet and Felipe Petroski Such and Dave Cummings and Matthias Plappert and Fotios Chantzis and Elizabeth Barnes and Ariel Herbert-Voss and William Hebgen Guss and Alex Nichol and Alex Paino and Nikolas Tezak and Jie Tang and Igor Babuschkin and Suchir Balaji and Shantanu Jain and William Saunders and Christopher Hesse and Andrew N. Carr and Jan Leike and Josh Achiam and Vedant Misra and Evan Morikawa and Alec Radford and Matthew Knight and Miles Brundage and Mira Murati and Katie Mayer and Peter Welinder and Bob McGrew and Dario Amodei and Sam McCandlish and Ilya Sutskever and Wojciech Zaremba},
  year={2021},
  eprint={2107.03374},
  archivePrefix={arXiv},
  primaryClass={cs.LG}
}


@inproceedings{Bisk2020,
    author = {Yonatan Bisk and Rowan Zellers and
            Ronan Le Bras and Jianfeng Gao
            and Yejin Choi},
    title = {PIQA: Reasoning about Physical Commonsense in
           Natural Language},
    booktitle = {AAAI},
    year = {2020},
}


@inproceedings{OpenBookQA2018,
    title={Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question Answering},
    author={Todor Mihaylov and Peter Clark and Tushar Khot and Ashish Sabharwal},
    booktitle={EMNLP},
    year={2018}
}


@misc{paperno2016lambada,
  title={The LAMBADA dataset: Word prediction requiring a broad discourse context},
  author={Paperno, Denis and Kruszewski, Germ{\'a}n and Lazaridou, Angeliki and Pham, Quan Ngoc and Bernardi, Raffaella and others},
  journal={arXiv preprint arXiv:1606.06031},
  year={2016}
}

@misc{Clark2018ThinkYH,
  title={Think you have solved question answering? try arc, the ai2 reasoning challenge},
  author={Clark, Peter and Cowhey, Isaac and Etzioni, Oren and Khot, Tushar and Sabharwal, Ashish and Schoenick, Carissa and Tafjord, Oyvind},
  journal={arXiv preprint arXiv:1803.05457},
  year={2018}
}

@article{reddy2019coqa,
  title={Coqa: A conversational question answering challenge},
  author={Reddy, Siva and Chen, Danqi and Manning, Christopher D},
  journal={TACL},
  volume={7},
  pages={249--266},
  year={2019},
  publisher={MIT Press One Rogers Street, Cambridge, MA 02142-1209, USA journals-info~â€¦}
}



@inproceedings{hartvigsen2022toxigen,
  title={ToxiGen: A Large-Scale Machine-Generated Dataset for Implicit and Adversarial Hate Speech Detection},
  author={Hartvigsen, Thomas and Gabriel, Saadia and Palangi, Hamid and Sap, Maarten and Ray, Dipankar and Kamar, Ece},
  booktitle={ACL},
  year={2022}
}


@article{zhou2023lima,
  title={Lima: Less is more for alignment},
  author={Zhou, Chunting and Liu, Pengfei and Xu, Puxin and Iyer, Srinivasan and Sun, Jiao and Mao, Yuning and Ma, Xuezhe and Efrat, Avia and Yu, Ping and Yu, Lili and others},
  journal={NeurIPS},
  volume={36},
  pages={55006--55021},
  year={2023}
}

@misc{DatabricksBlog2023DollyV2,
    author    = {Mike Conover and Matt Hayes and Ankit Mathur and Jianwei Xie and Jun Wan and Sam Shah and others},
    title     = {Free Dolly: Introducing the World's First Truly Open Instruction-Tuned {LLM}},
    year      = {2023},
    url       = {https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm},
    urldate   = {2023-06-30}
}


@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}



@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@misc{kotha2023understanding,
  title={Understanding catastrophic forgetting in language models via implicit inference},
  author={Kotha, Suhas and Springer, Jacob Mitchell and Raghunathan, Aditi},
  journal={arXiv preprint arXiv:2309.10105},
  year={2023}
}

@article{tamirisa2024tamper,
  title={Tamper-resistant safeguards for open-weight llms},
  author={Tamirisa, Rishub and Bharathi, Bhrugu and Phan, Long and Zhou, Andy and Gatti, Alice and Suresh, Tarun and Lin, Maxwell and Wang, Justin and Wang, Rowan and Arel, Ron and others},
  journal={arXiv preprint arXiv:2408.00761},
  year={2024}
}


@misc{huang2024booster,
  title={Booster: Tackling Harmful Fine-tuning for Large Language Models via Attenuating Harmful Perturbation},
  author={Huang, Tiansheng and Hu, Sihao and Ilhan, Fatih and Tekin, Selim Furkan and Liu, Ling},
  journal={arXiv preprint arXiv:2409.01586},
  year={2024}
}

@incollection{thrun1998learning,
  title={Learning to learn: Introduction and overview},
  author={Thrun, Sebastian and Pratt, Lorien},
  booktitle={Learning to learn},
  pages={3--17},
  year={1998},
  publisher={Springer}
}

@misc{li2017meta,
  title={Meta-sgd: Learning to learn quickly for few-shot learning},
  author={Li, Zhenguo and Zhou, Fengwei and Chen, Fei and Li, Hang},
  journal={arXiv preprint arXiv:1707.09835},
  year={2017}
}

@inproceedings{tang2023consistency,
  title={Consistency regularization for generalizable source-free domain adaptation},
  author={Tang, Longxiang and Li, Kai and He, Chunming and Zhang, Yulun and Li, Xiu},
  booktitle={ICCV workshop},
  pages={4323--4333},
  year={2023}
}

@article{xie2020unsupervised,
  title={Unsupervised data augmentation for consistency training},
  author={Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Thang and Le, Quoc},
  journal={NeurIPS},
  volume={33},
  pages={6256--6268},
  year={2020}
}

@article{sohn2020fixmatch,
  title={Fixmatch: Simplifying semi-supervised learning with consistency and confidence},
  author={Sohn, Kihyuk and Berthelot, David and Carlini, Nicholas and Zhang, Zizhao and Zhang, Han and others},
  journal={NeurIPS},
  volume={33},
  pages={596--608},
  year={2020}
}

@inproceedings{ni2025noise,
  title={Noise Consistency Regularization for Improved Subject-Driven Image Synthesis},
  author={Ni, Yao and Wen, Song and Koniusz, Piotr and Cherian, Anoop},
  booktitle={CVPR Workshop},
  pages={3116--3126},
  year={2025}
}

@article{wang2022selfcon,
  title={Self-consistency improves chain of thought reasoning in language models},
  author={Wang, Xuezhi and Wei, Jason and Schuurmans, Dale and Le, Quoc and Chi, Ed and Narang, Sharan and Chowdhery, Aakanksha and Zhou, Denny},
  journal={arXiv preprint arXiv:2203.11171},
  year={2022}
}

@misc{ba2024fill,
  title={Fill in the gaps: Model calibration and generalization with synthetic data},
  author={Ba, Yang and Mancenido, Michelle V and Pan, Rong},
  journal={arXiv preprint arXiv:2410.10864},
  year={2024}
}
@inproceedings{wang2025debiased,
  title={Debiased distillation for consistency regularization},
  author={Wang, Lu and Xu, Liuchi and Yang, Xiong and Huang, Zhenhua and Cheng, Jun},
  booktitle={AAAI},
  volume={39},
  pages={7799--7807},
  year={2025}
}

@article{feder2023data,
  title={Data augmentations for improved (large) language model generalization},
  author={Feder, Amir and Wald, Yoav and Shi, Claudia and Saria, Suchi and Blei, David},
  journal={NeurIPS},
  volume={36},
  pages={70638--70653},
  year={2023}
}

@misc{chen2023alpagasus,
  title={Alpagasus: Training a better alpaca with fewer data},
  author={Chen, Lichang and Li, Shiyang and Yan, Jun and Wang, Hai and Gunaratna, Kalpa and Yadav, Vikas and Tang, Zheng and Srinivasan, Vijay and Zhou, Tianyi and Huang, Heng and others},
  journal={arXiv preprint arXiv:2307.08701},
  year={2023}
}

@article{ji2023beavertails,
  title={Beavertails: Towards improved safety alignment of llm via a human-preference dataset},
  author={Ji, Jiaming and Liu, Mickel and Dai, Josef and Pan, Xuehai and Zhang, Chi and Bian, Ce and Chen, Boyuan and Sun, Ruiyang and Wang, Yizhou and Yang, Yaodong},
  journal={NeurIPS},
  volume={36},
  pages={24678--24704},
  year={2023}
}

@misc{zou2023universal,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{zou2023representation,
  title={Representation engineering: A top-down approach to ai transparency},
  author={Zou, Andy and Phan, Long and Chen, Sarah and Campbell, James and Guo, Phillip and Ren, Richard and Pan, Alexander and Yin, Xuwang and Mazeika, Mantas and Dombrowski, Ann-Kathrin and others},
  journal={arXiv preprint arXiv:2310.01405},
  year={2023}
}

@misc{openai2024gpt4ocard,
      title={G{PT}-4o System Card}, 
      author={OpenAI},
      year={2024},
      eprint={2410.21276},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2410.21276}, 
}


@inproceedings{zhang2020Consistency,
title={Consistency Regularization for Generative Adversarial Networks},
author={Han Zhang and Zizhao Zhang and Augustus Odena and Honglak Lee},
booktitle={ICLR},
year={2020},
url={https://openreview.net/forum?id=S1lxKlSKPH}
}

@article{jeong2020consistency,
  title={Consistency regularization for certified robustness of smoothed classifiers},
  author={Jeong, Jongheon and Shin, Jinwoo},
  journal={NeurIPS},
  volume={33},
  pages={10558--10570},
  year={2020}
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={ICML},
  pages={7472--7482},
  year={2019}
}

@inproceedings{tack2022consistency,
  title={Consistency regularization for adversarial robustness},
  author={Tack, Jihoon and Yu, Sihyun and Jeong, Jongheon and Kim, Minseon and Hwang, Sung Ju and Shin, Jinwoo},
  booktitle={AAAI},
  volume={36},
  pages={8414--8422},
  year={2022}
}

@misc{kirk2023understanding,
  title={Understanding the effects of RLHF on LLM generalisation and diversity},
  author={Kirk, Robert and Mediratta, Ishita and Nalmpantis, Christoforos and Luketina, Jelena and Hambro, Eric and others},
  journal={arXiv preprint arXiv:2310.06452},
  year={2023}
}