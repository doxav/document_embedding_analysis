@book{angelopoulos_conformal_2023-5,
	title = {Conformal {Prediction}: {A} {Gentle} {Introduction}},
	volume = {16},
	shorttitle = {Conformal {Prediction}},
	url = {https://www.nowpublishers.com/article/Details/MAL-101},
	abstract = {Conformal Prediction: A Gentle Introduction},
	language = {English},
	urldate = {2024-01-29},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen},
	month = mar,
	year = {2023},
	note = {Publisher: Now Publishers, Inc.},
}

@inproceedings{angelopoulos_conformal_2023-1,
	title = {Conformal {Risk} {Control}},
	url = {http://arxiv.org/abs/2208.02814},
	abstract = {We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an \${\textbackslash}mathcal\{O\}(1/n)\$ factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.},
	urldate = {2023-10-16},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
	month = apr,
	year = {2023},
	note = {arXiv:2208.02814 [cs, math, stat]
bibtex: angelopoulos\_conformal\_2023\_misc},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{malgorzewicz_intro--cp-for-time-series-forecasting_2025,
	title = {intro-to-cp-for-time-series-forecasting},
	url = {https://github.com/sacixr/intro-to-cp-for-time-series-forecasting},
	abstract = {An adjusted version of Stocker, Massimo's original .ipynb, with the addition of a GARCH(1,1) model and additional error bar plots.},
	urldate = {2025-11-13},
	author = {Malgorzewicz, Wiktoria},
	month = nov,
	year = {2025},
	note = {original-date: 2025-11-13T15:03:01Z},
}

@misc{noauthor_conformal_nodate,
	title = {Conformal {Forecasting} {Review}},
	url = {https://www.overleaf.com/project/67ab586cf692b59a19740533},
	abstract = {An online LaTeX editor that’s easy to use. No installation, real-time collaboration, version control, hundreds of LaTeX templates, and more.},
	language = {en},
	urldate = {2025-11-16},
}

@article{bollerslev_generalized_1986,
	title = {Generalized autoregressive conditional heteroskedasticity},
	volume = {31},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/0304407686900631},
	doi = {10.1016/0304-4076(86)90063-1},
	abstract = {A natural generalization of the ARCH (Autoregressive Conditional Heteroskedastic) process introduced in Engle (1982) to allow for past conditional variances in the current conditional variance equation is proposed. Stationarity conditions and autocorrelation structure for this new class of parametric models are derived. Maximum likelihood estimation and testing are also considered. Finally an empirical example relating to the uncertainty of the inflation rate is presented.},
	number = {3},
	urldate = {2025-11-16},
	journal = {Journal of Econometrics},
	author = {Bollerslev, Tim},
	month = apr,
	year = {1986},
	pages = {307--327},
}

@book{taleb_black_2007,
	title = {The {Black} {Swan}: {The} {Impact} of the {Highly} {Improbable}},
	isbn = {978-1-58836-583-5},
	shorttitle = {The {Black} {Swan}},
	abstract = {The Black Swan is a standalone book in Nassim Nicholas Taleb’s landmark Incerto series, an investigation of opacity, luck, uncertainty, probability, human error, risk, and decision-making in a world we don’t understand. The other books in the series are Fooled by Randomness, Antifragile, and The Bed of Procrustes.A black swan is a highly improbable event with three principal characteristics: It is unpredictable; it carries a massive impact; and, after the fact, we concoct an explanation that makes it appear less random, and more predictable, than it was. The astonishing success of Google was a black swan; so was 9/11. For Nassim Nicholas Taleb, black swans underlie almost everything about our world, from the rise of religions to events in our own personal lives. Why do we not acknowledge the phenomenon of black swans until after they occur? Part of the answer, according to Taleb, is that humans are hardwired to learn specifics when they should be focused on generalities. We concentrate on things we already know and time and time again fail to take into consideration what we don’t know. We are, therefore, unable to truly estimate opportunities, too vulnerable to the impulse to simplify, narrate, and categorize, and not open enough to rewarding those who can imagine the “impossible.” For years, Taleb has studied how we fool ourselves into thinking we know more than we actually do. We restrict our thinking to the irrelevant and inconsequential, while large events continue to surprise us and shape our world. In this revelatory book, Taleb explains everything we know about what we don’t know, and this second edition features a new philosophical and empirical essay, “On Robustness and Fragility,” which offers tools to navigate and exploit a Black Swan world. Elegant, startling, and universal in its applications, The Black Swan will change the way you look at the world. Taleb is a vastly entertaining writer, with wit, irreverence, and unusual stories to tell. He has a polymathic command of subjects ranging from cognitive science to business to probability theory. The Black Swan is a landmark book—itself a black swan. Praise for Nassim Nicholas Taleb “The most prophetic voice of all.”—GQ Praise for The Black Swan “[A book] that altered modern thinking.”—The Times (London) “A masterpiece.”—Chris Anderson, editor in chief of Wired, author of The Long Tail “Idiosyncratically brilliant.”—Niall Ferguson, Los Angeles Times “The Black Swan changed my view of how the world works.”—Daniel Kahneman, Nobel laureate “[Taleb writes] in a style that owes as much to Stephen Colbert as it does to Michel de Montaigne. . . . We eagerly romp with him through the follies of confirmation bias [and] narrative fallacy.”—The Wall Street Journal “Hugely enjoyable—compelling . . . easy to dip into.”—Financial Times “Engaging . . . The Black Swan has appealing cheek and admirable ambition.”—The New York Times Book ReviewFrom the Hardcover edition.},
	language = {en},
	publisher = {Random House Publishing Group},
	author = {Taleb, Nassim Nicholas},
	month = apr,
	year = {2007},
	note = {Google-Books-ID: gWW4SkJjM08C},
	keywords = {Business \& Economics / Corporate Finance / General, Business \& Economics / General, Philosophy / Epistemology},
}

@misc{noauthor_taleb_nodate,
	title = {taleb 2007 - {Cerca} con {Google}},
	url = {https://www.google.com/search?q=taleb+2007&oq=taleb+2007&gs_lcrp=EgZjaHJvbWUqDAgAEAAYExjjAhiABDIMCAAQABgTGOMCGIAEMgkIARAuGBMYgAQyDAgCEEUYExgWGB4YOzIKCAMQABgTGBYYHjIKCAQQABgTGBYYHjIKCAUQABgTGBYYHjIKCAYQABgTGBYYHjIKCAcQABgTGBYYHjIKCAgQABgTGBYYHjIHCAkQABjvBdIBCDIzMDNqMGo0qAIAsAIB&sourceid=chrome&ie=UTF-8},
	urldate = {2025-11-14},
}

@article{cont_empirical_2001,
	title = {Empirical properties of asset returns: stylized facts and statistical issues},
	volume = {1},
	issn = {1469-7688},
	shorttitle = {Empirical properties of asset returns},
	url = {https://doi.org/10.1080/713665670},
	doi = {10.1080/713665670},
	abstract = {We present a set of stylized empirical facts emerging from the statistical analysis of price variations in various types of financial markets. We first discuss some general issues common to all statistical studies of financial time series. Various statistical properties of asset returns are then described: distributional properties, tail properties and extreme fluctuations, pathwise regularity, linear and nonlinear dependence of returns in time and across stocks. Our description emphasizes properties common to a wide variety of markets and instruments. We then show how these statistical properties invalidate many of the common statistical approaches used to study financial data sets and examine some of the statistical problems encountered in each case.},
	number = {2},
	urldate = {2025-11-14},
	journal = {Quantitative Finance},
	author = {Cont, R.},
	month = feb,
	year = {2001},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/713665670},
	pages = {223--236},
}

@article{gneiting_strictly_2007,
	title = {Strictly {Proper} {Scoring} {Rules}, {Prediction}, and {Estimation}},
	volume = {102},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000001437},
	doi = {10.1198/016214506000001437},
	abstract = {Scoring rules assess the quality of probabilistic forecasts, by assigning a numerical score based on the predictive distribution and on the event or value that materializes. A scoring rule is proper if the forecaster maximizes the expected score for an observation drawn from the distributionF if he or she issues the probabilistic forecast F, rather than G ≠ F. It is strictly proper if the maximum is unique. In prediction problems, proper scoring rules encourage the forecaster to make careful assessments and to be honest. In estimation problems, strictly proper scoring rules provide attractive loss and utility functions that can be tailored to the problem at hand. This article reviews and develops the theory of proper scoring rules on general probability spaces, and proposes and discusses examples thereof. Proper scoring rules derive from convex functions and relate to information measures, entropy functions, and Bregman divergences. In the case of categorical variables, we prove a rigorous version of the Savage representation. Examples of scoring rules for probabilistic forecasts in the form of predictive densities include the logarithmic, spherical, pseudospherical, and quadratic scores. The continuous ranked probability score applies to probabilistic forecasts that take the form of predictive cumulative distribution functions. It generalizes the absolute error and forms a special case of a new and very general type of score, the energy score. Like many other scoring rules, the energy score admits a kernel representation in terms of negative definite functions, with links to inequalities of Hoeffding type, in both univariate and multivariate settings. Proper scoring rules for quantile and interval forecasts are also discussed. We relate proper scoring rules to Bayes factors and to cross-validation, and propose a novel form of cross-validation known as random-fold cross-validation. A case study on probabilistic weather forecasts in the North American Pacific Northwest illustrates the importance of propriety. We note optimum score approaches to point and quantile estimation, and propose the intuitively appealing interval score as a utility function in interval estimation that addresses width as well as coverage.},
	number = {477},
	urldate = {2025-11-14},
	journal = {Journal of the American Statistical Association},
	author = {Gneiting, Tilmann and Raftery, Adrian E},
	month = mar,
	year = {2007},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/016214506000001437},
	keywords = {Bayes factor, Bregman divergence, Brier score, Coherent, Continuous ranked probability score, Cross-validation, Entropy, Kernel score, Loss function, Minimum contrast estimation, Negative definite function, Prediction interval, Predictive distribution, Quantile forecast, Scoring rule, Skill score, Strictly proper, Utility function},
	pages = {359--378},
}

@article{fontana_anticipating_2025,
	title = {Anticipating human mobility: {Methods}, data, and policy in forecasting and foresight},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {Anticipating human mobility},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/anticipating-human-mobility-methods-data-and-policy-in-forecasting-and-foresight/DE780E46CEB5BA24B3C842451A668E0D},
	doi = {10.1017/dap.2025.10034},
	abstract = {The escalating complexity of global migration patterns renders evident the limitation of traditional reactive governance approaches and the urgent need for anticipatory and forward-thinking strategies. This Special Collection, “Anticipatory Methods in Migration Policy: Forecasting, Foresight, and Other Forward-Looking Methods in Migration Policymaking,” groups scholarly works and practitioners’ contributions dedicated to the state-of-the-art of anticipatory approaches. It showcases significant methodological evolutions, highlighting innovations from advanced quantitative forecasting using Machine Learning to predict displacement, irregular border crossings, and asylum trends, to rich, in-depth insights generated through qualitative foresight, participatory scenario building, and hybrid methodologies that integrate diverse knowledge forms. The contributions collectively emphasize the power of methodological pluralism, address a spectrum of migration drivers, including conflict and climate change, and critically examine the opportunities, ethical imperatives, and governance challenges associated with novel data sources, such as mobile phone data. By focusing on translating predictive insights and foresight into actionable policies and humanitarian action, this collection aims to advance both academic discourse and provide tangible guidance for policymakers and practitioners. It underscores the importance of navigating inherent uncertainties and strengthening ethical frameworks to ensure that innovations in anticipatory migration policy enhance preparedness, resource allocation, and uphold human dignity in an era of increasing global migration.},
	language = {en},
	urldate = {2025-10-28},
	journal = {Data \& Policy},
	author = {Fontana, Matteo and Belmonte, Martina and Bosco, Claudio and Jusselme, Damien and Peters, Alina Menocal and Minora, Umberto and Rosinska, Anna and Verhulst, Stefaan},
	month = jan,
	year = {2025},
	keywords = {anticipation, forecasting, foresight, migration},
	pages = {e70},
}

@article{hyndman_robust_2007,
	title = {Robust forecasting of mortality and fertility rates: {A} functional data approach},
	volume = {51},
	issn = {0167-9473},
	shorttitle = {Robust forecasting of mortality and fertility rates},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306002453},
	doi = {10.1016/j.csda.2006.07.028},
	abstract = {A new method is proposed for forecasting age-specific mortality and fertility rates observed over time. This approach allows for smooth functions of age, is robust for outlying years due to wars and epidemics, and provides a modelling framework that is easily adapted to allow for constraints and other information. Ideas from functional data analysis, nonparametric smoothing and robust statistics are combined to form a methodology that is widely applicable to any functional time series data observed discretely and possibly with error. The model is a generalization of the Lee–Carter (LC) model commonly used in mortality and fertility forecasting. The methodology is applied to French mortality data and Australian fertility data, and the forecasts obtained are shown to be superior to those from the LC method and several of its variants.},
	number = {10},
	urldate = {2019-09-19},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hyndman, Rob J. and Shahid Ullah, Md.},
	month = jun,
	year = {2007},
	keywords = {Fertility forecasting, Functional data, Mortality forecasting, Nonparametric smoothing, Principal components, Robustness},
	pages = {4942--4956},
}

@misc{gonzalez-sanz_monotone_2023,
	title = {Monotone {Measure}-{Preserving} {Maps} in {Hilbert} {Spaces}: {Existence}, {Uniqueness}, and {Stability}},
	shorttitle = {Monotone {Measure}-{Preserving} {Maps} in {Hilbert} {Spaces}},
	url = {http://arxiv.org/abs/2305.11751},
	doi = {10.48550/arXiv.2305.11751},
	abstract = {The contribution of this work is twofold. The first part deals with a Hilbert-space version of McCann's celebrated result on the existence and uniqueness of monotone measure-preserving maps: given two probability measures \${\textbackslash}rm P\$ and \${\textbackslash}rm Q\$ on a separable Hilbert space \${\textbackslash}mathcal\{H\}\$ where \${\textbackslash}rm P\$ does not give mass to "small sets" (namely, Lipschitz hypersurfaces), we show, without imposing any moment assumptions, that there exists a gradient of convex function \${\textbackslash}nabla{\textbackslash}psi\$ pushing \$\{{\textbackslash}rm P\} \$ forward to \$\{{\textbackslash}rm Q\}\$. In case \${\textbackslash}mathcal\{H\}\$ is infinite-dimensional, \$\{{\textbackslash}rm P\}\$-a.s. uniqueness is not guaranteed, though. If, however, \$\{{\textbackslash}rm Q\}\$ is boundedly supported (a natural assumption in several statistical applications), then this gradient is \$\{{\textbackslash}rm P\}\$ a.s. unique. In the second part of the paper, we establish stability results for transport maps in the sense of uniform convergence over compact "regularity sets". As a consequence, we obtain a central limit theorem for the fluctuations of the optimal quadratic transport cost in a separable Hilbert space.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {González-Sanz, Alberto and Hallin, Marc and Sen, Bodhisattva},
	month = may,
	year = {2023},
	note = {arXiv:2305.11751 [math]},
	keywords = {49Q22, Mathematics - Functional Analysis, Mathematics - Probability},
}

@article{fontana_identification_2024,
	title = {Identification of {Precursors} in {InSAR} {Time} {Series} {Using} {Functional} {Data} {Analysis} {Post}-{Processing}: {Demonstration} on {Mud} {Volcano} {Eruptions}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Identification of {Precursors} in {InSAR} {Time} {Series} {Using} {Functional} {Data} {Analysis} {Post}-{Processing}},
	url = {https://www.mdpi.com/2072-4292/16/7/1191},
	doi = {10.3390/rs16071191},
	abstract = {One of the most promising applications of satellite data is providing users in charge of land and emergency management with information and data to support decision making for geohazard mapping, monitoring and early warning. In this work, we consider ground displacement data obtained via interferometric processing of satellite radar imagery, and we provide a novel post-processing approach based on a Functional Data Analysis paradigm capable of detecting precursors in displacement time series. The proposed approach appropriately accounts for the spatial and temporal dependencies of the data and does not require prior assumptions on the deformation trend. As an illustrative case, we apply the developed method to the identification of precursors to a mud volcano eruption in the Santa Barbara village in Sicily, southern Italy, showing the advantages of using a Functional Data Analysis framework for anticipating the warning signal. Indeed, the proposed approach is able to detect precursors of the paroxysmal event in the time series of the locations close to the eruption vent and provides a warning signal months before a scalar approach would. The method presented can potentially be applied to a wide range of geological events, thus representing a valuable and far-reaching monitoring tool.},
	language = {en},
	number = {7},
	urldate = {2024-04-11},
	journal = {Remote Sensing},
	author = {Fontana, Matteo and Bernardi, Mara Sabina and Cigna, Francesca and Tapete, Deodato and Menafoglio, Alessandra and Vantini, Simone},
	month = jan,
	year = {2024},
	note = {Number: 7
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {InSAR data, early warning, functional boxplot, functional data analysis, post-processing},
	pages = {1191},
}

@book{vershynin_high-dimensional_2018,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Probability}: {An} {Introduction} with {Applications} in {Data} {Science}},
	isbn = {978-1-108-41519-4},
	shorttitle = {High-{Dimensional} {Probability}},
	url = {https://www.cambridge.org/core/books/highdimensional-probability/797C466DA29743D2C8213493BD2D2102},
	abstract = {High-dimensional probability offers insight into the behavior of random vectors, random matrices, random subspaces, and objects used to quantify uncertainty in high dimensions. Drawing on ideas from probability, analysis, and geometry, it lends itself to applications in mathematics, statistics, theoretical computer science, signal processing, optimization, and more. It is the first to integrate theory, key tools, and modern applications of high-dimensional probability. Concentration inequalities form the core, and it covers both classical results such as Hoeffding's and Chernoff's inequalities and modern developments such as the matrix Bernstein's inequality. It then introduces the powerful methods based on stochastic processes, including such tools as Slepian's, Sudakov's, and Dudley's inequalities, as well as generic chaining and bounds based on VC dimension. A broad range of illustrations is embedded throughout, including classical and modern results for covariance estimation, clustering, networks, semidefinite programming, coding, dimension reduction, matrix completion, machine learning, compressed sensing, and sparse regression.},
	urldate = {2024-11-08},
	publisher = {Cambridge University Press},
	author = {Vershynin, Roman},
	year = {2018},
	doi = {10.1017/9781108231596},
}

@article{madonia_geomorphological_2011,
	title = {Geomorphological and geochemical characterization of the 11 {August} 2008 mud volcano eruption at {S}. {Barbara} village ({Sicily}, {Italy}) and its possible relationship with seismic activity},
	volume = {11},
	issn = {1561-8633},
	url = {https://nhess.copernicus.org/articles/11/1545/2011/},
	doi = {10.5194/nhess-11-1545-2011},
	abstract = {On 11 August 2008 a paroxysmal eruption occurred at Santa Barbara mud volcano (MV), located close to Caltanissetta, one of the most densely populated cities of Sicily (Italy). An associated minor event took place on August 2009. Both the events caused severe damage to civil infrastructures located within a range of about 2 km from the eruptive vent. Geomorphological, geochemical, and seismological investigations were carried out for framing the events in the appropriate geodynamic context. Geomorphological surveys recognized, in the immediate surrounding of the main emission point, two different families of processes and landforms: (i) ground deformations and (ii) changes in morphology and number of the fluid emitting vents. These processes were associated to a wider network of fractures, seemingly generated by the shock wave produced by the gas blast that occurred at the main paroxysm. Geochemical characterization allowed an estimation of the source of the fluids, or at least their last standing, at about 3 km depth. Finally, the close time relationships observed between anomalous increments of seismic activity and the two main paroxysmal events accounted for a possible common trigger for both the phenomena, even with different timing due to the very different initial conditions and characteristics of the two processes, i.e. seismogenesis and gas overloading.},
	language = {English},
	number = {5},
	urldate = {2024-03-11},
	journal = {Natural Hazards and Earth System Sciences},
	author = {Madonia, P. and Grassa, F. and Cangemi, M. and Musumeci, C.},
	month = may,
	year = {2011},
	note = {Publisher: Copernicus GmbH},
	pages = {1545--1557},
}

@book{hyndman_forecasting_2021,
	address = {Melbourne, Australia},
	title = {Forecasting: {Principles} and {Practice}},
	isbn = {978-0-9875071-3-6},
	shorttitle = {Forecasting},
	abstract = {Forecasting is required in many situations. Deciding whether to build another power generation plant in the next five years requires forecasts of future demand. Scheduling staff in a call centre next week requires forecasts of call volumes. Stocking an inventory requires forecasts of stock requirements. Telecommunication routing requires traffic forecasts a few minutes ahead. Whatever the circumstances or time horizons involved, forecasting is an important aid in effective and efficient planning. This textbook provides a comprehensive introduction to forecasting methods and presents enough information about each method for readers to use them sensibly. Examples use R with many data sets taken from the authors' own consulting experience. In this third edition, all chapters have been updated to cover the latest research and forecasting methods. One new chapter has been added on time series features. The latest version of the book is freely available online at http://OTexts.com/fpp3.},
	language = {English},
	publisher = {OTexts},
	author = {Hyndman, Rob J. and Athanasopoulos, George},
	year = {2021},
}

@book{elliott_economic_2016,
	address = {Princeton},
	edition = {1st ed},
	title = {Economic {Forecasting}},
	isbn = {978-0-691-14013-1 978-1-4008-8089-8},
	abstract = {Cover -- Title -- Copyright -- Dedication -- Contents -- Preface -- I Foundations -- 1 Introduction -- 1.1 Outline of the Book -- 1.2 Technical Notes -- 2 Loss Functions -- 2.1 Construction and Specification of the Loss Function -- 2.2 Specific Loss Functions -- 2.3 Multivariate Loss Functions -- 2.4 Scoring Rules for Distribution Forecasts -- 2.5 Examples of Applications of Forecasts in Macroeconomics and Finance -- 2.6 Conclusion -- 3 The Parametric Forecasting Problem -- 3.1 Optimal Point Forecasts -- 3.2 Classical Approach -- 3.3 Bayesian Approach -- 3.4 Relating the Bayesian and Classical Methods -- 3.5 Empirical Example: Asset Allocation with Parameter Uncertainty -- 3.6 Conclusion -- 4 Classical Estimation of Forecasting Models -- 4.1 Loss-Based Estimators -- 4.2 Plug-In Estimators -- 4.3 Parametric versus Nonparametric Estimation Approaches -- 4.4 Conclusion -- 5 Bayesian Forecasting Methods -- 5.1 Bayes Risk -- 5.2 Ridge and Shrinkage Estimators -- 5.3 Computational Methods -- 5.4 Economic Applications of Bayesian Forecasting Methods -- 5.5 Conclusion -- 6 Model Selection -- 6.1 Trade-Offs in Model Selection -- 6.2 Sequential Hypothesis Testing -- 6.3 Information Criteria -- 6.4 Cross Validation -- 6.5 Lasso Model Selection -- 6.6 Hard versus Soft Thresholds: Bagging -- 6.7 Empirical Illustration: Forecasting Stock Returns -- 6.8 Properties of Model Selection Procedures -- 6.9 Risk for Model Selection Methods: Monte Carlo Simulations -- 6.10 Conclusion -- 6.11 Appendix: Derivation of Information Criteria -- II Forecast Methods -- 7 Univariate Linear Prediction Models -- 7.1 ARMA Models as Approximations -- 7.2 Estimation and Lag Selection for ARMA Models -- 7.3 Forecasting with ARMA Models -- 7.4 Deterministic and Seasonal Components -- 7.5 Exponential Smoothing and Unobserved Components -- 7.6 Conclusion},
	language = {eng},
	publisher = {Princeton University Press},
	author = {Elliott, Graham and Timmermann, Allan},
	year = {2016},
}

@article{vovk_e-values_2021,
	title = {E-values: {Calibration}, combination and applications},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	shorttitle = {E-values},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-3/E-values-Calibration-combination-and-applications/10.1214/20-AOS2020.full},
	doi = {10.1214/20-AOS2020},
	abstract = {Multiple testing of a single hypothesis and testing multiple hypotheses are usually done in terms of p-values. In this paper, we replace p-values with their natural competitor, e-values, which are closely related to betting, Bayes factors and likelihood ratios. We demonstrate that e-values are often mathematically more tractable; in particular, in multiple testing of a single hypothesis, e-values can be merged simply by averaging them. This allows us to develop efficient procedures using e-values for testing multiple hypotheses.},
	number = {3},
	urldate = {2024-11-06},
	journal = {The Annals of Statistics},
	author = {Vovk, Vladimir and Wang, Ruodu},
	month = jun,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62C07, 62C15, 62F03, 62G10, Bayes factor, Hypothesis testing, admissible decisions, global null, multiple hypothesis testing, test martingale},
	pages = {1736--1754},
}

@misc{diquigiovanni_distribution-free_2024,
	title = {Distribution-{Free} {Prediction} {Bands} for {Multivariate} {Functional} {Time} {Series}: an {Application} to the {Italian} {Gas} {Market}},
	shorttitle = {Distribution-{Free} {Prediction} {Bands} for {Multivariate} {Functional} {Time} {Series}},
	url = {http://arxiv.org/abs/2107.00527},
	doi = {10.48550/arXiv.2107.00527},
	abstract = {Uncertainty quantification in forecasting represents a topic of great importance in energy trading, as understanding the status of the energy market would enable traders to directly evaluate the impact of their own offers/bids. To this end, we propose a scalable procedure that outputs closed-form simultaneous prediction bands for multivariate functional response variables in a time series setting, which is able to guarantee performance bounds in terms of unconditional coverage and asymptotic exactness, both under some conditions. After evaluating its performance on synthetic data, the method is used to build multivariate prediction bands for daily demand and offer curves in the Italian gas market.},
	urldate = {2024-12-10},
	publisher = {arXiv},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	month = jan,
	year = {2024},
	note = {arXiv:2107.00527 [stat]},
	keywords = {Statistics - Applications, Statistics - Methodology},
}

@article{messoudi_copula-based_2021,
	title = {Copula-based conformal prediction for multi-target regression},
	volume = {120},
	issn = {0031-3203},
	url = {https://www.sciencedirect.com/science/article/pii/S0031320321002880},
	doi = {10.1016/j.patcog.2021.108101},
	abstract = {There are relatively few works dealing with conformal prediction for multi-task learning issues, and this is particularly true for multi-target regression. This paper focuses on the problem of providing valid (i.e., frequency calibrated) multi-variate predictions. To do so, we propose to use copula functions for inductive conformal prediction, and illustrate our proposal by applying it to deep neural networks and random forests. We show that the proposed method ensures efficiency and validity for multi-target regression problems on various data sets.},
	urldate = {2024-04-19},
	journal = {Pattern Recognition},
	author = {Messoudi, Soundouss and Destercke, Sébastien and Rousseau, Sylvain},
	month = dec,
	year = {2021},
	keywords = {Copula functions, Deep neural networks, Inductive conformal prediction, Multi-target regression, Random forests},
	pages = {108101},
}

@misc{cresswell_conformal_2024,
	title = {Conformal {Prediction} {Sets} {Improve} {Human} {Decision} {Making}},
	url = {http://arxiv.org/abs/2401.13744},
	doi = {10.48550/arXiv.2401.13744},
	abstract = {In response to everyday queries, humans explicitly signal uncertainty and offer alternative answers when they are unsure. Machine learning models that output calibrated prediction sets through conformal prediction mimic this human behaviour; larger sets signal greater uncertainty while providing alternatives. In this work, we study the usefulness of conformal prediction sets as an aid for human decision making by conducting a pre-registered randomized controlled trial with conformal prediction sets provided to human subjects. With statistical significance, we find that when humans are given conformal prediction sets their accuracy on tasks improves compared to fixed-size prediction sets with the same coverage guarantee. The results show that quantifying model uncertainty with conformal prediction is helpful for human-in-the-loop decision making and human-AI teams.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Cresswell, Jesse C. and Sui, Yi and Kumar, Bhargava and Vouitsis, Noël},
	month = feb,
	year = {2024},
	note = {arXiv:2401.13744 [cs, stat]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{xu_conformal_2021,
	title = {Conformal prediction interval for dynamic time-series},
	url = {https://proceedings.mlr.press/v139/xu21h.html},
	abstract = {We develop a method to construct distribution-free prediction intervals for dynamic time-series, called {\textbackslash}Verb{\textbar}EnbPI{\textbar} that wraps around any bootstrap ensemble estimator to construct sequential prediction intervals. {\textbackslash}Verb{\textbar}EnbPI{\textbar} is closely related to the conformal prediction (CP) framework but does not require data exchangeability. Theoretically, these intervals attain finite-sample, {\textbackslash}textit\{approximately valid\} marginal coverage for broad classes of regression functions and time-series with strongly mixing stochastic errors. Computationally, {\textbackslash}Verb{\textbar}EnbPI{\textbar} avoids overfitting and requires neither data-splitting nor training multiple ensemble estimators; it efficiently aggregates bootstrap estimators that have been trained. In general, {\textbackslash}Verb{\textbar}EnbPI{\textbar} is easy to implement, scalable to producing arbitrarily many prediction intervals sequentially, and well-suited to a wide range of regression functions. We perform extensive real-data analyses to demonstrate its effectiveness.},
	language = {en},
	urldate = {2025-10-27},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Xu, Chen and Xie, Yao},
	month = jul,
	year = {2021},
	note = {ISSN: 2640-3498},
	pages = {11559--11569},
}

@misc{vovk_cross-conformal_2020,
	title = {Cross-conformal e-prediction},
	url = {http://arxiv.org/abs/2001.05989},
	doi = {10.48550/arXiv.2001.05989},
	abstract = {This note discusses a simple modification of cross-conformal prediction inspired by recent work on e-values. The precursor of conformal prediction developed in the 1990s by Gammerman, Vapnik, and Vovk was also based on e-values and is called conformal e-prediction in this note. Replacing e-values by p-values led to conformal prediction, which has important advantages over conformal e-prediction without obvious disadvantages. The situation with cross-conformal prediction is, however, different: whereas for cross-conformal prediction validity is only an empirical fact (and can be broken with excessive randomization), this note draws the reader's attention to the obvious fact that cross-conformal e-prediction enjoys a guaranteed property of validity.},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Vovk, Vladimir},
	month = jan,
	year = {2020},
	note = {arXiv:2001.05989 [cs, stat]},
	keywords = {68T05, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vovk_conditional_2013,
	title = {Conditional validity of inductive conformal predictors},
	volume = {92},
	copyright = {http://www.springer.com/tdm},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/s10994-013-5355-6},
	doi = {10.1007/s10994-013-5355-6},
	abstract = {Conformal predictors are set predictors that are automatically valid in the sense of having coverage probability equal to or exceeding a given conﬁdence level. Inductive conformal predictors are a computationally eﬃcient version of conformal predictors satisfying the same property of validity. However, inductive conformal predictors have been only known to control unconditional coverage probability. This paper explores various versions of conditional validity and various ways to achieve them using inductive conformal predictors and their modiﬁcations.},
	language = {en},
	number = {2-3},
	urldate = {2024-12-10},
	journal = {Machine Learning},
	author = {Vovk, Vladimir},
	month = sep,
	year = {2013},
	keywords = {Batch mode of learning, Boosting, Conditional validity, Inductive conformal predictors, MART, ROC curves, Spam detection},
	pages = {349--376},
}

@incollection{bernardi_composition--function_2021,
	title = {Composition-on-{Function} {Regression} {Model} for the {Remote} {Analysis} of {Near}-{Earth} {Asteroids}},
	copyright = {All rights reserved},
	url = {https://iris.unitn.it/retrieve/01ce3fb1-69e2-40ba-baf8-a8c2d363394f/pearson-sis-book-2021-parte-1.pdf#page=822},
	urldate = {2024-03-17},
	booktitle = {Book of {Short} {Papers} - {SIS2021}},
	author = {Bernardi, Mara S. and Fontana, Matteo and Menafoglio, Alessandra and Pisello, Alessandro and Porreca, Massimiliano and Perugini, Diego and Vantini, Simone},
	year = {2021},
	pages = {801},
}

@article{aiello_bayesian_2023,
	title = {Bayesian functional emulation of {CO2} emissions on future climate change scenarios},
	volume = {34},
	copyright = {© 2023 The Authors. Environmetrics published by John Wiley \& Sons Ltd.},
	issn = {1099-095X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2821},
	doi = {10.1002/env.2821},
	abstract = {We propose a statistical emulator for a climate-economy deterministic integrated assessment model ensemble, based on a functional regression framework. Inference on the unknown parameters is carried out through a mixed effects hierarchical model using a fully Bayesian framework with a prior distribution on the vector of all parameters. We also suggest an autoregressive parameterization of the covariance matrix of the error, with matching marginal prior. In this way, we allow for a functional framework for the discretized output of the simulators that allows their time continuous evaluation.},
	language = {en},
	number = {8},
	urldate = {2024-07-23},
	journal = {Environmetrics},
	author = {Aiello, Luca and Fontana, Matteo and Guglielmi, Alessandra},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.2821},
	keywords = {Bayesian statistics, functional regression, hierarchical modeling, mixed effects model, uncertainty quantification},
	pages = {e2821},
}

@article{bircan_augmentation_2024,
	title = {Augmentation or {Replication}? {Assessing} {Big} {Data}’s {Role} in {Migration} {Studies}},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {Augmentation or {Replication}?},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/augmentation-or-replication-assessing-big-datas-role-in-migration-studies/41DD08C5021AE0936150A1AD2C6AF917},
	doi = {10.1017/dap.2024.57},
	abstract = {As the field of migration studies evolves in the digital age, big data analytics emerge as a potential game-changer, promising unprecedented granularity, timeliness, and dynamism in understanding migration patterns. However, the epistemic value added by this data explosion remains an open question. This paper critically appraises the claim, investigating the extent to which big data augments, rather than merely replicates, traditional data insights in migration studies. Through a rigorous literature review of empirical research, complemented by a conceptual analysis, we aim to map out the methodological shifts and intellectual advancements brought forth by big data. The potential scientific impact of this study extends into the heart of the discipline, providing critical illumination on the actual knowledge contribution of big data to migration studies. This, in turn, delivers a clarified roadmap for navigating the intersections of data science, migration research, and policymaking.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Bircan, Tuba},
	month = jan,
	year = {2024},
	keywords = {big data, human migration, predictive models},
	pages = {e51},
}

@article{price_are_nodate,
	title = {Are {Actuarial} {Crop} {Insurance} {Rates} {Fair}?: {An} {Analysis} {Using} a {Penalized} {Bivariate} {B}‐{Spline} {Method}},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12363?af=R},
	doi = {10.1111/rssc.12363},
	abstract = {{\textless}p{\textgreater}In this paper, we investigate whether the yield insurance premium rates given by the US Department of Agriculture's Risk Management Agency are actuarially fair by comparing the conditional yield density inferred from premium data with the conditional yield density inferred from yield data. A procedure is developed to estimate the conditional yield density by using premium data through partial derivatives of the premium rate function, as fitted by penalized bivariate tensor product \textit{B}‐splines. We study the asymptotic properties of partial derivatives of a penalized bivariate tensor product \textit{B}‐spline estimator and provide variance estimates. The conditional yield density inferred from premium data and its variance estimator are evaluated through simulation studies. The procedure is also applied to a crop insurance data set from the state of Iowa to examine the actuarial fairness of the premium rates. On average, premium rates are close to our estimates and this is true for each coverage level. However, premiums for low productivity land are generally too low whereas those for high productivity land are generally too high. Even after subsidies, premiums for the more productive land are generally substantially higher than are our estimates of the corresponding actuarially fair rates.{\textless}/p{\textgreater}},
	journal = {Journal of the Royal Statistical Society. Series C: Applied Statistics},
	author = {Price, Michael J. and Yu, Cindy L. and Hennessy, David A. and Du, Xiaodong},
}

@article{hamada_analyzing_nodate,
	title = {Analyzing {Unreplicated} {Factorial} {Experiments}: {A} {Review} with {Some} {New} {Proposals}},
	abstract = {Recently, there have been many proposals for objectively analyzing unreplicated factorial experiments. We review these methods along with some earlier and perhaps lesser known ones. New methods are also proposed. The primary aim of this paper is to compare these methods and their variants via an extensive simulation study. Robustness of the various methods to non-normality is also considered. Many methods are comparable, but clearly some cannot be recommended. The results from the study also suggest some basic principles for evaluating new methods. Finally, we outline some issues that this study has raised and which might beneﬁt from work in other areas such as multiple comparisons, outlier detection, ranking and selection, and robust statistics.},
	language = {en},
	author = {Hamada, M and Balakrishnan, N},
	pages = {41},
}

@article{angelopoulos_conformal_2023-2,
	title = {Conformal {PID} {Control} for {Time} {Series} {Prediction}},
	volume = {36},
	url = {https://proceedings.neurips.cc/paper_files/paper/2023/hash/47f2fad8c1111d07f83c91be7870f8db-Abstract-Conference.html},
	language = {en},
	urldate = {2025-10-27},
	journal = {Advances in Neural Information Processing Systems},
	author = {Angelopoulos, Anastasios and Candes, Emmanuel and Tibshirani, Ryan J.},
	month = dec,
	year = {2023},
	pages = {23047--23074},
}

@misc{xu_sequential_2023,
	title = {Sequential {Predictive} {Conformal} {Inference} for {Time} {Series}},
	abstract = {We present a new distribution-free conformal prediction algorithm for sequential data (e.g., time series), called the {\textbackslash}textbackslashtextit\{sequential predictive conformal inference\} ({\textbackslash}textbackslashtexttt\{SPCI\}). We specifically account for the nature that time series data are non-exchangeable, and thus many existing conformal prediction algorithms are not applicable. The main idea is to adaptively re-estimate the conditional quantile of non-conformity scores (e.g., prediction residuals), upon exploiting the temporal dependence among them. More precisely, we cast the problem of conformal prediction interval as predicting the quantile of a future residual, given a user-specified point prediction algorithm. Theoretically, we establish asymptotic valid conditional coverage upon extending consistency analyses in quantile regression. Using simulation and real-data experiments, we demonstrate a significant reduction in interval width of {\textbackslash}textbackslashtexttt\{SPCI\} compared to other existing methods under the desired empirical coverage.},
	urldate = {2025-08-18},
	publisher = {arXiv},
	author = {Xu, Chen and Xie, Yao},
	month = may,
	year = {2023},
	doi = {10.48550/arXiv.2212.03463},
	note = {Issue: arXiv:2212.03463
\_eprint: 2212.03463},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{stocker_conformal_2025,
	title = {Conformal {Forecasting} {Experiments}},
	urldate = {2025-09-09},
	author = {Stocker, Massimo},
	month = aug,
	year = {2025},
}

@misc{krebs_large_2017,
	title = {A {Large} {Deviation} {Inequality} for \{\vphantom{\}}{\textbackslash}beta\$\$-{Mixing} {Time} {Series} and {Its} {Applications} to the {Functional} {Kernel} {Regression} {Model}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	abstract = {We give a new large deviation inequality for sums of random variables of the form {\textbackslash}Z\_k = f(X\_k,X\_t){\textbackslash} for {\textbackslash}k,t{\textbackslash}textbackslashin {\textbackslash}textbackslashmathbb\{N\}{\textbackslash}, {\textbackslash}t{\textbackslash} fixed, where the underlying process {\textbackslash}X{\textbackslash} is \{\vphantom{\}}{\textbackslash}beta\$\$-mixing. The inequality can be used to derive concentration inequalities. We demonstrate its usefulness in the functional kernel regression model of Ferraty et al. (2007) where we study the consistency of dynamic forecasts.},
	urldate = {2025-09-08},
	publisher = {arXiv},
	author = {Krebs, Johannes T. N.},
	year = {2017},
	doi = {10.48550/ARXIV.1701.05380},
	keywords = {62G20 62M10 37A25 (Primary) 62G09 (Secondary), FOS: Mathematics, Statistics Theory (math.ST)},
}

@book{doukhan_mixing_1994,
	title = {Mixing : {Properties} and {Examples}},
	isbn = {978-0-387-94214-8 978-1-4612-2642-0},
	publisher = {Springer-Verlag},
	author = {Doukhan, Paul},
	month = jan,
	year = {1994},
}

@article{poinas_bound_2019,
	title = {A {Bound} of the β -{Mixing} {Coefficient} for {Point} {Processes} in {Terms} of {Their} {Intensity} {Functions}},
	volume = {148},
	issn = {01677152},
	doi = {10.1016/j.spl.2018.12.007},
	urldate = {2025-09-08},
	journal = {Statistics \& Probability Letters},
	author = {Poinas, Arnaud},
	month = may,
	year = {2019},
	pages = {88--93},
}

@inproceedings{chen_conformalized_2024,
	title = {Conformalized {Time} {Series} with {Semantic} {Features}},
	volume = {37},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Chen, Baiting and Ren, Zhimei and Cheng, Lu},
	editor = {Globerson, A. and Mackey, L. and Belgrave, D. and Fan, A. and Paquet, U. and Tomczak, J. and Zhang, C.},
	year = {2024},
	pages = {121449--121474},
}

@inproceedings{stankeviciute_conformal_2021,
	title = {Conformal {Time}-{Series} {Forecasting}},
	volume = {34},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Stankeviciute, Kamile and M. Alaa, Ahmed and van der Schaar, Mihaela},
	editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
	year = {2021},
	pages = {6216--6228},
}

@misc{lee_transformer_2024,
	title = {Transformer {Conformal} {Prediction} for {Time} {Series}},
	abstract = {We present a conformal prediction method for time series using the Transformer architecture to capture long-memory and long-range dependencies. Specifically, we use the Transformer decoder as a conditional quantile estimator to predict the quantiles of prediction residuals, which are used to estimate the prediction interval. We hypothesize that the Transformer decoder benefits the estimation of the prediction interval by learning temporal dependencies across past prediction residuals. Our comprehensive experiments using simulated and real data empirically demonstrate the superiority of the proposed method compared to the existing state-of-the-art conformal prediction methods.},
	urldate = {2025-09-08},
	publisher = {arXiv},
	author = {Lee, Junghwan and Xu, Chen and Xie, Yao},
	month = jun,
	year = {2024},
	doi = {10.48550/arXiv.2406.05332},
	note = {Issue: arXiv:2406.05332
\_eprint: 2406.05332},
	keywords = {Computer Science - Machine Learning},
}

@misc{farinhas_non-exchangeable_2024,
	title = {Non-{Exchangeable} {Conformal} {Risk} {Control}},
	abstract = {Split conformal prediction has recently sparked great interest due to its ability to provide formally guaranteed uncertainty sets or intervals for predictions made by black-box neural models, ensuring a predefined probability of containing the actual ground truth. While the original formulation assumes data exchangeability, some extensions handle non-exchangeable data, which is often the case in many real-world scenarios. In parallel, some progress has been made in conformal methods that provide statistical guarantees for a broader range of objectives, such as bounding the best {\textbackslash}F\_1{\textbackslash}-score or minimizing the false negative rate in expectation. In this paper, we leverage and extend these two lines of work by proposing non-exchangeable conformal risk control, which allows controlling the expected value of any monotone loss function when the data is not exchangeable. Our framework is flexible, makes very few assumptions, and allows weighting the data based on its relevance for a given test example; a careful choice of weights may result on tighter bounds, making our framework useful in the presence of change points, time series, or other forms of distribution drift. Experiments with both synthetic and real world data show the usefulness of our method.},
	urldate = {2025-08-18},
	publisher = {arXiv},
	author = {Farinhas, António and Zerva, Chrysoula and Ulmer, Dennis and Martins, André F. T.},
	month = jan,
	year = {2024},
	doi = {10.48550/arXiv.2310.01262},
	note = {Issue: arXiv:2310.01262
\_eprint: 2310.01262},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{bradley_basic_2005,
	title = {Basic {Properties} of {Strong} {Mixing} {Conditions}. {A} {Survey} and {Some} {Open} {Questions}},
	volume = {2},
	issn = {1549-5787},
	doi = {10.1214/154957805100000104},
	number = {none},
	urldate = {2025-09-08},
	journal = {Probability Surveys},
	author = {Bradley, Richard C.},
	month = jan,
	year = {2005},
}

@misc{bhattacharyya_group-weighted_2025,
	title = {Group-{Weighted} {Conformal} {Prediction}},
	abstract = {Conformal prediction (CP) is a method for constructing a prediction interval around the output of a fitted model, whose validity does not rely on the model being correct–the CP interval offers a coverage guarantee that is distribution-free, but relies on the training data being drawn from the same distribution as the test data. A recent variant, weighted conformal prediction (WCP), reweights the method to allow for covariate shift between the training and test distributions. However, WCP requires knowledge of the nature of the covariate shift-specifically,the likelihood ratio between the test and training covariate distributions. In practice, since this likelihood ratio is estimated rather than known exactly, the coverage guarantee may degrade due to the estimation error. In this paper, we consider a special scenario where observations belong to a finite number of groups, and these groups determine the covariate shift between the training and test distributions-for instance, this may arise if the training set is collected via stratified sampling. Our results demonstrate that in this special case, the predictive coverage guarantees of WCP can be drastically improved beyond the bounds given by existing estimation error bounds.},
	urldate = {2025-06-20},
	publisher = {arXiv},
	author = {Bhattacharyya, Aabesh and Barber, Rina Foygel},
	month = apr,
	year = {2025},
	doi = {10.48550/arXiv.2401.17452},
	note = {Issue: arXiv:2401.17452
\_eprint: 2401.17452},
	keywords = {Statistics - Methodology},
}

@misc{auer_conformal_2023,
	title = {Conformal {Prediction} for {Time} {Series} with {Modern} {Hopfield} {Networks}},
	abstract = {To quantify uncertainty, conformal prediction methods are gaining continuously more interest and have already been successfully applied to various domains. However, they are difficult to apply to time series as the autocorrelative structure of time series violates basic assumptions required by conformal prediction. We propose HopCPT, a novel conformal prediction approach for time series that not only copes with temporal structures but leverages them. We show that our approach is theoretically well justified for time series where temporal dependencies are present. In experiments, we demonstrate that our new approach outperforms state-of-the-art conformal prediction methods on multiple real-world time series datasets from four different domains.},
	urldate = {2025-08-18},
	publisher = {arXiv},
	author = {Auer, Andreas and Gauch, Martin and Klotz, Daniel and Hochreiter, Sepp},
	month = nov,
	year = {2023},
	doi = {10.48550/arXiv.2303.12783},
	note = {Issue: arXiv:2303.12783
\_eprint: 2303.12783},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{angelopoulos_online_2024,
	title = {Online {Conformal} {Prediction} with {Decaying} {Step} {Sizes}},
	abstract = {We introduce a method for online conformal prediction with decaying step sizes. Like previous methods, ours possesses a retrospective guarantee of coverage for arbitrary sequences. However, unlike previous methods, we can simultaneously estimate a population quantile when it exists. Our theory and experiments indicate substantially improved practical properties: in particular, when the distribution is stable, the coverage is close to the desired level for every time point, not just on average over the observed sequence.},
	urldate = {2025-08-18},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Barber, Rina Foygel and Bates, Stephen},
	month = may,
	year = {2024},
	doi = {10.48550/arXiv.2402.01139},
	note = {Issue: arXiv:2402.01139
\_eprint: 2402.01139},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{dandapanthula_conformal_2025,
	title = {Conformal {Changepoint} {Localization}},
	abstract = {Changepoint localization is the problem of estimating the index at which a change occurred in the data generating distribution of an ordered list of data, or declaring that no change occurred. We present the broadly applicable CONCH (CONformal CHangepoint localization) algorithm, which uses a matrix of conformal p-values to produce a confidence interval for a (single) changepoint under the mild assumption that the pre-change and post-change distributions are each exchangeable. We exemplify the CONCH algorithm on a variety of synthetic and real-world datasets, including using black-box pre-trained classifiers to detect changes in sequences of images or text.},
	urldate = {2025-06-20},
	publisher = {arXiv},
	author = {Dandapanthula, Sanjit and Ramdas, Aaditya},
	month = may,
	year = {2025},
	doi = {10.48550/arXiv.2505.00292},
	note = {Issue: arXiv:2505.00292
\_eprint: 2505.00292},
	keywords = {Electrical Engineering and Systems Science - Signal Processing, Mathematics - Statistics Theory, Statistics - Methodology, Statistics - Statistics Theory},
}

@article{berkmen_chest_1975,
	title = {Chest {Roentgenography} as a {Window} to the {Diagnosis} of {Takayasu}'s {Arteritis}},
	volume = {125},
	issn = {0002-9580},
	doi = {10.2214/ajr.125.4.842},
	abstract = {The chest roentgenographic findings in Takayasu's arteritis include widening of the ascending aorta, contour irregularities of the descending aorta, arotic calcifications, pulmonary arterial changes, rib notching, and hilar lymphadenopathy. The single most important diagnostic sign is a segmental calcification outlining a localized or diffuse narrowing of the aorta. The other signs may be suspicious or suggestive, but the diagnostic accuracy increases when several findings are present simultaneously.},
	number = {4},
	journal = {The American Journal of Roentgenology, Radium Therapy, and Nuclear Medicine},
	author = {Berkmen, Y. M. and Lande, A.},
	month = dec,
	year = {1975},
	pmid = {2023},
	keywords = {Adolescent, Adult, Aorta, Aortic Arch Syndromes, Arteritis, Calcinosis, Female, Humans, Hypertension Pulmonary, Lymphatic Diseases, Middle Aged, Radiography, Ribs, Takayasu Arteritis, United States},
	pages = {842--846},
}

@misc{angelopoulos_conformal_2025,
	title = {Conformal {Risk} {Control}},
	abstract = {We extend conformal prediction to control the expected value of any monotone loss function. The algorithm generalizes split conformal prediction together with its coverage guarantee. Like conformal prediction, the conformal risk control procedure is tight up to an \{\vphantom{\}}{\textbackslash}textbackslashmathcal\{O\}(1/n){\textbackslash} factor. We also introduce extensions of the idea to distribution shift, quantile risk control, multiple and adversarial risk control, and expectations of U-statistics. Worked examples from computer vision and natural language processing demonstrate the usage of our algorithm to bound the false negative rate, graph distance, and token-level F1-score.},
	urldate = {2025-08-18},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Bates, Stephen and Fisch, Adam and Lei, Lihua and Schuster, Tal},
	month = jun,
	year = {2025},
	doi = {10.48550/arXiv.2208.02814},
	note = {Issue: arXiv:2208.02814
\_eprint: 2208.02814},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology, Statistics - Statistics Theory},
}

@inproceedings{gibbs_adaptive_2021,
	title = {Adaptive {Conformal} {Inference} {Under} {Distribution} {Shift}},
	volume = {34},
	url = {https://papers.neurips.cc/paper_files/paper/2021/hash/0d441de75945e5acbc865406fc9a2559-Abstract.html},
	abstract = {We develop methods for forming prediction sets in an online setting where the data generating distribution is allowed to vary over time in an unknown fashion. Our framework builds on ideas from conformal inference to provide a general wrapper that can be combined with any black box method that produces point predictions of the unseen label or estimated quantiles of its distribution. While previous conformal inference methods rely on the assumption that the data are exchangeable, our adaptive approach provably achieves the desired coverage frequency over long-time intervals irrespective of the true data generating process. We accomplish this by modelling the distribution shift as a learning problem in a single parameter whose optimal value is varying over time and must be continuously re-estimated. We test our method, adaptive conformal inference, on two real world datasets and find that its predictions are robust to visible and significant distribution shifts.},
	urldate = {2025-10-27},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Gibbs, Isaac and Candes, Emmanuel},
	year = {2021},
	pages = {1660--1672},
}

@misc{angelopoulos_theoretical_2025,
	title = {Theoretical {Foundations} of {Conformal} {Prediction}},
	url = {http://arxiv.org/abs/2411.11824},
	doi = {10.48550/arXiv.2411.11824},
	abstract = {This book is about conformal prediction and related inferential techniques that build on permutation tests and exchangeability. These techniques are useful in a diverse array of tasks, including hypothesis testing and providing uncertainty quantification guarantees for machine learning systems. Much of the current interest in conformal prediction is due to its ability to integrate into complex machine learning workflows, solving the problem of forming prediction sets without any assumptions on the form of the data generating distribution. Since contemporary machine learning algorithms have generally proven difficult to analyze directly, conformal prediction's main appeal is its ability to provide formal, finite-sample guarantees when paired with such methods. The goal of this book is to teach the reader about the fundamental technical arguments that arise when researching conformal prediction and related questions in distribution-free inference. Many of these proof strategies, especially the more recent ones, are scattered among research papers, making it difficult for researchers to understand where to look, which results are important, and how exactly the proofs work. We hope to bridge this gap by curating what we believe to be some of the most important results in the literature and presenting their proofs in a unified language, with illustrations, and with an eye towards pedagogy.},
	urldate = {2025-10-23},
	publisher = {arXiv},
	author = {Angelopoulos, Anastasios N. and Barber, Rina Foygel and Bates, Stephen},
	month = jun,
	year = {2025},
	note = {arXiv:2411.11824 [math]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology, Statistics - Statistics Theory},
}

@article{arnold_isotonic_2025,
	title = {Isotonic conditional laws},
	volume = {31},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-31/issue-2/Isotonic-conditional-laws/10.3150/24-BEJ1764.full},
	doi = {10.3150/24-BEJ1764},
	language = {en},
	number = {2},
	urldate = {2025-10-15},
	journal = {Bernoulli},
	author = {Arnold, Sebastian and Ziegel, Johanna},
	month = may,
	year = {2025},
}

@inproceedings{dheur_unified_2025,
	title = {A {Unified} {Comparative} {Study} with {Generalized} {Conformity} {Scores} for {Multi}-{Output} {Conformal} {Regression}},
	url = {https://proceedings.mlr.press/v267/dheur25a.html},
	abstract = {Conformal prediction provides a powerful framework for constructing distribution-free prediction regions with finite-sample coverage guarantees. While extensively studied in univariate settings, its extension to multi-output problems presents additional challenges, including complex output dependencies and high computational costs, and remains relatively underexplored. In this work, we present a unified comparative study of nine conformal methods with different multivariate base models for constructing multivariate prediction regions within the same framework. This study highlights their key properties while also exploring the connections between them. Additionally, we introduce two novel classes of conformity scores for multi-output regression that generalize their univariate counterparts. These scores ensure asymptotic conditional coverage while maintaining exact finite-sample marginal coverage. One class is compatible with any generative model, offering broad applicability, while the other is computationally efficient, leveraging the properties of invertible generative models. Finally, we conduct a comprehensive empirical evaluation across 13 tabular datasets, comparing all the multi-output conformal methods explored in this work. To ensure a fair and consistent comparison, all methods are implemented within a unified code base.},
	language = {en},
	urldate = {2025-10-15},
	booktitle = {Proceedings of the 42nd {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Dheur, Victor and Fontana, Matteo and Estievenart, Yorick and Desobry, Naomi and Taieb, Souhaib Ben},
	month = oct,
	year = {2025},
	note = {ISSN: 2640-3498},
	pages = {13444--13485},
}

@article{pini_multi-aspect_2019,
	series = {Special {Issue} on {Functional} {Data} {Analysis} and {Related} {Topics}},
	title = {Multi-aspect local inference for functional data: {Analysis} of ultrasound tongue profiles},
	volume = {170},
	issn = {0047-259X},
	shorttitle = {Multi-aspect local inference for functional data},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X1730711X},
	doi = {10.1016/j.jmva.2018.11.006},
	abstract = {Motivated by the analysis of a dataset of ultrasound tongue profiles, we present multi-aspect interval-wise testing (IWT), i.e., a local nonparametric inferential technique for functional data embedded in Sobolev spaces. Multi-aspect IWT is a nonparametric procedure that tests differences between groups of functional data, jointly taking into account the curves and their derivatives. Multi-aspect IWT provides adjusted multi-aspect p-value functions that can be used to select intervals of the domain that are imputable for the rejection of a null hypothesis. As a result, it can impute the rejection of a functional null hypothesis to specific intervals of the domain and to specific orders of differentiation. We show that the multi-aspect p-value functions are provided with a control of the family-wise error rate and that they are consistent. We apply multi-aspect IWT to the analysis of a dataset of tongue profiles recorded for a study on Tyrolean, a German dialect spoken in South Tyrol. We test differences between five different ways of articulating the uvular /r/: vocalized /r/, approximant, fricative, tap, and trill. Multi-aspect IWT-based comparisons result in an informative and detailed representation of the regions of the tongue where a significant difference occurs.},
	urldate = {2025-10-14},
	journal = {Journal of Multivariate Analysis},
	author = {Pini, Alessia and Spreafico, Lorenzo and Vantini, Simone and Vietti, Alessandro},
	month = mar,
	year = {2019},
	keywords = {Articulatory phonetics, Derivatives, Functional data analysis, Inference, Interval-wise error rate},
	pages = {162--185},
}

@article{english_janet_2025,
	title = {{JANET}: {Joint} {Adaptive} {predictioN}-region {Estimation} for {Time}-series},
	volume = {114},
	issn = {1573-0565},
	shorttitle = {{JANET}},
	url = {https://doi.org/10.1007/s10994-025-06812-2},
	doi = {10.1007/s10994-025-06812-2},
	abstract = {Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET’s superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.},
	language = {en},
	number = {8},
	urldate = {2025-09-17},
	journal = {Machine Learning},
	author = {English, Eshant and Wong-Toi, Eliot and Fontana, Matteo and Mandt, Stephan and Smyth, Padhraic and Lippert, Christoph},
	month = jun,
	year = {2025},
	keywords = {Conformal prediction, Joint prediction region, Time series uncertainty quantification},
	pages = {177},
}

@misc{noauthor_dap-2023-0097r3_proof_hi_nodate,
	title = {{DAP}-2023-0097.{R3}\_Proof\_hi (1).pdf},
}

@article{krelinova_forecasting_2025,
	title = {Forecasting displacement and solutions for decision-making in volatile contexts: a case study from {Ukraine}},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {Forecasting displacement and solutions for decision-making in volatile contexts},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/forecasting-displacement-and-solutions-for-decisionmaking-in-volatile-contexts-a-case-study-from-ukraine/12F16A1452D313B8715DDA24920EB71C},
	doi = {10.1017/dap.2024.46},
	abstract = {Following the large-scale Russian invasion in February 2022, policymakers and humanitarian actors urgently sought to anticipate displacement flows within Ukraine. However, existing internal displacement data systems had not been adapted to contexts as dynamic as a full-fledged war marked by uneven trigger events. A year and a half later, policymakers and practitioners continue to seek forecasts, needing to anticipate how many internally displaced persons (IDPs) can be expected to return to their areas of origin and how many will choose to stay and seek a durable solution in their place of displacement. This article presents a case study of an anticipatory approach deployed by the International Organization for Migration (IOM) Mission in Ukraine since March 2022, delivering nationwide displacement figures less than 3 weeks following the invasion alongside near real-time data on mobility intentions as well as key data anticipating the timing, direction, and volume of future flows and needs related to IDP return and (re)integration. The authors review pre-existing mobility forecasting approaches, then discuss practical experiences with mobility prediction applications in the Ukraine response using the Ukraine General Population Survey (GPS), including in program and policy design related to facilitating durable solutions to displacement. The authors focus on the usability and ethics of the approach, already considered for replication in other displacement contexts.},
	language = {en},
	urldate = {2025-09-16},
	journal = {Data \& Policy},
	author = {Krelinova, Karolina and Loktieva, Iryna and Jusselme, Damien},
	month = jan,
	year = {2025},
	keywords = {Ukraine, decision-making, displacement, durable solutions, forecasting},
	pages = {e55},
}

@article{hu_bayesian_2023,
	title = {Bayesian {Spatial} {Homogeneity} {Pursuit} of {Functional} {Data}: {An} {Application} to the {U}.{S}. {Income} {Distribution}},
	volume = {18},
	issn = {1936-0975, 1931-6690},
	shorttitle = {Bayesian {Spatial} {Homogeneity} {Pursuit} of {Functional} {Data}},
	url = {https://projecteuclid.org/journals/bayesian-analysis/volume-18/issue-2/Bayesian-Spatial-Homogeneity-Pursuit-of-Functional-Data--An-Application/10.1214/22-BA1320.full},
	doi = {10.1214/22-BA1320},
	abstract = {An income distribution describes how an entity’s total wealth is distributed amongst its population. A problem of interest to regional economics researchers is to understand the spatial homogeneity of income distributions among different regions. In economics, the Lorenz curve is a well-known functional representation of income distribution. In this article, we propose a mixture of finite mixtures (MFM) model as well as a Markov random field constrained mixture of finite mixtures (MRFC-MFM) model in the context of spatial functional data analysis to capture spatial homogeneity of Lorenz curves. We design efficient Markov chain Monte Carlo (MCMC) algorithms to simultaneously infer the posterior distributions of the number of clusters and the clustering configuration of spatial functional data. Extensive simulation studies are carried out to show the effectiveness of the proposed methods compared with existing methods. We apply the proposed spatial functional clustering method to state level income Lorenz curves from the American Community Survey Public Use Microdata Sample (PUMS) data. The results reveal a number of important clustering patterns of state-level income distributions across the US.},
	number = {2},
	urldate = {2025-07-31},
	journal = {Bayesian Analysis},
	author = {Hu, Guanyu and Geng, Junxian and Xue, Yishu and Sang, Huiyan},
	month = jun,
	year = {2023},
	note = {Publisher: International Society for Bayesian Analysis},
	keywords = {62P20, 91B72, Lorenz curve, Markov random field, mixture of finite mixtures, spatial functional data clustering},
	pages = {579--605},
}

@article{sen_time_2019,
	title = {Time series of functional data with application to yield curves},
	volume = {35},
	copyright = {© 2019 John Wiley \& Sons, Ltd.},
	issn = {1526-4025},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2443},
	doi = {10.1002/asmb.2443},
	abstract = {We develop time series analysis of functional data observed discretely, treating the whole curve as a random realization from a distribution on functions that evolve over time. The method consists of principal components analysis of functional data and subsequently modeling the principal component scores as vector autoregressive moving averag (VARMA) process. We justify the method by showing that an underlying ARMAH structure of the curves leads to a VARMA structure on the principal component scores. We derive asymptotic properties of the estimators, fits, and forecast. For term structures of interest rates, these provide a unified framework for studying the time and maturity components of interest rates under one setup with few parametric assumptions. We apply the method to the yield curves of USA and India. We compare our forecasts to the parametric model that is based on Nelson-Siegel curves. In another application, we study the dependence of long term interest rate on the short term interest rate using functional regression.},
	language = {en},
	number = {4},
	urldate = {2025-07-31},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Sen, Rituparna and Klüppelberg, Claudia},
	year = {2019},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/asmb.2443},
	keywords = {asymptotics, functional principal component, functional regression, prediction, vector ARMA},
	pages = {1028--1043},
}

@inproceedings{bogani_conformalise_2025,
	address = {Cham},
	title = {To {Conformalise} or {Not} to {Conformalise} in {Growth}-at-{Risk} {Computations}: {The} {Role} of {Model} {Dimensionality}},
	isbn = {978-3-031-96736-8},
	shorttitle = {To {Conformalise} or {Not} to {Conformalise} in {Growth}-at-{Risk} {Computations}},
	doi = {10.1007/978-3-031-96736-8_32},
	abstract = {The reliable estimation of extreme quantiles is vital in various practical domains, among them economic policy development and financial risk assessment. Accurate quantile estimation is of key importance in computing risk measures such as Growth-at-Risk (GaR). This work explores the use of a conformal prediction-based methodology to obtain calibrated quantile estimates. We demonstrate the efficacy of this framework through extensive simulations and a detailed empirical study focused on GaR. Our findings reveal that conformal prediction improves the calibration accuracy and robustness of quantile estimations in extreme scenarios, where conventional approaches exhibit weaknesses. This advancement equips practitioners with robust analytical tools for evaluating and managing the likelihood of future extreme events.},
	language = {en},
	booktitle = {Statistics for {Innovation} {I}},
	publisher = {Springer Nature Switzerland},
	author = {Bogani, Pietro and Fontana, Matteo and Neri, Luca and Vantini, Simone},
	editor = {di Bella, Enrico and Gioia, Vincenzo and Lagazio, Corrado and Zaccarin, Susanna},
	year = {2025},
	pages = {189--193},
}

@article{english_janet_2025-1,
	title = {{JANET}: {Joint} {Adaptive} {predictioN}-region {Estimation} for {Time}-series},
	copyright = {All rights reserved},
	shorttitle = {{JANET}},
	url = {http://arxiv.org/abs/2407.06390},
	doi = {10.48550/arXiv.2407.06390},
	abstract = {Conformal prediction provides machine learning models with prediction sets that offer theoretical guarantees, but the underlying assumption of exchangeability limits its applicability to time series data. Furthermore, existing approaches struggle to handle multi-step ahead prediction tasks, where uncertainty estimates across multiple future time points are crucial. We propose JANET (Joint Adaptive predictioN-region Estimation for Time-series), a novel framework for constructing conformal prediction regions that are valid for both univariate and multivariate time series. JANET generalises the inductive conformal framework and efficiently produces joint prediction regions with controlled K-familywise error rates, enabling flexible adaptation to specific application needs. Our empirical evaluation demonstrates JANET's superior performance in multi-step prediction tasks across diverse time series datasets, highlighting its potential for reliable and interpretable uncertainty quantification in sequential data.},
	urldate = {2024-08-26},
	journal = {Machine Learning, forthcoming},
	author = {English, Eshant and Wong-Toi, Eliot and Fontana, Matteo and Mandt, Stephan and Smyth, Padhraic and Lippert, Christoph},
	year = {2025},
	note = {arXiv:2407.06390 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{mccraken_fred-md_2015,
	title = {{FRED}-{MD} and {FRED}-{QD}: {Monthly} and {Quarterly} {Databases} for {Macroeconomic} {Research}},
	shorttitle = {{FRED}-{MD} and {FRED}-{QD}},
	url = {https://www.stlouisfed.org/research/economists/mccracken/fred-databases},
	abstract = {FRED-MD and FRED-QD are large macroeconomic databases designed for the empirical analysis of 'big data.'},
	language = {en},
	urldate = {2025-06-14},
	author = {McCraken, Micheal and Ng, Serena},
	year = {2015},
}

@misc{barigozzi_ea-md-qd_2024,
	title = {{EA}-{MD}-{QD}: {Large} {Euro} {Area} and {Euro} {Member} {Countries} {Datasets} for {Macroeconomic} {Research}},
	shorttitle = {{EA}-{MD}-{QD}},
	url = {https://zenodo.org/records/14245656},
	abstract = {EA-MD-QD is a collection of large monthly and quarterly EA and EA member countries datasets for macroeconomic analysis.The EA member countries covered are: AT, BE, DE, EL, ES, FR, IE, IT, NL, PT.

The formal reference to this dataset is: 

Barigozzi, M. and Lissona, C. (2024) "EA-MD-QD: Large Euro Area and Euro Member Countries Datasets for Macroeconomic Research". Zenodo.

Please refer to it when using the data.

Each zip file contains:- Excel files for the EA and the countries covered, each containing an unbalanced panel of raw de-seasonalized data.- A Matlab code taking as input the raw data and allowing to perform various operations such as:choose the frequency, fill-in missing values, transform data to stationarity, and control for covid outliers.- A pdf file with all informations about the series names, sources, and transformation codes.

This version (11.2024):

Updated data as of 29-November-2024},
	language = {eng},
	urldate = {2025-06-14},
	publisher = {Zenodo},
	author = {Barigozzi, Matteo and Lissona, Claudio},
	month = nov,
	year = {2024},
	keywords = {Euro Area, Factor Models, High-Dimensional Data, Macroeconometrics},
}

@book{lang_medium-term_2023,
	address = {LU},
	title = {Medium-term growth-at-risk in the euro area.},
	url = {https://data.europa.eu/doi/10.2866/581800},
	language = {en},
	urldate = {2025-06-14},
	publisher = {Publications Office},
	author = {Lang, Jan Hannes and Rusnak, Marek and Greiwe, Moritz},
	year = {2023},
}

@misc{tibshirani_conformal_2023,
	title = {Conformal {Prediction}: {Advanced} {Topics} in {Statistical} {Learning}},
	url = {https://www.stat.berkeley.edu/~ryantibs/statlearn-s23/lectures/conformal.pdf},
	urldate = {2025-06-14},
	author = {Tibshirani, Ryan J},
	year = {2023},
}

@misc{zaffran_distribution-free_2024,
	title = {Distribution-{Free} {Uncertainty} {Quantification} {A} short introduction to {Conformal} {Prediction}},
	url = {https://conformalpredictionintro.github.io/},
	urldate = {2025-06-14},
	author = {Zaffran, Margaux},
	year = {2024},
}

@misc{manokhin_awesome_2025,
	title = {Awesome {Conformal} {Prediction}},
	url = {https://github.com/valeman/awesome-conformal-prediction},
	abstract = {A professionally curated list of awesome Conformal Prediction videos, tutorials, books, papers, PhD and MSc theses, articles and open-source libraries.},
	urldate = {2025-06-14},
	author = {Manokhin, Valery},
	month = jun,
	year = {2025},
	doi = {10.5281/zenodo.6467204},
}

@book{european_central_bank_medium-term_2023,
	address = {LU},
	title = {Medium-term growth-at-risk in the euro area.},
	url = {https://data.europa.eu/doi/10.2866/581800},
	language = {en},
	urldate = {2025-06-14},
	publisher = {Publications Office},
	author = {{European Central Bank.}},
	year = {2023},
}

@misc{allen_-sample_2025,
	title = {In-sample calibration yields conformal calibration guarantees},
	url = {http://arxiv.org/abs/2503.03841},
	doi = {10.48550/arXiv.2503.03841},
	abstract = {Conformal predictive systems allow forecasters to issue predictive distributions for real-valued future outcomes that have out-of-sample calibration guarantees. On a more abstract level, conformal prediction makes use of in-sample calibration guarantees to construct bands of predictions with out-of-sample guarantees under exchangeability. The calibration guarantees are typically that prediction intervals derived from the predictive distributions have the correct marginal coverage. We extend this line of reasoning to stronger notions of calibration that are common in statistical forecasting theory. We take two prediction methods that are calibrated in-sample, and conformalize them to obtain conformal predictive systems with stronger out-of-sample calibration guarantees than existing approaches. The first method corresponds to a binning of the data, while the second leverages isotonic distributional regression (IDR), a non-parametric distributional regression method under order constraints. We study the theoretical properties of these new conformal predictive systems, and compare their performance in a simulation experiment. They are then applied to two case studies on European temperature forecasts and on predictions for the length of patient stay in Swiss intensive care units. Both approaches are found to outperform existing conformal predictive systems, while conformal IDR additionally provides a natural method for quantifying epistemic uncertainty of the predictions.},
	urldate = {2025-06-05},
	publisher = {arXiv},
	author = {Allen, Sam and Gavrilopoulos, Georgios and Henzi, Alexander and Kleger, Gian-Reto and Ziegel, Johanna},
	month = mar,
	year = {2025},
	note = {arXiv:2503.03841 [stat]},
	keywords = {Statistics - Methodology},
}

@article{bosco_machine_2024,
	title = {A {Machine} {Learning} architecture to forecast {Irregular} {Border} {Crossings} and {Asylum} requests for policy support in {Europe}: a case study},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {A {Machine} {Learning} architecture to forecast {Irregular} {Border} {Crossings} and {Asylum} requests for policy support in {Europe}},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/machine-learning-architecture-to-forecast-irregular-border-crossings-and-asylum-requests-for-policy-support-in-europe-a-case-study/45652144691DA4EA1BEC80BFA39BAFDC},
	doi = {10.1017/dap.2024.48},
	abstract = {Anticipating future migration trends is instrumental to the development of effective policies to manage the challenges and opportunities that arise from population movements. However, anticipation is challenging. Migration is a complex system, with multifaceted drivers, such as demographic structure, economic disparities, political instability, and climate change. Measurements encompass inherent uncertainties, and the majority of migration theories are either under-specified or hardly actionable. Moreover, approaches for forecasting generally target specific migration flows, and this poses challenges for generalisation.In this paper, we present the results of a case study to predict Irregular Border Crossings (IBCs) through the Central Mediterranean Route and Asylum requests in Italy. We applied a set of Machine Learning techniques in combination with a suite of traditional data to forecast migration flows. We then applied an ensemble modelling approach for aggregating the results of the different Machine Learning models to improve the modelling prediction capacity.Our results show the potential of this modelling architecture in producing forecasts of IBCs and Asylum requests over 6 months. The explained variance of our models through a validation set is as high as 80\%. This study offers a robust basis for the construction of timely forecasts. In the discussion, we offer a comment on how this approach could benefit migration management in the European Union at various levels of policy making.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Bosco, Claudio and Minora, Umberto and Rosińska, Anna and Teobaldelli, Maurizio and Belmonte, Martina},
	month = jan,
	year = {2024},
	keywords = {Machine Learning, forecasting, migration, policy support},
	pages = {e81},
}

@article{golesorkhi_migration_2024,
	title = {Migration scenarios for gender apartheid and asylum: when {International} {Criminal} {Law} and {International} {Refugee} {Law} {Meet}},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {Migration scenarios for gender apartheid and asylum},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/migration-scenarios-for-gender-apartheid-and-asylum-when-international-criminal-law-and-international-refugee-law-meet/CE5BEA3B876CC74698442ABC930ACD16},
	doi = {10.1017/dap.2024.51},
	abstract = {A multi-year process of debate around draft articles for a Crimes Against Humanity Treaty is underway and calls to categorize gender-based persecution as a stand-alone crime and to codify gender apartheid form fundamental aspects of discussion. These developments in international criminal law are significant to anticipate forced migration as recent changes in asylum regulations across the EU suggest. Between December 2022 and February 2023, Sweden, Finland, and Denmark moved to grant asylum to women and girls from Afghanistan on general risks of gender-based persecution. This falls in line with the EU Agency for Asylum establishing that the accumulation of repressive measures against women and girls in the country, which have been described as gender apartheid, amounts to persecution. In efforts to offer new perspectives on foresight in forced migration, I use case study method and legal-institutional analysis to delineate migration scenarios for gender apartheid and asylum. On the example of Afghanistan, I compare Sweden, Finland, and Denmark as case studies in which asylum is granted to women and girls on general risks of gender-based persecution in contrast to Germany and France as case studies for main destination countries of Afghan asylum-seekers absent of such policies. I explore factors towards policy in/action and provide outlooks for further lines of inquiry regarding anticipatory methods in forced migration.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Golesorkhi, Lara-Zuzan},
	month = jan,
	year = {2024},
	pages = {e77},
}

@article{casagran_developing_2024,
	title = {Developing {AI} predictive migration tools to enhance humanitarian support: {The} case of {EUMigraTool}},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {Developing {AI} predictive migration tools to enhance humanitarian support},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/developing-ai-predictive-migration-tools-to-enhance-humanitarian-support-the-case-of-eumigratool/54E3FF814CD44FF426272335AFDD76AE},
	doi = {10.1017/dap.2024.76},
	abstract = {The EUMigraTool (EMT) provides short-term and mid-term predictions of asylum seekers arriving in the European Union, drawing on multiple sources of public information and with a focus on human rights. After 3 years of development, it has been tested in real environments by 17 NGOs working with migrants in Spain, Italy, and Greece.This paper will first describe the functionalities, models, and features of the EMT. It will then analyze the main challenges and limitations of developing a tool for non-profit organizations, focusing on issues such as (1) the validation process and accuracy, and (2) the main ethical concerns, including the challenging exploitation plan when the main target group are NGOs.The overall purpose of this paper is to share the results and lessons learned from the creation of the EMT, and to reflect on the main elements that need to be considered when developing a predictive tool for assisting NGOs in the field of migration.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Casagran, Cristina Blasi and Stavropoulos, Georgios},
	month = jan,
	year = {2024},
	keywords = {Artificial intelligence, EUMigraTool, Migration, humanitarian tools, predictive tools},
	pages = {e64},
}

@article{udovyk_anticipating_2024,
	title = {Anticipating return migration to {Ukraine}: participatory foresight with {Ukrainians} displaced to {Spain}},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {Anticipating return migration to {Ukraine}},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/anticipating-return-migration-to-ukraine-participatory-foresight-with-ukrainians-displaced-to-spain/837DA699379A2AAD60A87D9C801A6598},
	doi = {10.1017/dap.2024.45},
	abstract = {In the context of the ongoing Russian invasion and the uncertainties surrounding the potential return migration of millions of displaced Ukrainians, this study explores the future of (return) migration through an innovative and inclusive participatory foresight approach, engaging 20 displaced Ukrainians residing in Valencia, Spain, from May to December 2023. The foresight process included workshops, discussions via online messaging groups, interviews, participatory observations, and culminated in an open art exhibition. Through this process, we conducted a collective horizon scanning, identifying weak signals and emerging trends, followed by an examination of critical uncertainties, which led to the development of four distinct scenarios: Exhaustion Return, Energetic Return, Virtual Return, and Disconnection. The insights derived from this foresight exercise hold practical relevance for both Ukrainian and EU migration policymakers, emphasizing the importance of lived experiences in shaping anticipatory migration policies. This study also offers theoretical contributions by applying participatory foresight to the field of return migration, challenging established knowledge paradigms, and fostering a more inclusive and nuanced understanding of migration dynamics and their broader implications.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Udovyk, Oksana and M-Domènech, Ruth},
	month = jan,
	year = {2024},
	keywords = {displaced, future scenarios, horizon scanning, migration, participation},
	pages = {e48},
}

@article{ruhnke_predicting_2024,
	title = {Predicting mobility aspirations in {Lebanon} and {Turkey}: a data-driven exploration using machine learning},
	volume = {6},
	issn = {2632-3249},
	shorttitle = {Predicting mobility aspirations in {Lebanon} and {Turkey}},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/predicting-mobility-aspirations-in-lebanon-and-turkey-a-datadriven-exploration-using-machine-learning/5584F38342A1CE3E4592EC93AF388885},
	doi = {10.1017/dap.2024.32},
	abstract = {The aspirations-ability framework proposed by Carling has begun to place the question of who aspires to migrate at the center of migration research. In this article, building on key determinants assumed to impact individual migration decisions, we investigate their prediction accuracy when observed in the same dataset and in different mixed-migration contexts. In particular, we use a rigorous model selection approach and develop a machine learning algorithm to analyze two original cross-sectional face-to-face surveys conducted in Turkey and Lebanon among Syrian migrants and their respective host populations in early 2021. Studying similar nationalities in two hosting contexts with a distinct history of both immigration and emigration and large shares of assumed-to-be mobile populations, we illustrate that a) (im)mobility aspirations are hard to predict even under ‘ideal’ methodological circumstances, b) commonly referenced “migration drivers” fail to perform well in predicting migration aspirations in our study contexts, while c) aspects relating to social cohesion, political representation and hope play an important role that warrants more emphasis in future research and policymaking. Methodologically, we identify key challenges in quantitative research on predicting migration aspirations and propose a novel modeling approach to address these challenges.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Ruhnke, Simon and Rischke, Ramona},
	month = jan,
	year = {2024},
	keywords = {(im)mobility aspirations, Backwards stepwise regression, Lasso-regression, Lebanon, Random Forest, Random Forest Algorithm, Syria, Turkey, aspirations-capability framework, machine learning, migration theory, prediction, refugees},
	pages = {e47},
}

@article{kjaerum_pushing_2025,
	title = {Pushing the boundaries of anticipatory action using machine learning},
	volume = {7},
	issn = {2632-3249},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/pushing-the-boundaries-of-anticipatory-action-using-machine-learning/5CA9D6358F29AE35C886B92492CFFF65},
	doi = {10.1017/dap.2024.88},
	abstract = {Displacement continues to increase at a global scale and is increasingly happening in complex, multicrisis settings, leading to more complex and deeper humanitarian needs. Humanitarian needs are therefore increasingly outgrowing the available humanitarian funding. Thus, responding to vulnerabilities before disaster strikes is crucial but anticipatory action is contingent on the ability to accurately forecast what will happen in the future. Forecasting and contingency planning are not new in the humanitarian sector, where scenario-building continues to be an exercise conducted in most humanitarian operations to strategically plan for coming events. However, the accuracy of these exercises remains limited. To address this challenge and work with the objective of providing the humanitarian sector with more accurate forecasts to enhance the protection of vulnerable groups, the Danish Refugee Council has already developed several machine learning models. The Anticipatory Humanitarian Action for Displacement uses machine learning to forecast displacement in subdistricts in the Liptako-Gourma region in Sahel, covering Burkina Faso, Mali, and Niger. The model is mainly built on data related to conflict, food insecurity, vegetation health, and the prevalence of underweight to forecast displacement. In this article, we will detail how the model works, the accuracy and limitations of the model, and how we are translating the forecasts into action by using them for anticipatory action in South Sudan and Burkina Faso, including concrete examples of activities that can be implemented ahead of displacement in the place of origin, along routes and in place of destination.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Kjærum, Alexander and Madsen, Bo S.},
	month = jan,
	year = {2025},
	keywords = {anticipatory action, displacement forecasting, humanitarian response, machine learning, predictive analytics},
	pages = {e8},
}

@article{aydogdu_mobile_2025,
	title = {Mobile phone data for anticipating displacements: practices, opportunities, and challenges},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {Mobile phone data for anticipating displacements},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/mobile-phone-data-for-anticipating-displacements-practices-opportunities-and-challenges/3CEF3CA3289EA57100B298FCF203D3A9},
	doi = {10.1017/dap.2024.94},
	abstract = {The global number of individuals experiencing forced displacement has reached its highest level in the past decade. In this context, the provision of services for those in need requires timely and evidence-based approaches. How can mobile phone data (MPD) based analyses address the knowledge gap on mobility patterns and needs assessments in forced displacement settings? To answer this question, in this paper, we examine the capacity of MPD to function as a tool for anticipatory analysis, particularly in response to natural disasters and conflicts that lead to internal or cross-border displacement. The paper begins with a detailed review of the processes involved in acquiring, processing, and analyzing MPD in forced displacement settings. Following this, we critically assess the challenges associated with employing MPD in policy-making, with a specific focus on issues of user privacy and data ethics. The paper concludes by evaluating the potential benefits of MPD analysis for targeted and effective policy interventions and discusses future research avenues, drawing on recent studies and ongoing collaborations with mobile network operators.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Aydoğdu, Bilgeçağ and Bilgili, Özge and Güneş, Subhi and Salah, Albert Ali},
	month = jan,
	year = {2025},
	keywords = {Conflict management, Data collaboratives, Disaster management, Ethical data sharing, Flows, Migration indicators, Mobile phone data, Stocks},
	pages = {e5},
}

@article{marcucci_when_2025,
	title = {When forecasting and foresight meet data and innovation: toward a taxonomy of anticipatory methods for migration policy},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {When forecasting and foresight meet data and innovation},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/when-forecasting-and-foresight-meet-data-and-innovation-toward-a-taxonomy-of-anticipatory-methods-for-migration-policy/493E652497F13C2653BC9323F4DE9D60},
	doi = {10.1017/dap.2024.56},
	abstract = {The various global refugee and migration events of the last few years underscore the need for advancing anticipatory strategies in migration policy. The struggle to manage large inflows (or outflows) highlights the demand for proactive measures based on a sense of the future. Anticipatory methods, ranging from predictive models to foresight techniques, emerge as valuable tools for policymakers. These methods, now bolstered by advancements in technology and leveraging nontraditional data sources, can offer a pathway to develop more precise, responsive, and forward-thinking policies.This paper seeks to map out the rapidly evolving domain of anticipatory methods in the realm of migration policy, capturing the trend toward integrating quantitative and qualitative methodologies and harnessing novel tools and data. It introduces a new taxonomy designed to organize these methods into three core categories: Experience-based, Exploration-based, and Expertise-based. This classification aims to guide policymakers in selecting the most suitable methods for specific contexts or questions, thereby enhancing migration policies.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Marcucci, Sara and Verhulst, Stefaan and Cervantes, María Esther},
	month = jan,
	year = {2025},
	keywords = {anticipatory methods, migration, nontraditional data, policy making, taxonomy},
	pages = {e24},
}

@article{martin_anticipating_2025,
	title = {Anticipating climate change-related mobility in {Karachi} and {Ho} {Chi} {Minh} {City}: lessons from a hybrid foresight approach},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {Anticipating climate change-related mobility in {Karachi} and {Ho} {Chi} {Minh} {City}},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/anticipating-climate-changerelated-mobility-in-karachi-and-ho-chi-minh-city-lessons-from-a-hybrid-foresight-approach/032B5739EFD7EA65AA5DE61AEF261163},
	doi = {10.1017/dap.2025.2},
	abstract = {Climate change exacerbates existing risks and vulnerabilities for people globally, and migration is a longstanding adaptation response to climate risk. The mechanisms through which climate change shapes human mobility are complex, however, and gaps in data and knowledge persist. In response to these gaps, the United Nations Development Programme’s (UNDP) Predictive Analytics, Human Mobility, and Urbanization Project employed a hybrid approach that combined predictive analytics with participatory foresight to explore climate change-related mobility in Pakistan and Viet Nam from 2020 to 2050. Focusing on Karachi and Ho Chi Minh City, the project estimated temporal and spatial mobility patterns under different climate change scenarios and evaluated the impact of such in-migration across key social, political, economic, and environmental domains. Findings indicate that net migration into these cities could significantly increase under extreme climate scenarios, highlighting both the complex spatial patterns of population change and the potential for anticipatory policies to mitigate these impacts. While extensive research exists on foresight methods and theory, process reflections are underrepresented. The innovative approach employed within this project offers valuable insights on foresight exercise design choices and their implications for effective stakeholder engagement, as well as the applicability and transferability of insights in support of policymaking. Beyond substantive findings, this paper offers a critical reflection on the methodological alignment of data-driven and participatory foresight with the aim of anticipatory policy ideation, seeking to contribute to the enhanced effectiveness of foresight practices.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Martin, Aaron and Kistemaker, Boukje and Allen, Beth and Jones, Bryan},
	month = jan,
	year = {2025},
	keywords = {anticipatory policy, climate change, human mobility, predictive analytics, strategic foresight},
	pages = {e13},
}

@article{barker_mixed-frequency_2025,
	title = {Mixed-frequency {VAR}: a new approach to forecasting migration in {Europe} using macroeconomic data},
	volume = {7},
	issn = {2632-3249},
	shorttitle = {Mixed-frequency {VAR}},
	url = {https://www.cambridge.org/core/journals/data-and-policy/article/mixedfrequency-var-a-new-approach-to-forecasting-migration-in-europe-using-macroeconomic-data/4ADD7BB416B912BB2AD9BEC1F6495ED0},
	doi = {10.1017/dap.2024.82},
	abstract = {Forecasting international migration is a challenge that, despite its political and policy salience, has seen a limited success so far. In this proof-of-concept paper, we employ a range of macroeconomic data to represent different drivers of migration. We also take into account the relatively consistent set of migration policies within the European Common Market, with its constituent freedom of movement of labour. Using panel vector autoregressive (VAR) models for mixed-frequency data, we forecast migration in the short- and long-term horizons for 26 of the 32 countries within the Common Market. We demonstrate how the methodology can be used to assess the possible responses of other macroeconomic variables to unforeseen migration events—and vice versa. Our results indicate reasonable in-sample performance of migration forecasts, especially in the short term, although with varying levels of accuracy. They also underline the need for taking country-specific factors into account when constructing forecasting models, with different variables being important across the regions of Europe. For the longer term, the proposed methods, despite high prediction errors, can still be useful as tools for setting coherent migration scenarios and analysing responses to exogenous shocks.},
	language = {en},
	urldate = {2025-05-16},
	journal = {Data \& Policy},
	author = {Barker, Emily R. and Bijak, Jakub},
	month = jan,
	year = {2025},
	keywords = {Bayesian forecasting, migration, mixed-frequency models, panel VAR},
	pages = {e3},
}

@article{gneiting_probabilistic_2007,
	title = {Probabilistic forecasts, calibration and sharpness},
	volume = {69},
	issn = {1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2007.00587.x},
	doi = {10.1111/j.1467-9868.2007.00587.x},
	abstract = {Summary. Probabilistic forecasts of continuous variables take the form of predictive densities or predictive cumulative distribution functions. We propose a diagnostic approach to the evaluation of predictive performance that is based on the paradigm of maximizing the sharpness of the predictive distributions subject to calibration. Calibration refers to the statistical consistency between the distributional forecasts and the observations and is a joint property of the predictions and the events that materialize. Sharpness refers to the concentration of the predictive distributions and is a property of the forecasts only. A simple theoretical framework allows us to distinguish between probabilistic calibration, exceedance calibration and marginal calibration. We propose and study tools for checking calibration and sharpness, among them the probability integral transform histogram, marginal calibration plots, the sharpness diagram and proper scoring rules. The diagnostic approach is illustrated by an assessment and ranking of probabilistic forecasts of wind speed at the Stateline wind energy centre in the US Pacific Northwest. In combination with cross-validation or in the time series context, our proposal provides very general, nonparametric alternatives to the use of information criteria for model diagnostics and model selection.},
	language = {en},
	number = {2},
	urldate = {2025-04-24},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Gneiting, Tilmann and Balabdaoui, Fadoua and Raftery, Adrian E.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2007.00587.x},
	keywords = {Cross-validation, Density forecast, Ensemble prediction system, Ex post evaluation, Forecast verification, Model diagnostics, Posterior predictive assessment, Predictive distribution, Prequential principle, Probability integral transform, Proper scoring rule},
	pages = {243--268},
}

@article{mokkadem_mixing_1988,
	title = {Mixing properties of {ARMA} processes},
	volume = {29},
	issn = {0304-4149},
	url = {https://www.sciencedirect.com/science/article/pii/0304414988900452},
	doi = {10.1016/0304-4149(88)90045-2},
	abstract = {In this paper, we show that stationary vector ARMA processes are geometrically completely regular, and hence geometrically strong mixing, provided the innovations have absolutely continuous distribution with respect to Lebesgue measure.},
	number = {2},
	urldate = {2025-04-24},
	journal = {Stochastic Processes and their Applications},
	author = {Mokkadem, Abdelkader},
	month = sep,
	year = {1988},
	keywords = {ARMA process, Markov chain, complete regularity, geometric ergodicity},
	pages = {309--315},
}

@article{shiraishi_time_2024,
	title = {Time {Series} {Quantile} {Regression} {Using} {Random} {Forests}},
	volume = {45},
	copyright = {© 2024 The Authors. Journal of Time Series Analysis published by John Wiley \& Sons Ltd.},
	issn = {1467-9892},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsa.12731},
	doi = {10.1111/jtsa.12731},
	abstract = {We discuss an application of Generalized Random Forests (GRF) proposed to quantile regression for time series data. We extended the theoretical results of the GRF consistency for i.i.d. data to time series data. In particular, in the main theorem, based only on the general assumptions for time series data and trees, we show that the tsQRF (time series Quantile Regression Forest) estimator is consistent. Compare with existing article, different ideas are used throughout the theoretical proof. In addition, a simulation and real data analysis were conducted. In the simulation, the accuracy of the conditional quantile estimation was evaluated under time series models. In the real data using the Nikkei Stock Average, our estimator is demonstrated to capture volatility more efficiently, thus preventing underestimation of uncertainty.},
	language = {en},
	number = {4},
	urldate = {2025-04-10},
	journal = {Journal of Time Series Analysis},
	author = {Shiraishi, Hiroshi and Nakamura, Tomoshige and Shibuki, Ryotato},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jtsa.12731},
	keywords = {Quantile regression, nonlinear autoregressive model, random forest},
	pages = {639--659},
}

@article{gneiting_regression_2023,
	title = {Regression diagnostics meets forecast evaluation: conditional calibration, reliability diagrams, and coefficient of determination},
	volume = {17},
	issn = {1935-7524, 1935-7524},
	shorttitle = {Regression diagnostics meets forecast evaluation},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-17/issue-2/Regression-diagnostics-meets-forecast-evaluation--conditional-calibration-reliability-diagrams/10.1214/23-EJS2180.full},
	doi = {10.1214/23-EJS2180},
	abstract = {A common principle in model diagnostics and forecast evaluation is that fitted or predicted distributions ought to be reliable, ideally in the sense of auto-calibration, where the outcome is a random draw from the posited distribution. For binary responses, auto-calibration is the universal concept of reliability. For real-valued outcomes, a general theory of calibration has been elusive, despite a recent surge of interest in distributional regression and machine learning. We develop a framework rooted in probability theory, which gives rise to hierarchies of calibration, and applies to both predictive distributions and stand-alone point forecasts. In a nutshell, a prediction is conditionally T-calibrated if it can be taken at face value in terms of an identifiable functional T. We introduce population versions of T-reliability diagrams and revisit a score decomposition into measures of miscalibration, discrimination, and uncertainty. In empirical settings, stable and efficient estimators of T-reliability diagrams and score components arise via nonparametric isotonic regression and the pool-adjacent-violators algorithm. For in-sample model diagnostics, we propose a universal coefficient of determination that nests and reinterprets the classical R2 in least squares regression and its natural analog R1 in quantile regression, yet applies to T-regression in general.},
	number = {2},
	urldate = {2025-04-01},
	journal = {Electronic Journal of Statistics},
	author = {Gneiting, Tilmann and Resin, Johannes},
	month = jan,
	year = {2023},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62G99, 62J20, Calibration test, canonical loss, consistent scoring function, model diagnostics, nonparametric isotonic regression, prequential principle, score decomposition, skill score},
	pages = {3226--3286},
}

@article{baviera_daily_2024,
	title = {Daily middle-term probabilistic forecasting of power consumption in {North}-{East} {England}},
	volume = {15},
	issn = {1868-3975},
	url = {https://doi.org/10.1007/s12667-023-00577-0},
	doi = {10.1007/s12667-023-00577-0},
	abstract = {Probabilistic forecasting of power consumption in a middle-term horizon (few months to a year) is a main challenge in the energy sector. It plays a key role in planning future generation plants and transmission grid. This paper proposes a novel model that (i) incorporates seasonality and autoregressive features in a traditional time-series analysis and (ii) includes weather conditions in a parsimonious machine learning approach, known as Gaussian Process. Applying to a daily power consumption dataset in North East England, provided by one of the largest energy suppliers, we obtain promising results in Out-of-Sample density forecasts up to one year, even using a small dataset, with only a two-year calibration set. For the evaluation of the achieved probabilistic forecasts, we consider the pinball loss—a metric common in the energy sector—and we assess the coverage—a procedure standard in the banking sector after the introduction of Basel II Accords—also running the conditional and unconditional tests for probability intervals. Results show that the proposed model outperforms benchmarks in terms of both accuracy and reliability.},
	language = {en},
	number = {4},
	urldate = {2025-04-01},
	journal = {Energy Systems},
	author = {Baviera, Roberto and Messuti, Giuseppe},
	month = nov,
	year = {2024},
	keywords = {C14, C51, C53, Gaussian process, Machine learning, Middle-term, Power consumption, Probabilistic forecast, Q47},
	pages = {1595--1617},
}

@inproceedings{chernozhukov_exact_2018,
	title = {Exact and {Robust} {Conformal} {Inference} {Methods} for {Predictive} {Machine} {Learning} with {Dependent} {Data}},
	url = {https://proceedings.mlr.press/v75/chernozhukov18a.html},
	abstract = {We extend conformal inference to general settings that allow for time series data. Our proposal is developed as a randomization method and  accounts for potential serial dependence by including  block structures in the permutation scheme. As a result, the proposed method retains the exact, model-free validity when the data are i.i.d. or more generally exchangeable, similar to usual conformal inference methods.  When exchangeability fails, as is the case for common time series data, the proposed approach is approximately valid under weak assumptions on the conformity score.},
	language = {en},
	urldate = {2025-03-25},
	booktitle = {Proceedings of the 31st  {Conference} {On} {Learning} {Theory}},
	publisher = {PMLR},
	author = {Chernozhukov, Victor and Wüthrich, Kaspar and Yinchu, Zhu},
	month = jul,
	year = {2018},
	note = {ISSN: 2640-3498},
	pages = {732--749},
}

@article{koenker_quantile_2006,
	title = {Quantile {Autoregression}},
	volume = {101},
	issn = {0162-1459},
	url = {https://doi.org/10.1198/016214506000000672},
	doi = {10.1198/016214506000000672},
	abstract = {We consider quantile autoregression (QAR) models in which the autoregressive coefficients can be expressed as monotone functions of a single, scalar random variable. The models can capture systematic influences of conditioning variables on the location, scale, and shape of the conditional distribution of the response, and thus constitute a significant extension of classical constant coefficient linear time series models in which the effect of conditioning is confined to a location shift. The models may be interpreted as a special case of the general random-coefficient autoregression model with strongly dependent coefficients. Statistical properties of the proposed model and associated estimators are studied. The limiting distributions of the autoregression quantile process are derived. QAR inference methods are also investigated. Empirical applications of the model to the U.S. unemployment rate, short-term interest rate, and gasoline prices highlight the model's potential.},
	number = {475},
	urldate = {2025-03-25},
	journal = {Journal of the American Statistical Association},
	author = {Koenker, Roger and and Xiao, Zhijie},
	month = sep,
	year = {2006},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1198/016214506000000672},
	keywords = {Asymmetric persistence, Autoregression, Comonotonicity, Quantile, Random coefficients},
	pages = {980--990},
}

@article{lenza_density_2023,
	title = {Density {Forecasts} of {Inflation}: {A} {Quantile} {Regression} {Forest} {Approach}},
	issn = {1556-5068},
	shorttitle = {Density {Forecasts} of {Inflation}},
	url = {https://www.ssrn.com/abstract=4511273},
	doi = {10.2139/ssrn.4511273},
	language = {en},
	urldate = {2025-03-25},
	journal = {SSRN Electronic Journal},
	author = {Lenza, Michele and Moutachaker, Inès and Paredes, Joan},
	year = {2023},
}

@book{european_central_bank_economic_2021,
	address = {LU},
	title = {Economic predictions with big data: the illusion of sparsity.},
	shorttitle = {Economic predictions with big data},
	url = {https://data.europa.eu/doi/10.2866/703510},
	language = {en},
	urldate = {2025-03-25},
	publisher = {Publications Office},
	author = {{European Central Bank.}},
	year = {2021},
}

@misc{lang_aifs_2024,
	title = {{AIFS} -- {ECMWF}'s data-driven forecasting system},
	url = {https://arxiv.org/abs/2406.01465v2},
	abstract = {Machine learning-based weather forecasting models have quickly emerged as a promising methodology for accurate medium-range global weather forecasting. Here, we introduce the Artificial Intelligence Forecasting System (AIFS), a data driven forecast model developed by the European Centre for Medium-Range Weather Forecasts (ECMWF). AIFS is based on a graph neural network (GNN) encoder and decoder, and a sliding window transformer processor, and is trained on ECMWF's ERA5 re-analysis and ECMWF's operational numerical weather prediction (NWP) analyses. It has a flexible and modular design and supports several levels of parallelism to enable training on high-resolution input data. AIFS forecast skill is assessed by comparing its forecasts to NWP analyses and direct observational data. We show that AIFS produces highly skilled forecasts for upper-air variables, surface weather parameters and tropical cyclone tracks. AIFS is run four times daily alongside ECMWF's physics-based NWP model and forecasts are available to the public under ECMWF's open data policy.},
	language = {en},
	urldate = {2025-03-09},
	journal = {arXiv.org},
	author = {Lang, Simon and Alexe, Mihai and Chantry, Matthew and Dramsch, Jesper and Pinault, Florian and Raoult, Baudouin and Clare, Mariana C. A. and Lessig, Christian and Maier-Gerber, Michael and Magnusson, Linus and Bouallègue, Zied Ben and Nemesio, Ana Prieto and Dueben, Peter D. and Brown, Andrew and Pappenberger, Florian and Rabier, Florence},
	month = jun,
	year = {2024},
}

@article{chen_generative_2024,
	title = {Generative machine learning methods for multivariate ensemble postprocessing},
	volume = {18},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/journals/annals-of-applied-statistics/volume-18/issue-1/Generative-machine-learning-methods-for-multivariate-ensemble-postprocessing/10.1214/23-AOAS1784.full},
	doi = {10.1214/23-AOAS1784},
	abstract = {Ensemble weather forecasts based on multiple runs of numerical weather prediction models typically show systematic errors and require postprocessing to obtain reliable forecasts. Accurately modeling multivariate dependencies is crucial in many practical applications, and various approaches to multivariate postprocessing have been proposed where ensemble predictions are first postprocessed separately in each margin and multivariate dependencies are then restored via copulas. These two-step methods share common key limitations, in particular, the difficulty to include additional predictors in modeling the dependencies. We propose a novel multivariate postprocessing method based on generative machine learning to address these challenges. In this new class of nonparametric data-driven distributional regression models, samples from the multivariate forecast distribution are directly obtained as output of a generative neural network. The generative model is trained by optimizing a proper scoring rule, which measures the discrepancy between the generated and observed data, conditional on exogenous input variables. Our method does not require parametric assumptions on univariate distributions or multivariate dependencies and allows for incorporating arbitrary predictors. In two case studies on multivariate temperature and wind speed forecasting at weather stations over Germany, our generative model shows significant improvements over state-of-the-art methods and particularly improves the representation of spatial dependencies.},
	number = {1},
	urldate = {2025-03-09},
	journal = {The Annals of Applied Statistics},
	author = {Chen, Jieyu and Janke, Tim and Steinke, Florian and Lerch, Sebastian},
	month = mar,
	year = {2024},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Generative machine learning, ensemble postprocessing, multivariate postprocessing, probabilistic forecasting, weather forecasting},
	pages = {159--183},
}

@article{papamakarios_normalizing_2021,
	title = {Normalizing flows for probabilistic modeling and inference},
	volume = {22},
	issn = {1532-4435},
	abstract = {Normalizing flows provide a general mechanism for defining expressive probability distributions, only requiring the specification of a (usually simple) base distribution and a series of bijective transformations. There has been much recent work on normalizing flows, ranging from improving their expressive power to expanding their application. We believe the field has now matured and is in need of a unified perspective. In this review, we attempt to provide such a perspective by describing flows through the lens of probabilistic modeling and inference. We place special emphasis on the fundamental principles of flow design, and discuss foundational topics such as expressive power and computational trade-offs. We also broaden the conceptual framing of flows by relating them to more general probability transformations. Lastly, we summarize the use of flows for tasks such as generative modeling, approximate inference, and supervised learning.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Papamakarios, George and Nalisnick, Eric and Rezende, Danilo Jimenez and Mohamed, Shakir and Lakshminarayanan, Balaji},
	month = jan,
	year = {2021},
	pages = {57:2617--57:2680},
}

@article{cauchois_knowing_2021,
	title = {Knowing what {You} {Know}: valid and validated confidence sets in multiclass and multilabel prediction},
	volume = {22},
	issn = {1533-7928},
	shorttitle = {Knowing what {You} {Know}},
	url = {http://jmlr.org/papers/v22/20-753.html},
	abstract = {We develop conformal prediction methods for constructing valid predictive  confidence sets in multiclass and multilabel problems without assumptions on the data generating distribution. A challenge here is that typical conformal prediction methods---which give marginal validity (coverage) guarantees---provide uneven coverage, in that they address easy examples at the expense of essentially ignoring difficult examples.  By leveraging ideas from quantile regression, we build methods that always guarantee correct coverage but additionally provide (asymptotically consistent) conditional coverage for both multiclass and multilabel prediction problems. To address the potential challenge of exponentially large confidence sets in multilabel prediction, we build tree-structured classifiers that efficiently account for interactions between labels.  Our methods can be bolted on top of any classification model---neural network, random forest, boosted tree---to guarantee its validity.  We also provide an empirical evaluation, simultaneously providing new validation methods, that suggests the more robust coverage of our confidence sets.},
	number = {81},
	urldate = {2025-03-04},
	journal = {Journal of Machine Learning Research},
	author = {Cauchois, Maxime and Gupta, Suyash and Duchi, John C.},
	year = {2021},
	pages = {1--42},
}

@inproceedings{zhou_conformalized_2024,
	address = {Vienna, Austria},
	series = {{ICML}'24},
	title = {Conformalized adaptive forecasting of heterogeneous trajectories},
	volume = {235},
	abstract = {This paper presents a new conformal method for generating simultaneous forecasting bands guaranteed to cover the entire path of a new random trajectory with sufficiently high probability. Prompted by the need for dependable uncertainty estimates in motion planning applications where the behavior of diverse objects may be more or less unpredictable, we blend different techniques from online conformal prediction of single and multiple time series, as well as ideas for addressing heteroscedasticity in regression. This solution is both principled, providing precise finite-sample guarantees, and effective, often leading to more informative predictions than prior methods.},
	urldate = {2025-03-04},
	booktitle = {Proceedings of the 41st {International} {Conference} on {Machine} {Learning}},
	publisher = {JMLR.org},
	author = {Zhou, Yanfei and Lindemann, Lars and Sesia, Matteo},
	month = jul,
	year = {2024},
	pages = {62002--62056},
}

@inproceedings{wang_probabilistic_2023,
	title = {Probabilistic {Conformal} {Prediction} {Using} {Conditional} {Random} {Samples}},
	url = {https://proceedings.mlr.press/v206/wang23n.html},
	abstract = {This paper proposes probabilistic conformal prediction (PCP), a predictive inference algorithm that estimates a target variable by a discontinuous predictive set. Given inputs, PCP constructs the predictive set based on random samples from an estimated generative model. It is efficient and compatible with conditional generative models with either explicit or implicit density functions. We show that PCP guarantees correct marginal coverage with finite samples and give empirical evidence of conditional coverage. We study PCP on a variety of simulated and real datasets. Compared to existing conformal prediction methods, PCP provides sharper predictive sets.},
	language = {en},
	urldate = {2025-03-04},
	booktitle = {Proceedings of {The} 26th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Wang, Zhendong and Gao, Ruijiang and Yin, Mingzhang and Zhou, Mingyuan and Blei, David},
	month = apr,
	year = {2023},
	note = {ISSN: 2640-3498},
	pages = {8814--8836},
}

@article{sadinle_least_2019,
	title = {Least {Ambiguous} {Set}-{Valued} {Classifiers} {With} {Bounded} {Error} {Levels}},
	volume = {114},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1395341},
	doi = {10.1080/01621459.2017.1395341},
	abstract = {In most classification tasks, there are observations that are ambiguous and therefore difficult to correctly label. Set-valued classifiers output sets of plausible labels rather than a single label, thereby giving a more appropriate and informative treatment to the labeling of ambiguous instances. We introduce a framework for multiclass set-valued classification, where the classifiers guarantee user-defined levels of coverage or confidence (the probability that the true label is contained in the set) while minimizing the ambiguity (the expected size of the output). We first derive oracle classifiers assuming the true distribution to be known. We show that the oracle classifiers are obtained from level sets of the functions that define the conditional probability of each class. Then we develop estimators with good asymptotic and finite sample properties. The proposed estimators build on existing single-label classifiers. The optimal classifier can sometimes output the empty set, but we provide two solutions to fix this issue that are suitable for various practical needs. Supplementary materials for this article are available online.},
	number = {525},
	urldate = {2025-03-04},
	journal = {Journal of the American Statistical Association},
	author = {Sadinle, Mauricio and Lei, Jing and Wasserman, Larry},
	month = jan,
	year = {2019},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2017.1395341},
	keywords = {Ambiguous observation, Bayes classifier, Multiclass classification, Nondeterministic classifier, Oracle classifier, Reject option},
	pages = {223--234},
}

@inproceedings{saunders_transduction_1999,
	title = {Transduction with {Conﬁdence} and {Credibility}},
	abstract = {In this paper we follow the same general ideology as in [Gammerman et al., 1998], and describe a new transductive learning algorithm using Support Vector Machines. The algorithm presented provides conﬁdence values for its predicted classiﬁcations of new examples. We also obtain a measure of “credibility” which serves as an indicator of the reliability of the data upon which we make our prediction. Experiments compare the new algorithm to a standard Support Vector Machine and other transductive methods which use Support Vector Machines, such as Vapnik’s margin transduction. Empirical results show that the new algorithm not only produces conﬁdence and credibility measures, but is comparable to, and sometimes exceeds the performance of the other algorithms.},
	language = {en},
	author = {Saunders, C and Gammerman, A and Vovk, V},
	year = {1999},
	pages = {722--726},
}

@article{hernandez_simultaneous_2024,
	title = {Simultaneous predictive bands for functional time series using minimum entropy sets},
	issn = {0361-0918},
	url = {https://www.tandfonline.com/doi/full/10.1080/03610918.2024.2391869},
	doi = {10.1080/03610918.2024.2391869},
	urldate = {2025-01-27},
	journal = {Communications in Statistics - Simulation and Computation},
	author = {Hernández, Nicolás and Cugliari, Jairo and Jacques, Julien},
	year = {2024},
	note = {Publisher: Taylor \& Francis},
	keywords = {37M10, 62G, 62M, Autoregressive Hilbertian process, Bootstrap, Entropy, Functional time series, Predictive bands, RKHS},
	pages = {1--25},
}

@article{hwang_uncertainty_2024,
	title = {Uncertainty quantification using {Gaussian} processes for topographic speed-up factors from {CFD} simulations},
	url = {https://www.nist.gov/publications/uncertainty-quantification-using-gaussian-processes-topographic-speed-factors-cfd},
	abstract = {Computational fluid dynamics (CFD) simulation has become increasingly popular for evaluating the topographic effects on wind fields due to its relative advantag},
	language = {en},
	urldate = {2025-01-24},
	journal = {NIST},
	author = {Hwang, Yunjae and Pintar, Adam L. and Yeo, DongHun},
	month = jul,
	year = {2024},
	note = {Last Modified: 2024-07-31T18:07-04:00
Publisher: Yunjae Hwang, Adam L. Pintar, DongHun Yeo},
}

@misc{noauthor_creating_nodate,
	title = {Creating {Low}-{Cost} {Soil} {Maps} for {Tropical} {Agriculture} using {Gaussian} {Processes}},
	url = {https://www.ri.cmu.edu/publications/creating-low-cost-soil-maps-for-tropical-agriculture-using-gaussian-processes/},
	abstract = {Soil maps are essential resources to soil scientists and researchers in any fields related to soil, land use, species conservation, hunger reduction, social development, etc. However, creating detailed soil maps is an expensive and time consuming task that most developing nations cannot afford. In recent years, there has been a significant shift towards digital representation […]},
	language = {en-US},
	urldate = {2025-01-24},
	journal = {Robotics Institute Carnegie Mellon University},
}

@misc{gruber_sources_2023,
	title = {Sources of {Uncertainty} in {Machine} {Learning} -- {A} {Statisticians}' {View}},
	url = {http://arxiv.org/abs/2305.16703},
	doi = {10.48550/arXiv.2305.16703},
	abstract = {Machine Learning and Deep Learning have achieved an impressive standard today, enabling us to answer questions that were inconceivable a few years ago. Besides these successes, it becomes clear, that beyond pure prediction, which is the primary strength of most supervised machine learning algorithms, the quantification of uncertainty is relevant and necessary as well. While first concepts and ideas in this direction have emerged in recent years, this paper adopts a conceptual perspective and examines possible sources of uncertainty. By adopting the viewpoint of a statistician, we discuss the concepts of aleatoric and epistemic uncertainty, which are more commonly associated with machine learning. The paper aims to formalize the two types of uncertainty and demonstrates that sources of uncertainty are miscellaneous and can not always be decomposed into aleatoric and epistemic. Drawing parallels between statistical concepts and uncertainty in machine learning, we also demonstrate the role of data and their influence on uncertainty.},
	urldate = {2025-01-19},
	publisher = {arXiv},
	author = {Gruber, Cornelia and Schenk, Patrick Oliver and Schierholz, Malte and Kreuter, Frauke and Kauermann, Göran},
	month = may,
	year = {2023},
	note = {arXiv:2305.16703 [stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{romano_conformalized_2019,
	title = {Conformalized {Quantile} {Regression}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper_files/paper/2019/hash/5103c3584b063c431bd1268e9b5e76fb-Abstract.html},
	abstract = {Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.},
	urldate = {2025-01-14},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Romano, Yaniv and Patterson, Evan and Candes, Emmanuel},
	year = {2019},
}

@misc{moller_multivariate_2014,
	type = {Dissertation},
	title = {Multivariate and spatial ensemble postprocessing methods},
	copyright = {info:eu-repo/semantics/openAccess},
	url = {https://archiv.ub.uni-heidelberg.de/volltextserver/17066/},
	abstract = {In the recent past the state of the art in meteorology has been to produce weather forecasts from ensemble prediction systems. Forecast ensembles are generated from multiple runs of dynamical numerical weather prediction models, each with different initial and boundary conditions or parameterizations of the model.
However, ensemble forecasts are not able to catch the full uncertainty of numerical weather predictions and therefore often display biases and dispersion errors and thus are uncalibrated. To account for this problem, statistical postprocessing methods have been developed successfully. However, many state of the art methods are designed for a single weather quantity at a fixed location and for a fixed forecast horizon.
This work introduces extensions of two established univariate postprocessing methods, Bayesian model averaging (BMA) and Ensemble model output statistics (EMOS) to recover inter-variable and spatial dependencies from the original ensemble forecasts. For this purpose, a multi-stage procedure is proposed that can be applied for modeling dependence structures between different weather quantities as well as modeling spatial or temporal dependencies. This multi-stage procedure combines the postprocessing of the margins by the application of a univariate method as BMA or EMOS with a multivariate dependence structure, for example via a correlation matrix or via the multivariate rank structure of the original ensemble.
The multivariate postprocessing procedure that models inter-variable dependence employs the UWME 8-member forecast ensemble over the North West region of the US and the standard BMA method, resulting in predictive distributions with good multivariate calibration and sharpness.
The spatial postprocessing procedure is applied to temperature forecasts of the ECMWF 50-member ensemble over Germany. The procedure employs a spatially adaptive extension of EMOS, utilizing recently proposed methods for fast and accurate Bayesian estimation in a spatial setting. It yields excellent spatial univariate and multivariate calibration and sharpness. Further the method is able to capture the spatial structure of observed weather fields.
Both extensions improve calibration and sharpness in comparison to the raw ensemble and to the respective standard univariate postprocessing methods.},
	language = {eng},
	urldate = {2025-01-08},
	author = {Möller, Annette},
	year = {2014},
	doi = {10.11588/heidok.00017066},
}

@article{jobst_time-series-based_2024,
	title = {Time-series-based ensemble model output statistics for temperature forecasts postprocessing},
	volume = {150},
	copyright = {© 2024 The Author(s). Quarterly Journal of the Royal Meteorological Society published by John Wiley \& Sons Ltd on behalf of Royal Meteorological Society.},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.4844},
	doi = {10.1002/qj.4844},
	abstract = {The uncertainty in numerical weather prediction models is nowadays quantified by the use of ensemble forecasts. Although these forecasts are continuously improved, they still suffer from systematic bias and dispersion errors. Statistical postprocessing methods, such as the ensemble model output statistics (EMOS), have been shown to substantially correct the forecasts. This work proposes an extension of EMOS in a time-series framework. Besides taking account of seasonality and trend in the location and scale parameter of the predictive distribution, the autoregressive process in the mean forecast errors or the standardized forecast errors is considered. The models can be further extended by allowing generalized autoregressive conditional heteroscedasticity. Furthermore, it is outlined how to use these models for arbitrary forecast horizons. To illustrate the performance of the suggested EMOS models in time-series fashion, we present a case study for the postprocessing of 2 m surface temperature forecasts using five different lead times and a set of observation stations in Germany. The results indicate that the time-series EMOS extensions are able to significantly outperform the benchmark models EMOS and autoregressive EMOS (AR-EMOS) in most of the lead time–station cases.},
	language = {en},
	number = {765},
	urldate = {2025-01-08},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Jobst, David and Möller, Annette and Groß, Jürgen},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4844},
	keywords = {autoregressive process, ensemble model output statistics, ensemble postprocessing, generalized autoregressive conditional heteroscedasticity, probabilistic forecasting, temperature, time series models},
	pages = {4838--4855},
}

@article{gneiting_calibrated_2005,
	title = {Calibrated {Probabilistic} {Forecasting} {Using} {Ensemble} {Model} {Output} {Statistics} and {Minimum} {CRPS} {Estimation}},
	url = {https://journals.ametsoc.org/view/journals/mwre/133/5/mwr2904.1.xml},
	doi = {10.1175/MWR2904.1},
	abstract = {Ensemble prediction systems typically show positive spread-error correlation, but they are subject to forecast bias and dispersion errors, and are therefore uncalibrated. This work proposes the use of ensemble model output statistics (EMOS), an easy-to-implement postprocessing technique that addresses both forecast bias and underdispersion and takes into account the spread-skill relationship. The technique is based on multiple linear regression and is akin to the superensemble approach that has traditionally been used for deterministic-style forecasts. The EMOS technique yields probabilistic forecasts that take the form of Gaussian predictive probability density functions (PDFs) for continuous weather variables and can be applied to gridded model output. The EMOS predictive mean is a bias-corrected weighted average of the ensemble member forecasts, with coefficients that can be interpreted in terms of the relative contributions of the member models to the ensemble, and provides a highly competitive deterministic-style forecast. The EMOS predictive variance is a linear function of the ensemble variance. For fitting the EMOS coefficients, the method of minimum continuous ranked probability score (CRPS) estimation is introduced. This technique finds the coefficient values that optimize the CRPS for the training data. The EMOS technique was applied to 48-h forecasts of sea level pressure and surface temperature over the North American Pacific Northwest in spring 2000, using the University of Washington mesoscale ensemble. When compared to the bias-corrected ensemble, deterministic-style EMOS forecasts of sea level pressure had root-mean-square error 9\% less and mean absolute error 7\% less. The EMOS predictive PDFs were sharp, and much better calibrated than the raw ensemble or the bias-corrected ensemble.},
	language = {en},
	urldate = {2025-01-08},
	author = {Gneiting, Tilmann and Raftery, Adrian E. and Westveld, Anton H. and Goldman, Tom},
	month = may,
	year = {2005},
	note = {Section: Monthly Weather Review},
}

@misc{eisenbach_hail_2023,
	title = {Hail record broken again – 19cm hailstone confirmed in {Italy} {\textbar} {European} {Severe} {Storms} {Laboratory}},
	url = {https://www.essl.org/cms/hail-record-broken-again-19cm-hailstone-confirmed-in-italy/},
	language = {en-US},
	urldate = {2025-01-08},
	author = {Eisenbach, Stefan},
	month = jul,
	year = {2023},
}

@article{noauthor_devastating_2024,
	title = {Devastating rainfall hits {Spain} in yet another flood-related disaster},
	url = {https://wmo.int/media/news/devastating-rainfall-hits-spain-yet-another-flood-related-disaster},
	abstract = {The Valencia region was worst affected, with many places receiving more than 300 l/m². On 29/30 October, a weather station in Chiva received 491 l/m² in just eight hours - the equivalent of a year's worth of rainfall, according to AEMET - Agencia Estatal de Meteorología.},
	language = {en},
	urldate = {2025-01-08},
	journal = {World Meteorological Organization},
	month = oct,
	year = {2024},
}

@article{tondo_land_2024,
	chapter = {Environment},
	title = {‘{The} land is becoming desert’: drought pushes {Sicily}’s farming heritage to the brink},
	issn = {0261-3077},
	shorttitle = {‘{The} land is becoming desert’},
	url = {https://www.theguardian.com/environment/article/2024/aug/19/the-land-is-becoming-desert-drought-pushes-sicilys-farming-heritage-to-the-brink},
	abstract = {While tourists flock to the Italian island in greater numbers, a water crisis is intensifying for its rural population},
	language = {en-GB},
	urldate = {2025-01-08},
	journal = {The Guardian},
	author = {Tondo, Lorenzo},
	month = aug,
	year = {2024},
	keywords = {Climate crisis, Drought, Environment, Europe, Italy, Overtourism, Water},
}

@article{bian_training-conditional_2023,
	title = {Training-conditional coverage for distribution-free predictive inference},
	volume = {17},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-17/issue-2/Training-conditional-coverage-for-distribution-free-predictive-inference/10.1214/23-EJS2145.full},
	doi = {10.1214/23-EJS2145},
	abstract = {The field of distribution-free predictive inference provides tools for provably valid prediction without any assumptions on the distribution of the data, which can be paired with any regression algorithm to provide accurate and reliable predictive intervals. The guarantees provided by these methods are typically marginal, meaning that predictive accuracy holds on average over both the training data set and the test point that is queried. However, it may be preferable to obtain a stronger guarantee of training-conditional coverage, which would ensure that most draws of the training data set result in accurate predictive accuracy on future test points. This property is known to hold for the split conformal prediction method. In this work, we examine the training-conditional coverage properties of several other distribution-free predictive inference methods, and find that training-conditional coverage is achieved by some methods but is impossible to guarantee without further assumptions for others.},
	number = {2},
	urldate = {2024-12-10},
	journal = {Electronic Journal of Statistics},
	author = {Bian, Michael and Barber, Rina Foygel},
	month = jan,
	year = {2023},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62F40, 62G08, 62G15, conformal prediction, distribution-free inference, jackknife+},
	pages = {2044--2066},
}

@book{koenker_quantile_2005,
	address = {Cambridge},
	series = {Econometric {Society} {Monographs}},
	title = {Quantile {Regression}},
	isbn = {978-0-521-84573-1},
	url = {https://www.cambridge.org/core/books/quantile-regression/C18AE7BCF3EC43C16937390D44A328B1},
	abstract = {Quantile regression is gradually emerging as a unified statistical methodology for estimating models of conditional quantile functions. By complementing the exclusive focus of classical least squares regression on the conditional mean, quantile regression offers a systematic strategy for examining how covariates influence the location, scale and shape of the entire response distribution. This monograph is the first comprehensive treatment of the subject, encompassing models that are linear and nonlinear, parametric and nonparametric. The author has devoted more than 25 years of research to this topic. The methods in the analysis are illustrated with a variety of applications from economics, biology, ecology and finance. The treatment will find its core audiences in econometrics, statistics, and applied mathematics in addition to the disciplines cited above.},
	urldate = {2024-12-10},
	publisher = {Cambridge University Press},
	author = {Koenker, Roger},
	year = {2005},
	doi = {10.1017/CBO9780511754098},
}

@article{thrampoulidis_precise_2018,
	title = {Precise {Error} {Analysis} of {Regularized} {M} -{Estimators} in {High} {Dimensions}},
	volume = {64},
	issn = {1557-9654},
	url = {https://ieeexplore.ieee.org/document/8365826/?arnumber=8365826},
	doi = {10.1109/TIT.2018.2840720},
	abstract = {A popular approach for estimating an unknown signal x0 ∈ ℝn from noisy, linear measurements y = Ax0 + z ∈ ℝm is via solving a so called regularized M-estimator: x̂ := arg minx £(y - Ax) + λf(x). Here, £ is a convex loss function, f is a convex (typically, non-smooth) regularizer, and λ {\textgreater} 0 is a regularizer parameter. We analyze the squared error performance ∥x̂-x0∥22 of such estimators in the high-dimensional proportional regime where m, n → ∞ and m/n → δ. The design matrix A is assumed to have entries iid Gaussian; only minimal and rather mild regularity conditions are imposed on the loss function, the regularizer, and on the noise and signal distributions. We show that the squared error converges in probability to a nontrivial limit that is given as the solution to a minimax convex-concave optimization problem on four scalar optimization variables. We identify a new summary parameter, termed the expected Moreau envelope to play a central role in the error characterization. The precise nature of the results permits an accurate performance comparison between different instances of regularized M-estimators and allows to optimally tune the involved parameters (such as the regularizer parameter and the number of measurements). The key ingredient of our proof is the convex Gaussian min-max theorem which is a tight and strengthened version of a classical Gaussian comparison inequality that was proved by Gordon in 1988.},
	number = {8},
	urldate = {2024-12-02},
	journal = {IEEE Transactions on Information Theory},
	author = {Thrampoulidis, Christos and Abbasi, Ehsan and Hassibi, Babak},
	month = aug,
	year = {2018},
	note = {Conference Name: IEEE Transactions on Information Theory},
	keywords = {Compressed sensing, Electrical engineering, Estimation theory, Inverse problems, Measurement uncertainty, Noise measurement, Optimization, Standards, compressed sensing, convex optimization, gaussian processes, phase transitions},
	pages = {5592--5628},
}

@misc{timans_adaptive_2024,
	title = {Adaptive {Bounding} {Box} {Uncertainties} via {Two}-{Step} {Conformal} {Prediction}},
	url = {http://arxiv.org/abs/2403.07263},
	doi = {10.48550/arXiv.2403.07263},
	abstract = {Quantifying a model's predictive uncertainty is essential for safety-critical applications such as autonomous driving. We consider quantifying such uncertainty for multi-object detection. In particular, we leverage conformal prediction to obtain uncertainty intervals with guaranteed coverage for object bounding boxes. One challenge in doing so is that bounding box predictions are conditioned on the object's class label. Thus, we develop a novel two-step conformal approach that propagates uncertainty in predicted class labels into the uncertainty intervals of bounding boxes. This broadens the validity of our conformal coverage guarantees to include incorrectly classified objects, thus offering more actionable safety assurances. Moreover, we investigate novel ensemble and quantile regression formulations to ensure the bounding box intervals are adaptive to object size, leading to a more balanced coverage. Validating our two-step approach on real-world datasets for 2D bounding box localization, we find that desired coverage levels are satisfied with practically tight predictive uncertainty intervals.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Timans, Alexander and Straehle, Christoph-Nikolas and Sakmann, Kaspar and Nalisnick, Eric},
	month = jul,
	year = {2024},
	note = {arXiv:2403.07263},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@misc{park_semiparametric_2024,
	title = {Semiparametric conformal prediction},
	url = {http://arxiv.org/abs/2411.02114},
	doi = {10.48550/arXiv.2411.02114},
	abstract = {Many risk-sensitive applications require well-calibrated prediction sets over multiple, potentially correlated target variables, for which the prediction algorithm may report correlated non-conformity scores. In this work, we treat the scores as random vectors and aim to construct the prediction set accounting for their joint correlation structure. Drawing from the rich literature on multivariate quantiles and semiparametric statistics, we propose an algorithm to estimate the \$1-{\textbackslash}alpha\$ quantile of the scores, where \${\textbackslash}alpha\$ is the user-specified miscoverage rate. In particular, we flexibly estimate the joint cumulative distribution function (CDF) of the scores using nonparametric vine copulas and improve the asymptotic efficiency of the quantile estimate using its influence function. The vine decomposition allows our method to scale well to a large number of targets. We report desired coverage and competitive efficiency on a range of real-world regression problems, including those with missing-at-random labels in the calibration set.},
	urldate = {2024-12-02},
	publisher = {arXiv},
	author = {Park, Ji Won and Tibshirani, Robert and Cho, Kyunghyun},
	month = nov,
	year = {2024},
	note = {arXiv:2411.02114},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@inproceedings{bai_understanding_2021,
	title = {Understanding the {Under}-{Coverage} {Bias} in {Uncertainty} {Estimation}},
	volume = {34},
	url = {https://proceedings.neurips.cc/paper_files/paper/2021/hash/9854d7afce413aa13cd0a1d39d0bcec5-Abstract.html},
	urldate = {2024-11-29},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Bai, Yu and Mei, Song and Wang, Huan and Xiong, Caiming},
	year = {2021},
	pages = {18307--18319},
}

@article{chronopoulos_forecasting_2024,
	title = {Forecasting {Value}-at-{Risk} {Using} {Deep} {Neural} {Network} {Quantile} {Regression}*},
	volume = {22},
	issn = {1479-8409},
	url = {https://doi.org/10.1093/jjfinec/nbad014},
	doi = {10.1093/jjfinec/nbad014},
	abstract = {In this article, we use a deep quantile estimator, based on neural networks and their universal approximation property to examine a non-linear association between the conditional quantiles of a dependent variable and predictors. This methodology is versatile and allows both the use of different penalty functions, as well as high dimensional covariates. We present a Monte Carlo exercise where we examine the finite sample properties of the deep quantile estimator and show that it delivers good finite sample performance. We use the deep quantile estimator to forecast value-at-risk and find significant gains over linear quantile regression alternatives and other models, which are supported by various testing schemes. Further, we consider also an alternative architecture that allows the use of mixed frequency data in neural networks. This article also contributes to the interpretability of neural network output by making comparisons between the commonly used Shapley Additive Explanation values and an alternative method based on partial derivatives.},
	number = {3},
	urldate = {2024-11-28},
	journal = {Journal of Financial Econometrics},
	author = {Chronopoulos, Ilias and Raftapostolos, Aristeidis and Kapetanios, George},
	month = jul,
	year = {2024},
	pages = {636--669},
}

@article{meister_j_a_conformalised_2023,
	title = {Conformalised data synthesis with statistical quality guarantees},
	journal = {Machine Learning},
	author = {Meister, J. A. and Nguyen, Khuong An},
	year = {2023},
	pages = {1--32},
}

@article{chernozhukov_extremal_2005,
	title = {Extremal quantile regression},
	volume = {33},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-33/issue-2/Extremal-quantile-regression/10.1214/009053604000001165.full},
	doi = {10.1214/009053604000001165},
	abstract = {Quantile regression is an important tool for estimation of conditional quantiles of a response Y given a vector of covariates X. It can be used to measure the effect of covariates not only in the center of a distribution, but also in the upper and lower tails. This paper develops a theory of quantile regression in the tails. Specifically, it obtains the large sample properties of extremal (extreme order and intermediate order) quantile regression estimators for the linear quantile regression model with the tails restricted to the domain of minimum attraction and closed under tail equivalence across regressor values. This modeling setup combines restrictions of extreme value theory with leading homoscedastic and heteroscedastic linear specifications of regression analysis. In large samples, extreme order regression quantiles converge weakly to arg min functionals of stochastic integrals of Poisson processes that depend on regressors, while intermediate regression quantiles and their functionals converge to normal vectors with variance matrices dependent on the tail parameters and the regressor design.},
	number = {2},
	urldate = {2024-11-22},
	journal = {The Annals of Statistics},
	author = {Chernozhukov, Victor},
	month = apr,
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62E30, 62G30, 62G32, 62J05, 62P20, Conditional quantile estimation, Extreme value theory, regression},
	pages = {806--839},
}

@article{steinwart_estimating_2011,
	title = {Estimating conditional quantiles with the help of the pinball loss},
	volume = {17},
	issn = {1350-7265},
	url = {https://projecteuclid.org/journals/bernoulli/volume-17/issue-1/Estimating-conditional-quantiles-with-the-help-of-the-pinball-loss/10.3150/10-BEJ267.full},
	doi = {10.3150/10-BEJ267},
	abstract = {The so-called pinball loss for estimating conditional quantiles is a well-known tool in both statistics and machine learning. So far, however, only little work has been done to quantify the efficiency of this tool for nonparametric approaches. We fill this gap by establishing inequalities that describe how close approximate pinball risk minimizers are to the corresponding conditional quantile. These inequalities, which hold under mild assumptions on the data-generating distribution, are then used to establish so-called variance bounds, which recently turned out to play an important role in the statistical analysis of (regularized) empirical risk minimization approaches. Finally, we use both types of inequalities to establish an oracle inequality for support vector machines that use the pinball loss. The resulting learning rates are min–max optimal under some standard regularity assumptions on the conditional quantile.},
	number = {1},
	urldate = {2024-11-18},
	journal = {Bernoulli},
	author = {Steinwart, Ingo and Christmann, Andreas},
	month = feb,
	year = {2011},
	note = {Publisher: Bernoulli Society for Mathematical Statistics and Probability},
	keywords = {Nonparametric regression, Quantile estimation, Support vector machines},
	pages = {211--225},
}

@article{angrist_quantile_2006,
	title = {Quantile {Regression} under {Misspecification}, with an {Application} to the {U}.{S}. {Wage} {Structure}},
	volume = {74},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-0262.2006.00671.x},
	doi = {10.1111/j.1468-0262.2006.00671.x},
	abstract = {Quantile regression (QR) fits a linear model for conditional quantiles just as ordinary least squares (OLS) fits a linear model for conditional means. An attractive feature of OLS is that it gives the minimum mean-squared error linear approximation to the conditional expectation function even when the linear model is misspecified. Empirical research using quantile regression with discrete covariates suggests that QR may have a similar property, but the exact nature of the linear approximation has remained elusive. In this paper, we show that QR minimizes a weighted mean-squared error loss function for specification error. The weighting function is an average density of the dependent variable near the true conditional quantile. The weighted least squares interpretation of QR is used to derive an omitted variables bias formula and a partial quantile regression concept, similar to the relationship between partial regression and OLS. We also present asymptotic theory for the QR process under misspecification of the conditional quantile function. The approximation properties of QR are illustrated using wage data from the U.S. census. These results point to major changes in inequality from 1990 to 2000.},
	language = {en},
	number = {2},
	urldate = {2024-11-18},
	journal = {Econometrica},
	author = {Angrist, Joshua and Chernozhukov, Victor and Fernández-Val, Iván},
	year = {2006},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1468-0262.2006.00671.x},
	keywords = {Conditional quantile function, best linear predictor, income distribution, wage inequality},
	pages = {539--563},
}

@misc{bogani_calibrated_2024,
	title = {Calibrated quantile prediction for {Growth}-at-{Risk}},
	url = {http://arxiv.org/abs/2411.00520},
	doi = {10.48550/arXiv.2411.00520},
	abstract = {Accurate computation of robust estimates for extremal quantiles of empirical distributions is an essential task for a wide range of applicative fields, including economic policymaking and the financial industry. Such estimates are particularly critical in calculating risk measures, such as Growth-at-Risk (GaR). \% and Value-at-Risk (VaR). This work proposes a conformal framework to estimate calibrated quantiles, and presents an extensive simulation study and a real-world analysis of GaR to examine its benefits with respect to the state of the art. Our findings show that CP methods consistently improve the calibration and robustness of quantile estimates at all levels. The calibration gains are appreciated especially at extremal quantiles, which are critical for risk assessment and where traditional methods tend to fall short. In addition, we introduce a novel property that guarantees coverage under the exchangeability assumption, providing a valuable tool for managing risks by quantifying and controlling the likelihood of future extreme observations.},
	urldate = {2024-11-08},
	publisher = {arXiv},
	author = {Bogani, Pietro and Fontana, Matteo and Neri, Luca and Vantini, Simone},
	month = nov,
	year = {2024},
	note = {arXiv:2411.00520},
	keywords = {Economics - Econometrics, Statistics - Methodology},
}

@book{wainwright_high-dimensional_2019,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {High-{Dimensional} {Statistics}: {A} {Non}-{Asymptotic} {Viewpoint}},
	isbn = {978-1-108-49802-9},
	shorttitle = {High-{Dimensional} {Statistics}},
	url = {https://www.cambridge.org/core/books/highdimensional-statistics/8A91ECEEC38F46DAB53E9FF8757C7A4E},
	abstract = {Recent years have witnessed an explosion in the volume and variety of data collected in all scientific disciplines and industrial settings. Such massive data sets present a number of challenges to researchers in statistics and machine learning. This book provides a self-contained introduction to the area of high-dimensional statistics, aimed at the first-year graduate level. It includes chapters that are focused on core methodology and theory - including tail bounds, concentration inequalities, uniform laws and empirical process, and random matrices - as well as chapters devoted to in-depth exploration of particular model classes - including sparse linear models, matrix models with rank constraints, graphical models, and various types of non-parametric models. With hundreds of worked examples and exercises, this text is intended both for courses and for self-study by graduate students and researchers in statistics, machine learning, and related fields who must understand, apply, and adapt modern statistical methods suited to large-scale data.},
	urldate = {2024-11-08},
	publisher = {Cambridge University Press},
	author = {Wainwright, Martin J.},
	year = {2019},
	doi = {10.1017/9781108627771},
}

@article{del_barrio_nonparametric_2024,
	title = {Nonparametric {Multiple}-{Output} {Center}-{Outward} {Quantile} {Regression}},
	volume = {0},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2024.2366029},
	doi = {10.1080/01621459.2024.2366029},
	abstract = {Building on recent measure-transportation-based concepts of multivariate quantiles, we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content, the graphs of which constitute nested center-outward quantile regression tubes with given unconditional probability content; these (conditional and unconditional) probability contents do not depend on the underlying distribution—an essential property of quantile concepts. Empirical counterparts of these concepts are constructed, yielding interpretable empirical contours, regions, and tubes which are shown to consistently reconstruct (in the Pompeiu-Hausdorff topology) their population versions. Our method is entirely nonparametric and performs well in simulations—with possible heteroscedasticity and nonlinear trends. Its potential as a data-analytic tool is illustrated on some real datasets. Supplementary materials for this article are available online, including a standardized description of the materials available for reproducing the work.},
	number = {0},
	urldate = {2024-11-06},
	journal = {Journal of the American Statistical Association},
	author = {del Barrio, Eustasio and Sanz, Alberto González and Hallin, Marc},
	year = {2024},
	note = {Publisher: ASA Website
\_eprint: https://doi.org/10.1080/01621459.2024.2366029},
	keywords = {Center-outward quantiles, Multiple-output regression, Optimal transport},
	pages = {1--15},
}

@article{henzi_isotonic_2021,
	title = {Isotonic {Distributional} {Regression}},
	volume = {83},
	issn = {1369-7412},
	url = {https://doi.org/10.1111/rssb.12450},
	doi = {10.1111/rssb.12450},
	abstract = {Isotonic distributional regression (IDR) is a powerful non-parametric technique for the estimation of conditional distributions under order restrictions. In a nutshell, IDR learns conditional distributions that are calibrated, and simultaneously optimal relative to comprehensive classes of relevant loss functions, subject to isotonicity constraints in terms of a partial order on the covariate space. Non-parametric isotonic quantile regression and non-parametric isotonic binary regression emerge as special cases. For prediction, we propose an interpolation method that generalizes extant specifications under the pool adjacent violators algorithm. We recommend the use of IDR as a generic benchmark technique in probabilistic forecast problems, as it does not involve any parameter tuning nor implementation choices, except for the selection of a partial order on the covariate space. The method can be combined with subsample aggregation, with the benefits of smoother regression functions and gains in computational efficiency. In a simulation study, we compare methods for distributional regression in terms of the continuous ranked probability score (CRPS) and L2 estimation error, which are closely linked. In a case study on raw and post-processed quantitative precipitation forecasts from a leading numerical weather prediction system, IDR is competitive with state of the art techniques.},
	number = {5},
	urldate = {2024-11-06},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Henzi, Alexander and Ziegel, Johanna F. and Gneiting, Tilmann},
	month = nov,
	year = {2021},
	pages = {963--993},
}

@article{oliveira_split_2024,
	title = {Split {Conformal} {Prediction} and {Non}-{Exchangeable} {Data}},
	volume = {25},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v25/23-1553.html},
	abstract = {Split conformal prediction (CP) is arguably the most popular CP method for uncertainty quantification, enjoying both academic interest and widespread deployment. However, the original theoretical analysis of split CP makes the crucial assumption of data exchangeability, which hinders many real-world applications. In this paper, we present a novel theoretical framework based on concentration inequalities and decoupling properties of the data, proving that split CP remains valid for many non-exchangeable processes by adding a small coverage penalty. Through experiments with both real and synthetic data, we show that our theoretical results translate to good empirical performance under non-exchangeability, e.g., for time series and spatiotemporal data. Compared to recent conformal algorithms designed to counter specific exchangeability violations, we show that split CP is competitive in terms of coverage and interval size, with the benefit of being extremely simple and orders of magnitude faster than alternatives.},
	number = {225},
	urldate = {2024-10-30},
	journal = {Journal of Machine Learning Research},
	author = {Oliveira, Roberto I. and Orenstein, Paulo and Ramos, Thiago and Romano, João Vitor},
	year = {2024},
	pages = {1--38},
}

@book{mcneil_quantitative_2010,
	address = {New Jersey},
	series = {Princeton {Series} in {Finance}},
	title = {Quantitative {Risk} {Management}: {Concepts}, {Techniques}, and {Tools}},
	isbn = {978-0-691-12255-7 978-1-4008-3757-1},
	shorttitle = {Quantitative {Risk} {Management}},
	language = {eng},
	publisher = {Princeton University Press},
	author = {McNeil, Alexander J. and Frey, Rüdiger and Embrechts, Paul and Frey, R. Diger},
	year = {2010},
}

@article{meinshausen_quantile_2006,
	title = {Quantile {Regression} {Forests}},
	volume = {7},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v7/meinshausen06a.html},
	abstract = {Random forests were introduced as a machine learning tool 
in Breiman (2001) and have
since proven to be very popular and powerful for high-dimensional 
regression and classification. 
For regression, random forests give an accurate approximation of the
conditional mean of a response variable. 
It is shown here that random forests provide information about
the full conditional distribution of the response variable, not only
about the conditional mean. Conditional quantiles can be inferred with
quantile regression forests, a generalisation of random forests.
Quantile regression forests give a non-parametric and accurate
way of estimating conditional quantiles for high-dimensional predictor
variables. 
The algorithm is shown to be consistent. Numerical examples suggest that
the algorithm is competitive in terms of predictive power.},
	number = {35},
	urldate = {2024-10-30},
	journal = {Journal of Machine Learning Research},
	author = {Meinshausen, Nicolai},
	year = {2006},
	pages = {983--999},
}

@article{koenker_regression_1978,
	title = {Regression {Quantiles}},
	volume = {46},
	issn = {0012-9682},
	url = {https://www.jstor.org/stable/1913643},
	doi = {10.2307/1913643},
	abstract = {A simple minimization problem yielding the ordinary sample quantiles in the location model is shown to generalize naturally to the linear model generating a new class of statistics we term "regression quantiles." The estimator which minimizes the sum of absolute residuals is an important special case. Some equivariance properties and the joint asymptotic distribution of regression quantiles are established. These results permit a natural generalization of the linear model of certain well-known robust estimators of location. Estimators are suggested, which have comparable efficiency to least squares for Gaussian linear models while substantially out-performing the least-squares estimator over a wide class of non-Gaussian error distributions.},
	number = {1},
	urldate = {2024-10-30},
	journal = {Econometrica},
	author = {Koenker, Roger and Bassett, Gilbert},
	year = {1978},
	note = {Publisher: [Wiley, Econometric Society]},
	pages = {33--50},
}

@article{stock_testing_1988,
	title = {Testing for {Common} {Trends}},
	volume = {83},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1988.10478707},
	doi = {10.1080/01621459.1988.10478707},
	abstract = {Cointegrated multiple time series share at least one common trend. Two tests are developed for the number of common stochastic trends (i.e., for the order of cointegration) in a multiple time series with and without drift. Both tests involve the roots of the ordinary least squares coefficient matrix obtained by regressing the series onto its first lag. Critical values for the tests are tabulated, and their power is examined in a Monte Carlo study. Economic time series are often modeled as having a unit root in their autoregressive representation, or (equivalently) as containing a stochastic trend. But both casual observation and economic theory suggest that many series might contain the same stochastic trends so that they are cointegrated. If each of n series is integrated of order 1 but can be jointly characterized by k {\textgreater} n stochastic trends, then the vector representation of these series has k unit roots and n — k distinct stationary linear combinations. Our proposed tests can be viewed alternatively as tests of the number of common trends, linearly independent cointegrating vectors, or autoregressive unit roots of the vector process. Both of the proposed tests are asymptotically similar. The first test (qf ) is developed under the assumption that certain components of the process have a finite-order vector autoregressive (VAR) representation, and the nuisance parameters are handled by estimating this VAR. The second test (qc ) entails computing the eigenvalues of a corrected sample first-order autocorrelation matrix, where the correction is essentially a sum of the autocovariance matrices. Previous researchers have found that U.S. postwar interest rates, taken individually, appear to be integrated of order 1. In addition, the theory of the term structure implies that yields on similar assets of different maturities will be cointegrated. Applying these tests to postwar U.S. data on the federal funds rate and the three- and twelve-month treasury bill rates provides support for this prediction: The three interest rates appear to be cointegrated.},
	number = {404},
	urldate = {2024-10-30},
	journal = {Journal of the American Statistical Association},
	author = {Stock, James H. and Watson, Mark W.},
	month = dec,
	year = {1988},
	note = {Publisher: ASA Website
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1988.10478707},
	keywords = {Cointegration, Factor models, Integrated processes, Multiple time series, Unit roots, Yield curve},
	pages = {1097--1107},
}

@misc{noauthor_national_nodate,
	title = {National {Financial} {Conditions} {Index}: {Current} {Data} - {Federal} {Reserve} {Bank} of {Chicago}},
	shorttitle = {National {Financial} {Conditions} {Index}},
	url = {https://www.chicagofed.org/research/data/nfci/current-data},
	language = {en},
	urldate = {2024-10-30},
}

@book{international_monetary_fund_global_2017,
	address = {Washington, D.C},
	series = {Global {Financial} {Stability} {Report}},
	title = {Global {Financial} {Stability} {Report}, {October} 2017: {Is} {Growth} at {Risk}?},
	isbn = {978-1-4843-0839-4},
	shorttitle = {Global {Financial} {Stability} {Report}, {October} 2017},
	abstract = {The October 2017 Global Financial Stability Report finds that the global financial system continues to strengthen in response to extraordinary policy support, regulatory enhancements, and the cyclical upturn in growth. It also includes a chapter that examines the short- and medium-term implications for economic growth and financial stability of the past decades' rise in household debt. It documents large differences in household debt-to-GDP ratios across countries but a common increasing trajectory that was moderated but not reversed by the global financial crisis. Another chapter develops a new macroeconomic measure of financial stability by linking financial conditions to the probability distribution of future GDP growth and applies it to a set of 20 major advanced and emerging market economies. The chapter shows that changes in financial conditions shift the whole distribution of future GDP growth},
	language = {eng},
	publisher = {International Monetary Fund},
	author = {{International Monetary Fund}},
	year = {2017},
	doi = {10.5089/9781484308394.082},
}

@book{cressie_statistics_2011,
	address = {Hoboken, N.J},
	series = {Wiley series in probability and statistics},
	title = {Statistics for spatio-temporal data},
	isbn = {978-0-471-69274-4 978-1-119-24306-9},
	abstract = {Intro -- Table of Contents -- Series -- Title -- Copyright -- Preface -- Acknowledgments -- CHAPTER 1: Space-Time: The Next Frontier -- CHAPTER 2: Statistical Preliminaries -- 2.1 CONDITIONAL PROBABILITIES AND HIERARCHICAL MODELING (HM) -- 2.2 INFERENCE AND DIAGNOSTICS -- 2.3 COMPUTATION OF THE POSTERIOR DISTRIBUTION -- 2.4 GRAPHICAL REPRESENTATIONS OF STATISTICAL DEPENDENCIES -- 2.5 DATA/MODEL/COMPUTING COMPROMISES -- CHAPTER 3: Fundamentals of Temporal Processes -- 3.1 CHARACTERIZATION OF TEMPORAL PROCESSES -- 3.2 INTRODUCTION TO DETERMINISTIC DYNAMICAL SYSTEMS -- 3.3 TIME SERIES PRELIMINARIES -- 3.4 BASIC TIME SERIES MODELS -- 3.5 SPECTRAL REPRESENTATION OF TEMPORAL PROCESSES -- 3.6 HIERARCHICAL MODELING OF TIME SERIES -- 3.7 BIBLIOGRAPHIC NOTES -- CHAPTER 4: Fundamentals of Spatial Random Processes -- 4.1 GEOSTATISTICAL PROCESSES -- 4.2 LATTICE PROCESSES -- 4.3 SPATIAL POINT PROCESSES -- 4.4 RANDOM SETS -- 4.5 BIBLIOGRAPHIC NOTES -- CHAPTER 5: Exploratory Methods for Spatio-Temporal Data -- 5.1 VISUALIZATION -- 5.2 SPECTRAL ANALYSIS -- 5.3 EMPIRICAL ORTHOGONAL FUNCTION (EOF) ANALYSIS -- 5.4 EXTENSIONS OF EOF ANALYSIS -- 5.5 PRINCIPAL OSCILLATION PATTERNS (POPS) -- 5.6 SPATIO-TEMPORAL CANONICAL CORRELATION ANALYSIS (CCA) -- 5.7 SPATIO-TEMPORAL FIELD COMPARISONS -- 5.8 BIBLIOGRAPHIC NOTES -- CHAPTER 6: Spatio-Temporal Statistical Models -- 6.1 SPATIO-TEMPORAL COVARIANCE FUNCTIONS -- 6.2 SPATIO-TEMPORAL KRIGING -- 6.3 STOCHASTIC DIFFERENTIAL AND DIFFERENCE EQUATIONS -- 6.4 TIME SERIES OF SPATIAL PROCESSES -- 6.5 SPATIO-TEMPORAL POINT PROCESSES -- 6.6 SPATIO-TEMPORAL COMPONENTS-OF-VARIATION MODELS -- 6.7 BIBLIOGRAPHIC NOTES -- CHAPTER 7: Hierarchical Dynamical Spatio-Temporal Models -- 7.1 DATA MODELS FOR THE DSTM -- 7.2 PROCESS MODELS FOR THE DSTM: LINEAR MODELS -- 7.3 PROCESS MODELS FOR THE DSTM: NONLINEAR MODELS},
	language = {eng},
	publisher = {Wiley},
	author = {Cressie, Noel A. C. and Wikle, Christopher K.},
	year = {2011},
}

@article{zhu_functional_2024,
	title = {Functional optimal transport: regularized map estimation and domain adaptation for functional data},
	volume = {25},
	issn = {1533-7928},
	shorttitle = {Functional optimal transport},
	url = {http://jmlr.org/papers/v25/22-0217.html},
	abstract = {We introduce a formulation of regularized optimal transport problem for distributions on function spaces, where the stochastic map between functional domains can be approximated in terms of an (infinite-dimensional) Hilbert-Schmidt operator mapping a Hilbert space of functions to another. For numerous machine learning applications, data can be naturally viewed as samples drawn from spaces of functions, such as curves and surfaces, in high dimensions. Optimal transport for functional data analysis provides a useful framework of treatment for such domains. Since probability measures in infinite dimensional spaces generally lack absolute continuity (i.e., with respect to non-degenerate Gaussian measures), the Monge map in the standard optimal transport theory for finite dimensional spaces typically does not exist in the functional settings arising in such machine learning applications. This necessitates a suitable notion of approximation for the best pushforward measure to be obtained via a transport map. Indeed, our approach to the transportation problem in functional spaces is by a suitable regularization technique --- we restrict the class of transport maps to be a Hilbert-Schmidt space of operators.Within this regularization framework, we develop an efficient algorithm for finding the stochastic transport map between functional domains and provide theoretical guarantees on the existence, uniqueness, and consistency of our estimate for the Hilbert-Schmidt space of compact linear operators. We validate our method on synthetic datasets and examine the functional properties of the transport map. Experiments on real-world datasets of robot arm trajectories further demonstrate the effectiveness of our method on applications in domain adaptation.},
	number = {276},
	urldate = {2024-10-18},
	journal = {Journal of Machine Learning Research},
	author = {Zhu, Jiacheng and Guha, Aritra and Do, Dat and Xu, Mengdi and Nguyen, XuanLong and Zhao, Ding},
	year = {2024},
	pages = {1--49},
}

@misc{zhou_conformal_2024,
	title = {Conformal {Prediction}: {A} {Data} {Perspective}},
	shorttitle = {Conformal {Prediction}},
	url = {http://arxiv.org/abs/2410.06494},
	abstract = {Conformal prediction (CP), a distribution-free uncertainty quantification (UQ) framework, reliably provides valid predictive inference for black-box models. CP constructs prediction sets that contain the true output with a specified probability. However, modern data science diverse modalities, along with increasing data and model complexity, challenge traditional CP methods. These developments have spurred novel approaches to address evolving scenarios. This survey reviews the foundational concepts of CP and recent advancements from a data-centric perspective, including applications to structured, unstructured, and dynamic data. We also discuss the challenges and opportunities CP faces in large-scale data and models.},
	urldate = {2024-10-18},
	publisher = {arXiv},
	author = {Zhou, Xiaofan and Chen, Baiting and Gui, Yu and Cheng, Lu},
	month = oct,
	year = {2024},
	note = {arXiv:2410.06494},
	keywords = {Computer Science - Machine Learning},
}

@article{fontana_global_2024,
	title = {Global sensitivity and domain-selective testing for functional-valued responses: {An} application to climate economy models},
	volume = {35},
	copyright = {© 2024 The Author(s). Environmetrics published by John Wiley \& Sons Ltd.},
	issn = {1099-095X},
	shorttitle = {Global sensitivity and domain-selective testing for functional-valued responses},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2866},
	doi = {10.1002/env.2866},
	abstract = {Understanding the dynamics and evolution of climate change and associated uncertainties is key for designing robust policy actions. Computer models are key tools in this scientific effort, which have now reached a high level of sophistication and complexity. Model auditing is needed in order to better understand their results, and to deal with the fact that such models are increasingly opaque with respect to their inner workings. Current techniques such as Global Sensitivity Analysis (GSA) are limited to dealing either with multivariate outputs, stochastic ones, or finite-change inputs. This limits their applicability to time-varying variables such as future pathways of greenhouse gases. To provide additional semantics in the analysis of a model ensemble, we provide an extension of GSA methodologies tackling the case of stochastic functional outputs with finite change inputs. To deal with finite change inputs and functional outputs, we propose an extension of currently available GSA methodologies while we deal with the stochastic part by introducing a novel, domain-selective inferential technique for sensitivity indices. Our method is explored via a simulation study that shows its robustness and efficacy in detecting sensitivity patterns. We apply it to real-world data, where its capabilities can provide to practitioners and policymakers additional information about the time dynamics of sensitivity patterns, as well as information about robustness.},
	language = {en},
	number = {6},
	urldate = {2024-10-01},
	journal = {Environmetrics},
	author = {Fontana, Matteo and Tavoni, Massimo and Vantini, Simone},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.2866},
	keywords = {functional data analysis, global sensitivity analysis, permutation testing, robust sensitivity, scenario analysis, significance testing},
	pages = {e2866},
}

@article{signorelli_towards_2024,
	title = {Towards a taxonomy for {Business}-to-{Government} data sharing},
	volume = {40},
	copyright = {All rights reserved},
	issn = {1874-7655},
	url = {https://content.iospress.com/articles/statistical-journal-of-the-iaos/sji230122},
	doi = {10.3233/SJI-230122},
	abstract = {The phenomenon of Business-to-Government (B2G) data sharing represents a growing trend, especially in latest years. In fact, research has shown how privately held data could have a huge potential when used to tackle societal policy issues. B2G data s},
	language = {en},
	number = {3},
	urldate = {2024-10-01},
	journal = {Statistical Journal of the IAOS},
	author = {Signorelli, Serena and Fontana, Matteo and Vespe, Michele and Gabrielli, Lorenzo and Bertoni, Eleonora},
	month = jan,
	year = {2024},
	note = {Publisher: IOS Press},
	pages = {713--726},
}

@misc{noauthor_center-outward_nodate,
	title = {Center-outward quantiles and the measurement of multivariate risk - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167668720301165},
	urldate = {2024-08-26},
}

@misc{noauthor_conformalized_nodate,
	title = {Conformalized {Physics}-{Informed} {Neural} {Networks}},
	url = {https://arxiv.org/html/2405.08111v1},
	urldate = {2024-08-01},
}

@misc{campos_conformal_2024,
	title = {Conformal {Prediction} for {Natural} {Language} {Processing}: {A} {Survey}},
	shorttitle = {Conformal {Prediction} for {Natural} {Language} {Processing}},
	url = {http://arxiv.org/abs/2405.01976},
	abstract = {The rapid proliferation of large language models and natural language processing (NLP) applications creates a crucial need for uncertainty quantification to mitigate risks such as hallucinations and to enhance decision-making reliability in critical applications. Conformal prediction is emerging as a theoretically sound and practically useful framework, combining flexibility with strong statistical guarantees. Its modelagnostic and distribution-free nature makes it particularly promising to address the current shortcomings of NLP systems that stem from the absence of uncertainty quantification. This paper provides a comprehensive survey of conformal prediction techniques, their guarantees, and existing applications in NLP, pointing to directions for future research and open challenges.},
	language = {en},
	urldate = {2024-07-25},
	publisher = {arXiv},
	author = {Campos, Margarida M. and Farinhas, António and Zerva, Chrysoula and Figueiredo, Mário A. T. and Martins, André F. T.},
	month = may,
	year = {2024},
	note = {arXiv:2405.01976 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{noauthor_conformal_nodate,
	title = {Conformal {Language} {Modeling}},
	url = {https://research.google/pubs/conformal-language-modeling/},
	urldate = {2024-07-24},
}

@inproceedings{babbar_utility_2022,
	address = {Vienna, Austria},
	title = {On the {Utility} of {Prediction} {Sets} in {Human}-{AI} {Teams}},
	isbn = {978-1-956792-00-3},
	url = {https://www.ijcai.org/proceedings/2022/341},
	doi = {10.24963/ijcai.2022/341},
	abstract = {Research on human-AI teams usually provides experts with a single label, which ignores the uncertainty in a model’s recommendation. Conformal prediction (CP) is a well established line of research that focuses on building a theoretically grounded, calibrated prediction set, which may contain multiple labels. We explore how such prediction sets impact expert decision-making in human-AI teams. Our evaluation on human subjects finds that set valued predictions positively impact experts. However, we notice that the predictive sets provided by CP can be very large, which leads to unhelpful AI assistants. To mitigate this, we introduce D-CP, a method to perform CP on some examples and defer to experts. We prove that D-CP can reduce the prediction set size of non-deferred examples. We show how D-CP performs in quantitative and in human subject experiments (n=120). Our results suggest that CP prediction sets improve human-AI team performance over showing the top-1 prediction alone, and that experts find D-CP prediction sets are more useful than CP prediction sets.},
	language = {en},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the {Thirty}-{First} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	publisher = {International Joint Conferences on Artificial Intelligence Organization},
	author = {Babbar, Varun and Bhatt, Umang and Weller, Adrian},
	month = jul,
	year = {2022},
	pages = {2457--2463},
}

@inproceedings{zhang_evaluating_2024,
	address = {Honolulu HI USA},
	title = {Evaluating the {Utility} of {Conformal} {Prediction} {Sets} for {AI}-{Advised} {Image} {Labeling}},
	isbn = {9798400703300},
	url = {https://dl.acm.org/doi/10.1145/3613904.3642446},
	doi = {10.1145/3613904.3642446},
	abstract = {As deep neural networks are more commonly deployed in highstakes domains, their black-box nature makes uncertainty quantification challenging. We investigate the effects of presenting conformal prediction sets—a distribution-free class of methods for generating prediction sets with specified coverage—to express uncertainty in AI-advised decision-making. Through a large online experiment, we compare the utility of conformal prediction sets to displays of Top-1 and Top-�� predictions for AI-advised image labeling. In a pre-registered analysis, we find that the utility of prediction sets for accuracy varies with the difficulty of the task: while they result in accuracy on par with or less than Top-1 and Top-�� displays for easy images, prediction sets excel at assisting humans in labeling out-of-distribution (OOD) images, especially when the set size is small. Our results empirically pinpoint practical challenges of conformal prediction sets and provide implications on how to incorporate them for real-world decision-making.},
	language = {en},
	urldate = {2024-07-24},
	booktitle = {Proceedings of the {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Zhang, Dongping and Chatzimparmpas, Angelos and Kamali, Negar and Hullman, Jessica},
	month = may,
	year = {2024},
	pages = {1--19},
}

@misc{ghosal_multivariate_2021,
	title = {Multivariate {Ranks} and {Quantiles} using {Optimal} {Transport}: {Consistency}, {Rates}, and {Nonparametric} {Testing}},
	shorttitle = {Multivariate {Ranks} and {Quantiles} using {Optimal} {Transport}},
	url = {http://arxiv.org/abs/1905.05340},
	abstract = {In this paper we study multivariate ranks and quantiles, deﬁned using the theory of optimal transport, and build on the work of Chernozhukov et al. [22] and Hallin et al. [54]. We study the characterization, computation and properties of the multivariate rank and quantile functions and their empirical counterparts. We derive the uniform consistency of these empirical estimates to their population versions, under certain assumptions. In fact, we prove a Glivenko-Cantelli type theorem that shows the asymptotic stability of the empirical rank map in any direction. Under mild structural assumptions, we provide global and local rates of convergence of the empirical quantile and rank maps. We also provide a sub-Gaussian tail bound for the global L2-loss of the empirical quantile function. Further, we propose tuning parameter-free multivariate nonparametric tests — a two-sample test and a test for mutual independence — based on our notion of multivariate quantiles/ranks. Asymptotic consistency of these tests are shown and the rates of convergence of the associated test statistics are derived, both under the null and alternative hypotheses.},
	language = {en},
	urldate = {2024-07-18},
	publisher = {arXiv},
	author = {Ghosal, Promit and Sen, Bodhisattva},
	month = may,
	year = {2021},
	note = {arXiv:1905.05340 [math, stat]},
	keywords = {62G30, 62G20, 60F15, 35J96, Mathematics - Probability, Mathematics - Statistics Theory},
}

@article{wasserman_universal_2020,
	title = {Universal inference},
	volume = {117},
	url = {https://www.pnas.org/doi/full/10.1073/pnas.1922664117},
	doi = {10.1073/pnas.1922664117},
	abstract = {We propose a general method for constructing confidence sets and hypothesis tests that have finite-sample guarantees without regularity conditions. We refer to such procedures as “universal.” The method is very simple and is based on a modified version of the usual likelihood-ratio statistic that we call “the split likelihood-ratio test” (split LRT) statistic. The (limiting) null distribution of the classical likelihood-ratio statistic is often intractable when used to test composite null hypotheses in irregular statistical models. Our method is especially appealing for statistical inference in these complex setups. The method we suggest works for any parametric model and also for some nonparametric models, as long as computing a maximum-likelihood estimator (MLE) is feasible under the null. Canonical examples arise in mixture modeling and shape-constrained inference, for which constructing tests and confidence sets has been notoriously difficult. We also develop various extensions of our basic methods. We show that in settings when computing the MLE is hard, for the purpose of constructing valid tests and intervals, it is sufficient to upper bound the maximum likelihood. We investigate some conditions under which our methods yield valid inferences under model misspecification. Further, the split LRT can be used with profile likelihoods to deal with nuisance parameters, and it can also be run sequentially to yield anytime-valid P values and confidence sequences. Finally, when combined with the method of sieves, it can be used to perform model selection with nested model classes.},
	number = {29},
	urldate = {2024-07-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Wasserman, Larry and Ramdas, Aaditya and Balakrishnan, Sivaraman},
	month = jul,
	year = {2020},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {16880--16890},
}

@inproceedings{ndiaye_computing_2019,
	title = {Computing {Full} {Conformal} {Prediction} {Set} with {Approximate} {Homotopy}},
	volume = {32},
	url = {https://proceedings.neurips.cc/paper/2019/hash/9e3cfc48eccf81a0d57663e129aef3cb-Abstract.html},
	urldate = {2024-07-10},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ndiaye, Eugene and Takeuchi, Ichiro},
	year = {2019},
}

@misc{liu_quantiles_2024,
	title = {Quantiles, {Ranks} and {Signs} in {Metric} {Spaces}},
	url = {http://arxiv.org/abs/2209.04090},
	abstract = {Non-Euclidean data become more prevalent in practice, necessitating the development of a framework for statistical inference analogous to that for Euclidean data. Quantile is one of the most important concepts in traditional statistical inference; we introduce the counterpart, both locally and globally, for data objects in metric spaces. This is realized by expanding upon the metric distribution function proposed by Wang et al. (2021). Rank and sign are defined at local and global levels as a natural consequence of the center-outward ordering of metric spaces brought about by the local and global quantiles. The theoretical properties are established, such as the root-n consistency and uniform consistency of the local and global empirical quantiles and the distribution-freeness of ranks and signs. The empirical metric median, which is defined here as the 0th empirical global metric quantile, is proven to be resistant to contamination by means of both theoretical and numerical approaches. Quantiles have been shown to be valuable through extensive simulations in a number of metric spaces. Moreover, we introduce a family of fast rank-based independence tests for a generic metric space. Monte Carlo experiments show good finite-sample performance of the test.},
	language = {en},
	urldate = {2024-07-09},
	publisher = {arXiv},
	author = {Liu, Hang and Wang, Xueqin and Zhu, Jin and Zhang, Heping},
	month = mar,
	year = {2024},
	note = {arXiv:2209.04090 [math, stat]},
	keywords = {Mathematics - Statistics Theory},
}

@article{camehl_superlevel_2024,
	title = {On superlevel sets of conditional densities and multivariate quantile regression},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407624001532},
	doi = {10.1016/j.jeconom.2024.105807},
	abstract = {Some common proposals of multivariate quantiles do not sufficiently control the probability content, while others do not always accurately reflect the concentration of probability mass. We suggest superlevel sets of conditional multivariate densities as an alternative to current multivariate quantile definitions. Hence, the superlevel set is a function of conditioning variables much like in quantile regression. We show that conditional superlevel sets have favorable mathematical and intuitive features, and support a clear probabilistic interpretation. We derive the superlevel sets for a conditional or marginal density of interest from an (overfitted) multivariate Gaussian mixture model. This approach guarantees logically consistent (i.e., non-crossing) conditional superlevel sets and also allows us to obtain more traditional univariate quantiles. We demonstrate recovery of the true conditional univariate quantiles for distributions with correlation, heteroskedasticity, or asymmetry and apply our method in univariate and multivariate settings to a study on household expenditures.},
	urldate = {2024-07-07},
	journal = {Journal of Econometrics},
	author = {Camehl, Annika and Fok, Dennis and Gruber, Kathrin},
	month = jul,
	year = {2024},
	keywords = {Bayesian quantile regression, Gaussian mixture model, Multiple response},
	pages = {105807},
}

@inproceedings{zaffran_adaptive_2022,
	title = {Adaptive {Conformal} {Predictions} for {Time} {Series}},
	url = {https://proceedings.mlr.press/v162/zaffran22a.html},
	abstract = {Uncertainty quantification of predictive models is crucial in decision-making problems. Conformal prediction is a general and theoretically sound answer. However, it requires exchangeable data, excluding time series. While recent works tackled this issue, we argue that Adaptive Conformal Inference (ACI, Gibbs \& Cand\{è\}s, 2021), developed for distribution-shift time series, is a good procedure for time series with general dependency. We theoretically analyse the impact of the learning rate on its efficiency in the exchangeable and auto-regressive case. We propose a parameter-free method, AgACI, that adaptively builds upon ACI based on online expert aggregation. We lead extensive fair simulations against competing methods that advocate for ACI’s use in time series. We conduct a real case study: electricity price forecasting. The proposed aggregation algorithm provides efficient prediction intervals for day-ahead forecasting. All the code and data to reproduce the experiments are made available on GitHub.},
	language = {en},
	urldate = {2024-06-28},
	booktitle = {Proceedings of the 39th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Zaffran, Margaux and Feron, Olivier and Goude, Yannig and Josse, Julie and Dieuleveut, Aymeric},
	month = jun,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {25834--25866},
}

@article{shmueli_explain_2010,
	title = {To {Explain} or to {Predict}?},
	volume = {25},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
	doi = {10.1214/10-STS330},
	abstract = {Statistical modeling is a powerful tool for developing and testing theories by way of causal explanation, prediction, and description. In many disciplines there is near-exclusive use of statistical modeling for causal explanation and the assumption that models with high explanatory power are inherently of high predictive power. Conflation between explanation and prediction is common, yet the distinction must be understood for progressing scientific knowledge. While this distinction has been recognized in the philosophy of science, the statistical literature lacks a thorough discussion of the many differences that arise in the process of modeling for an explanatory versus a predictive goal. The purpose of this article is to clarify the distinction between explanatory and predictive modeling, to discuss its sources, and to reveal the practical implications of the distinction to each step in the modeling process.},
	number = {3},
	urldate = {2024-06-19},
	journal = {Statistical Science},
	author = {Shmueli, Galit},
	month = aug,
	year = {2010},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Explanatory modeling, causality, data mining, predictive modeling, predictive power, scientific research, statistical strategy},
	pages = {289--310},
}

@article{shtiliyanova_kriging-based_2017,
	title = {Kriging-based approach to predict missing air temperature data},
	volume = {142},
	issn = {0168-1699},
	url = {https://www.sciencedirect.com/science/article/pii/S016816991730371X},
	doi = {10.1016/j.compag.2017.09.033},
	abstract = {The geo-statistical Kriging method is conventionally used in the spatial dimension to predict missing values in a series by utilizing information from neighbouring data, supported by the hypothesis that mathematical expectation is a function of distance between observations. By using a data-driven machine learning-based inferencing and exploration framework, this research applies a Kriging-based interpolation in the temporal dimension to fill in data gaps in time-series of air temperatures. It assesses its performance for artificial gap scenarios (ranging in length from single one to six consecutive data points) generated using data with both daily and hourly resolutions from five sites in Europe (Laqueuille, France; Grillenburg, Germany; Monte Bondone, Italy; Oensingen, Switzerland; Rothamsted, United Kingdom) and one in France overseas (Sedael, Réunion Island). Results show that the method is capable of predicting missing temperatures with acceptable accuracy, especially with the hourly resolution and for non-high elevation sites: modeling efficiency (EF≤1, optimum) {\textgreater}0.8, with the exception of Monte Bondone, placed at {\textgreater}2000ma.s.l. (EF{\textless}0). With daily data, maximum temperature was correctly predicted at all sites (0.6≤EF{\textless}0.9), while some less accuracy (down to EF{\textless}0.4) was noted when predicting missing daily minimum temperatures. In conclusion, the method appears suitable to be applied to fill in hourly temperature gaps, requiring more stringent hypotheses concerning daily data and mountain sites (but further studies are required to draw concluding recommendations).},
	urldate = {2024-06-13},
	journal = {Computers and Electronics in Agriculture},
	author = {Shtiliyanova, Anastasiya and Bellocchi, Gianni and Borras, David and Eza, Ulrich and Martin, Raphaël and Carrère, Pascal},
	month = nov,
	year = {2017},
	keywords = {Air temperature, Daily and hourly resolutions, Data-driven machine learning, Infilling, Kriging-based temporal interpolation},
	pages = {440--449},
}

@article{cho_improvement_2020,
	title = {Improvement of spatial interpolation accuracy of daily maximum air temperature in urban areas using a stacking ensemble technique},
	volume = {57},
	issn = {1548-1603},
	url = {https://doi.org/10.1080/15481603.2020.1766768},
	doi = {10.1080/15481603.2020.1766768},
	abstract = {The reliable and robust monitoring of air temperature distribution is essential for urban thermal environmental analysis. In this study, a stacking ensemble model consisting of multi-linear regression (MLR), support vector regression (SVR), and random forest (RF) optimized by the SVR is proposed to interpolate the daily maximum air temperature (Tmax) during summertime in a mega urban area. A total of 10 geographic variables, including the clear-sky averaged land surface temperature and the normalized difference vegetation index, were used as input variables. The stacking model was compared to Cokriging, three individual data-driven methods, and a simple average ensemble model, all through leave-one-station-out cross validation. The stacking model showed the best performance by improving the generalizability of the individual models and mitigating the sensitivity to the extreme daily Tmax. This study demonstrates that the stacking ensemble method can improve the accuracy of spatial interpolation of environmental variables in various research fields.},
	number = {5},
	urldate = {2024-06-13},
	journal = {GIScience \& Remote Sensing},
	author = {Cho, Dongjin and Yoo, Cheolhee and Im, Jungho and Lee, Yeonsu and Lee, Jaese},
	month = jul,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/15481603.2020.1766768},
	keywords = {Cokriging, Multi-linear regression, Random forest, Simple average ensemble, Spatial interpolation, Stacking ensemble, Support vector regression},
	pages = {633--649},
}

@article{zhang_downscaling_2022,
	title = {Downscaling {Hourly} {Air} {Temperature} of {WRF} {Simulations} {Over} {Complex} {Topography}: {A} {Case} {Study} of {Chongli} {District} in {Hebei} {Province}, {China}},
	volume = {127},
	issn = {2169-8996},
	shorttitle = {Downscaling {Hourly} {Air} {Temperature} of {WRF} {Simulations} {Over} {Complex} {Topography}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021JD035542},
	doi = {10.1029/2021JD035542},
	abstract = {Accurate and high-resolution air temperature prediction is important in many different applications. Hourly air temperature forecasting in mountainous areas is necessary and important because mountainous areas are becoming increasingly important areas of human activities. At present, scientists successfully employ numerical weather prediction (NWP) models, such as the Weather Research and Forecasting (WRF) model, to achieve reliable forecasts. However, air temperature forecasting and modeling over complex geographical zones are still difficult tasks. The WRF model is a mesoscale model and does not adequately account for the influence of terrain on the air temperature. It is important to downscale larger-scale models to a much finer scale. In this paper, a statistical temperature downscaling method based on geographically weighted regression (GWR) and diurnal temperature cycle (DTC) models is proposed. A statistical downscaling scheme of WRF simulation data is designed to forecast the hourly air temperature from 1-km spatial resolution to 30 m, up to 24 hr in advance. The combined downscaling model's root-mean-square error (RMSE) decreased by 0.87°C at the automatic weather station (AWS) level and 0.62°C over the domain when compared to WRF simulations, and the mean absolute error (MAE) decreased by 0.71°C and 0.51°C, respectively, at these two levels. The results reveal that the combined downscaling model performs very well in correcting and downscaling the air temperature in WRF simulations in the study areas.},
	language = {en},
	number = {3},
	urldate = {2024-06-13},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Zhang, Guangxing and Zhu, Shanyou and Zhang, Nan and Zhang, Guixin and Xu, Yongming},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021JD035542},
	pages = {e2021JD035542},
}

@article{wang_deep_2022,
	title = {On deep learning-based bias correction and downscaling of multiple climate models simulations},
	volume = {59},
	issn = {1432-0894},
	url = {https://doi.org/10.1007/s00382-022-06277-2},
	doi = {10.1007/s00382-022-06277-2},
	abstract = {Bias correcting and downscaling climate model simulations requires reconstructing spatial and intervariable dependences of the observations. However, the existing univariate bias correction methods often fail to account for such dependences. While the multivariate bias correction methods have been developed to address this issue, they do not consistently outperform the univariate methods due to various assumptions. In this study, using 20 state-of-the-art coupled general circulation models (GCMs) daily mean, maximum and minimum temperature (Tmean, Tmax and Tmin) from the Coupled Model Intercomparison Project phase 6 (CMIP6), we comprehensively evaluated the Super Resolution Deep Residual Network (SRDRN) deep learning model for climate downscaling and bias correction. The SRDRN model sequentially stacked 20 GCMs with single or multiple input-output channels, so that the biases can be efficiently removed based on the relative relations among different GCMs against observations, and the intervariable dependences can be retained for multivariate bias correction. It corrected biases in spatial dependences by deeply extracting spatial features and making adjustments for daily simulations according to observations. For univariate SRDRN, it considerably reduced larger biases of Tmean in space, time, as well as extremes compared to the quantile delta mapping (QDM) approach. For multivariate SRDRN, it performed better than the dynamic Optimal Transport Correction (dOTC) method and reduced greater biases of Tmax and Tmin but also reproduced intervariable dependences of the observations, where QDM and dOTC showed unrealistic artifacts (Tmax {\textless} Tmin). Additional studies on the deep learning-based approach may bring climate model bias correction and downscaling to the next level.},
	language = {en},
	number = {11},
	urldate = {2024-06-13},
	journal = {Climate Dynamics},
	author = {Wang, Fang and Tian, Di},
	month = dec,
	year = {2022},
	keywords = {Bias correction, Climate models, Deep learning, Downscaling, Model evaluation, Multivariate dependence, Spatial dependence},
	pages = {3451--3468},
}

@article{cho_new_2024,
	title = {A new statistical downscaling approach for short-term forecasting of summer air temperatures through a fusion of deep learning and spatial interpolation},
	volume = {150},
	issn = {1477-870X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/qj.4643},
	doi = {10.1002/qj.4643},
	abstract = {Reliable early forecasting of extreme summer air temperatures is essential for effectively managing and mitigating the socioeconomic damage caused by thermal disasters. Numerical weather prediction models have become valuable tools for forecasting air temperature; however, they incur high computational costs, resulting in coarse spatial resolution and systematic bias owing to imperfect parametrization. To address these problems, we developed a novel statistical downscaling and bias correction method (named DeU-Net) for the maximum and minimum air temperature (Tmax and Tmin respectively) forecasts obtained from the Global Data Assimilation and Prediction System with a spatial resolution of 10 to 1.5 km over South Korea through the fusion of deep learning (i.e., U-Net) and spatial interpolation. In this study, we used a methodology to decompose statistically downscaled Tmax and Tmin forecasts into temporal dynamics over South Korea and spatial fluctuations by pixels. When comparing the proposed DeU-Net with the dynamical downscaling model (i.e., Local Data Assimilation and Prediction System) and support vector regression-based statistical downscaling model at the seen and unseen stations for forecasting the next-day Tmax and Tmin, DeU-Net showed the highest spatial correlation and the lowest root-mean-square error in all cases. In a qualitative evaluation, DeU-Net successfully produced a detailed spatial distribution most similar to the observations. A further comparison extending the forecast lead time to 7 days indicated that the proposed DeU-Net is a better downscaling approach than support vector regression, regardless of the forecast lead time. These results demonstrate that bias-corrected high spatial resolution air temperature forecasts with relatively long forecast lead times in summer can be effectively produced using the proposed model for operational forecasting.},
	language = {en},
	number = {760},
	urldate = {2024-06-13},
	journal = {Quarterly Journal of the Royal Meteorological Society},
	author = {Cho, Dongjin and Im, Jungho and Jung, Sihun},
	year = {2024},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/qj.4643},
	keywords = {air temperature forecasting, decomposition, deep learning, spatial interpolation, statistical downscaling},
	pages = {1222--1242},
}

@article{spinoni_high-resolution_2014,
	title = {High-resolution temperature climatology for {Italy}: {Interpolation} method intercomparison},
	volume = {34},
	shorttitle = {High-resolution temperature climatology for {Italy}},
	doi = {10.1002/joc.3764},
	abstract = {High‐resolution monthly temperature climatologies for Italy are presented. They are based on a dense and quality‐controlled observational dataset which includes 1484 stations and on three distinct approaches: multi‐linear regression with local improvements ( MLRLI ), an enhanced version of the model recently used for the Greater Alpine Region, regression kriging ( RK ), widely used in literature and, lastly, local weighted linear regression (LWLR) of temperature versus elevation, which may be considered more suitable for the complex orography characterizing the Italian territory.
Dataset and methods used both to check the station records and to get the 1961–1990 normals used for the climatologies are discussed. Advantages and shortcomings of the three approaches are investigated and the results are compared.
All three approaches lead to quite reasonable models of station temperature normals, with lowest errors in spring and autumn and highest errors in winter. The LWLR approach shows slightly better performances than the other two, with monthly leave‐one‐out estimated root mean square errors ranging from 0.74 °C (April and May) to 1.03 °C (December). Further evidence in its favour is the greater reliability of local approach in modelling the behaviour of the temperature‐elevation relationship in Italy's complex territory.
The comparison of the different climatologies is a very effective tool to understand the robustness of each approach. Moreover, the first two methods ( MLRLI and RK ) turn out to be important to tune the third one ( LWLR ), as they help not only to understand the relationship between temperature normals and some important physiographical variables ( MLRLI ) but also to study the decrease of station normals covariance with distance ( RK ).},
	journal = {International Journal of Climatology},
	author = {Spinoni, Jonathan and Brunetti, Michele and Maugeri, Maurizio and Simolo, Claudia and Nanni, Teresa},
	month = mar,
	year = {2014},
}

@article{braum_improving_2023,
	title = {Improving maps of daily air temperature considering the effects of topography: {Data} from {Espírito} {Santo}, {Brazil} (2007–2020)},
	volume = {131},
	issn = {0895-9811},
	shorttitle = {Improving maps of daily air temperature considering the effects of topography},
	url = {https://www.sciencedirect.com/science/article/pii/S089598112300439X},
	doi = {10.1016/j.jsames.2023.104627},
	abstract = {Mapping air temperature is essential for climate studies and its application in environmental, forest, and agricultural sciences, among others. This study aimed to spatialize daily air temperature data, creating a database corrected by the effect of relief. This study was performed in the state of Espírito Santo, Southeastern Brazil, which has an altitude range of 2892 m from the coast to the highest point in Serra do Caparaó. Meteorological data were collected from 22 meteorological stations. The potential temperature equation and adiabatic lapse rate were used to estimate air temperature considering the effect of relief. These methods were compared with the inverse distance weighting (IDW) interpolator to evaluate the gain obtained by including topographic information in the temperature estimation. The performance of methods was evaluated by cross-validation, using the coefficient of determination (R2), bias, and mean absolute error (MAE). The method with the best performance in the spatialization of air temperature data was obtained by adjusting the temperature data for the same altitude condition, using the environmental lapse rate (−6.5 °C km−1) and the IDW interpolator to spatialize data. Finally, interpolated temperature values were adjusted for the original terrain altitude (−6.5 °C km−1), using the altitude existing in each pixel of a digital elevation model. This method provided a gain of 51\% in the reduction of MAE values compared with the IDW interpolator, which does not consider the effect of relief. It was used to spatialize daily air temperature data (maximum, average, and minimum) for the entire territory of Espírito Santo, Brazil, with 500-m spatial resolution.},
	urldate = {2024-06-13},
	journal = {Journal of South American Earth Sciences},
	author = {Braum, Edilson Sarter and Zanetti, Sidney Sara and Cecílio, Roberto Avelino and Pezzopane, José Eduardo Macedo},
	month = nov,
	year = {2023},
	keywords = {Adiabatic ratio, Altitude, Environmental lapse rate, Interpolation, Inverse distance weighted (IDW), Relief},
	pages = {104627},
}

@article{cremona_functional_2019,
	title = {Functional data analysis for computational biology},
	volume = {35},
	issn = {1367-4803},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6736445/},
	doi = {10.1093/bioinformatics/btz045},
	abstract = {Supplementary information:
 are available at Bioinformatics online.},
	number = {17},
	urldate = {2024-06-06},
	journal = {Bioinformatics},
	author = {Cremona, Marzia A and Xu, Hongyan and Makova, Kateryna D and Reimherr, Matthew and Chiaromonte, Francesca and Madrigal, Pedro},
	month = sep,
	year = {2019},
	pmid = {30668667},
	pmcid = {PMC6736445},
	pages = {3211--3213},
}

@article{lontzek_stochastic_2015,
	title = {Stochastic integrated assessment of climate tipping points indicates the need for strict climate policy},
	volume = {5},
	copyright = {2014 Springer Nature Limited},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/nclimate2570},
	doi = {10.1038/nclimate2570},
	abstract = {Analysis of the uncertainty associated with the timing of climate tipping points suggests that carbon taxes need to be increased by a minimum of 50\%. If considering a rapid, high-impact tipping event, these taxes should be more than 200\% higher. This implies that the discount rate to delay stochastic tipping points is much lower than that for deterministic climate damages.},
	language = {en},
	number = {5},
	urldate = {2024-06-04},
	journal = {Nature Climate Change},
	author = {Lontzek, Thomas S. and Cai, Yongyang and Judd, Kenneth L. and Lenton, Timothy M.},
	month = may,
	year = {2015},
	note = {Publisher: Nature Publishing Group},
	keywords = {Climate-change mitigation, Climate-change policy, Governance},
	pages = {441--444},
}

@article{cai_risk_2016,
	title = {Risk of multiple interacting tipping points should encourage rapid {CO2} emission reduction},
	volume = {6},
	copyright = {2016 Springer Nature Limited},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/nclimate2964},
	doi = {10.1038/nclimate2964},
	abstract = {Evidence suggests that several elements of the climate system could be tipped into a different state by global warming, causing irreversible economic damages. To address their policy implications, we incorporated five interacting climate tipping points into a stochastic-dynamic integrated assessment model, calibrating their likelihoods and interactions on results from an existing expert elicitation. Here we show that combining realistic assumptions about policymakers’ preferences under uncertainty, with the prospect of multiple future interacting climate tipping points, increases the present social cost of carbon in the model nearly eightfold from US\$15 per tCO2 to US\$116 per tCO2. Furthermore, passing some tipping points increases the likelihood of other tipping points occurring to such an extent that it abruptly increases the social cost of carbon. The corresponding optimal policy involves an immediate, massive effort to control CO2 emissions, which are stopped by mid-century, leading to climate stabilization at {\textless}1.5 °C above pre-industrial levels.},
	language = {en},
	number = {5},
	urldate = {2024-06-04},
	journal = {Nature Climate Change},
	author = {Cai, Yongyang and Lenton, Timothy M. and Lontzek, Thomas S.},
	month = may,
	year = {2016},
	note = {Publisher: Nature Publishing Group},
	keywords = {Climate change, Climate-change mitigation, Climate-change policy, Governance},
	pages = {520--525},
}

@misc{beirlant_center-outward_2019,
	title = {Center-outward quantiles and the measurement of multivariate risk},
	url = {http://arxiv.org/abs/1912.04924},
	abstract = {All multivariate extensions of the univariate theory of risk measurement run into the same fundamental problem of the absence, in dimension d {\textgreater} 1, of a canonical ordering of Rd. Based on measure transportation ideas, several attempts have been made recently in the statistical literature to overcome that conceptual diﬃculty. In Hallin (2017), the concepts of center-outward distribution and quantile functions are developed as generalisations of the classical univariate concepts of distribution and quantile functions, along with their empirical versions. The center-outward distribution function F± is a homeomorphic cyclically monotone mapping from Rd {\textbackslash} F−±1(0) to the open punctured unit ball Bd {\textbackslash} \{0\}, while its empirical counterpart F(±n) is a cyclically monotone mapping from the sample to a regular grid over Bd. In dimension d = 1, F± reduces to 2F − 1, while F(±n) generates the same sigma-ﬁeld as traditional univariate ranks. The empirical F(±n), however, involves a large number of ties, which is impractical in the context of risk measurement. We therefore propose a class of smooth approximations Fn,ξ (ξ a smoothness index) of F(±n) as an alternative to the interpolation developed in del Barrio et al. (2018). This approximation allows for the computation of some new empirical risk measures, based either on the convex potential associated with the proposed transports, or on the volumes of the resulting empirical quantile regions. We also discuss the role of such transports in the evaluation of the risk associated with multivariate regularly varying distributions. Some simulations and applications to case studies illustrate the value of the approach.},
	language = {en},
	urldate = {2024-06-03},
	publisher = {arXiv},
	author = {Beirlant, Jan and Buitendag, Sven and del Bario, Eustasio and Hallin, Marc},
	month = dec,
	year = {2019},
	note = {arXiv:1912.04924 [stat]},
	keywords = {Statistics - Applications, Statistics - Methodology},
}

@article{billard_statistics_2003,
	title = {From the {Statistics} of {Data} to the {Statistics} of {Knowledge}: {Symbolic} {Data} {Analysis}},
	volume = {98},
	shorttitle = {From the {Statistics} of {Data} to the {Statistics} of {Knowledge}},
	doi = {10.1198/016214503000242},
	abstract = {Increasingly, datasets are so large they must be summarized in some fashion so that the resulting summary dataset is of a more manageable size, while still retaining as much knowledge inherent to the entire dataset as possible. One consequence of this situation is that the data may no longer be formatted as single values such as is the case for classical data, but rather may be represented by lists, intervals, distributions, and the like. These summarized data are examples of symbolic data. This article looks at the concept of symbolic data in general, and then attempts to review the methods currently available to analyze such data. It quickly becomes clear that the range of methodologies available draws analogies with developments before 1900 that formed a foundation for the inferential statistics of the 1900s, methods largely limited to small (by comparison) datasets and classical data formats. The scarcity of available methodologies for symbolic data also becomes clear and so draws attention to an enormous need for the development of a vast catalog (so to speak) of new symbolic methodologies along with rigorous mathematical and statistical foundational work for these methods.},
	journal = {Journal of The American Statistical Association - J AMER STATIST ASSN},
	author = {Billard, L and Diday, Edwin},
	month = jun,
	year = {2003},
	pages = {470--487},
}

@misc{lugosi_uncertainty_2024,
	title = {Uncertainty quantification in metric spaces},
	url = {http://arxiv.org/abs/2405.05110},
	doi = {10.48550/arXiv.2405.05110},
	abstract = {This paper introduces a novel uncertainty quantification framework for regression models where the response takes values in a separable metric space, and the predictors are in a Euclidean space. The proposed algorithms can efficiently handle large datasets and are agnostic to the predictive base model used. Furthermore, the algorithms possess asymptotic consistency guarantees and, in some special homoscedastic cases, we provide non-asymptotic guarantees. To illustrate the effectiveness of the proposed uncertainty quantification framework, we use a linear regression model for metric responses (known as the global Fr{\textbackslash}'echet model) in various clinical applications related to precision and digital medicine. The different clinical outcomes analyzed are represented as complex statistical objects, including multivariate Euclidean data, Laplacian graphs, and probability distributions.},
	urldate = {2024-05-29},
	publisher = {arXiv},
	author = {Lugosi, Gábor and Matabuena, Marcos},
	month = may,
	year = {2024},
	note = {arXiv:2405.05110 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{zhu_spherical_2024,
	title = {Spherical autoregressive models, with application to distributional and compositional time series},
	volume = {239},
	issn = {0304-4076},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407623000209},
	doi = {10.1016/j.jeconom.2022.12.008},
	abstract = {We introduce a new class of autoregressive models for spherical time series. The dimension of the spheres on which the observations of the time series are situated may be finite-dimensional or infinite-dimensional, where in the latter case we consider the Hilbert sphere. Spherical time series arise in various settings. We focus here on distributional and compositional time series. Applying a square root transformation to the densities of the observations of a distributional time series maps the distributional observations to the Hilbert sphere, equipped with the Fisher–Rao metric. Likewise, applying a square root transformation to the components of the observations of a compositional time series maps the compositional observations to a finite-dimensional sphere, equipped with the geodesic metric on spheres. The challenge in modeling such time series lies in the intrinsic non-linearity of spheres and Hilbert spheres, where conventional arithmetic operations such as addition or scalar multiplication are no longer available. To address this difficulty, we consider rotation operators to map observations on the sphere. Specifically, we introduce a class of skew-symmetric operators such that the associated exponential operators are rotation operators that for each given pair of points on the sphere map the first point of the pair to the second point of the pair. We exploit the fact that the space of skew-symmetric operators is Hilbertian to develop autoregressive modeling of geometric differences that correspond to rotations of spherical and distributional time series. Differences expressed in terms of rotations can be taken between the Fréchet mean and the observations or between consecutive observations of the time series. We derive theoretical properties of the ensuing autoregressive models and showcase these approaches with several motivating data. These include a time series of yearly observations of bivariate distributions of the minimum/maximum temperatures for a period of 120 days during each summer for the years 1990-2018 at Los Angeles (LAX) and John F. Kennedy (JFK) international airports. A second data application concerns a compositional time series with annual observations of compositions of energy sources for power generation in the U.S..},
	number = {2},
	urldate = {2024-05-13},
	journal = {Journal of Econometrics},
	author = {Zhu, Changbo and Müller, Hans-Georg},
	month = feb,
	year = {2024},
	keywords = {Fisher–Rao metric, Random objects, Rotation, Skew-symmetric operators, Time series analysis},
	pages = {105389},
}

@misc{bulte_autoregressive_2024,
	title = {An {Autoregressive} {Model} for {Time} {Series} of {Random} {Objects}},
	url = {http://arxiv.org/abs/2405.03778},
	abstract = {Random variables in metric spaces indexed by time and observed at equally spaced time points are receiving increased attention due to their broad applicability. However, the absence of inherent structure in metric spaces has resulted in a literature that is predominantly non-parametric and model-free. To address this gap in models for time series of random objects, we introduce an adaptation of the classical linear autoregressive model tailored for data lying in a Hadamard space. The parameters of interest in this model are the Fr´echet mean and a concentration parameter, both of which we prove can be consistently estimated from data. Additionally, we propose a test statistic and establish its asymptotic normality, thereby enabling hypothesis testing for the absence of serial dependence. Finally, we introduce a bootstrap procedure to obtain critical values for the test statistic under the null hypothesis. Theoretical results of our method, including the convergence of the estimators as well as the size and power of the test, are illustrated through simulations, and the utility of the model is demonstrated by an analysis of a time series of consumer inflation expectations.},
	language = {en},
	urldate = {2024-05-09},
	publisher = {arXiv},
	author = {Bulté, Matthieu and Sørensen, Helle},
	month = may,
	year = {2024},
	note = {arXiv:2405.03778 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{patel_conformal_2023,
	title = {Conformal {Contextual} {Robust} {Optimization}},
	url = {http://arxiv.org/abs/2310.10003},
	doi = {10.48550/arXiv.2310.10003},
	abstract = {Data-driven approaches to predict-then-optimize decision-making problems seek to mitigate the risk of uncertainty region misspecification in safety-critical settings. Current approaches, however, suffer from considering overly conservative uncertainty regions, often resulting in suboptimal decisionmaking. To this end, we propose Conformal-Predict-Then-Optimize (CPO), a framework for leveraging highly informative, nonconvex conformal prediction regions over high-dimensional spaces based on conditional generative models, which have the desired distribution-free coverage guarantees. Despite guaranteeing robustness, such black-box optimization procedures alone inspire little confidence owing to the lack of explanation of why a particular decision was found to be optimal. We, therefore, augment CPO to additionally provide semantically meaningful visual summaries of the uncertainty regions to give qualitative intuition for the optimal decision. We highlight the CPO framework by demonstrating results on a suite of simulation-based inference benchmark tasks and a vehicle routing task based on probabilistic weather prediction.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Patel, Yash and Rayan, Sahana and Tewari, Ambuj},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10003 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Statistics - Methodology},
}

@misc{pegoraro_vector_2024,
	title = {Vector {Quantile} {Regression} on {Manifolds}},
	url = {http://arxiv.org/abs/2307.01037},
	doi = {10.48550/arXiv.2307.01037},
	abstract = {Quantile regression (QR) is a statistical tool for distribution-free estimation of conditional quantiles of a target variable given explanatory features. QR is limited by the assumption that the target distribution is univariate and defined on an Euclidean domain. Although the notion of quantiles was recently extended to multi-variate distributions, QR for multi-variate distributions on manifolds remains underexplored, even though many important applications inherently involve data distributed on, e.g., spheres (climate and geological phenomena), and tori (dihedral angles in proteins). By leveraging optimal transport theory and c-concave functions, we meaningfully define conditional vector quantile functions of high-dimensional variables on manifolds (M-CVQFs). Our approach allows for quantile estimation, regression, and computation of conditional confidence sets and likelihoods. We demonstrate the approach's efficacy and provide insights regarding the meaning of non-Euclidean quantiles through synthetic and real data experiments.},
	urldate = {2024-05-08},
	publisher = {arXiv},
	author = {Pegoraro, Marco and Vedula, Sanketh and Rosenberg, Aviv A. and Tallini, Irene and Rodolà, Emanuele and Bronstein, Alex M.},
	month = feb,
	year = {2024},
	note = {arXiv:2307.01037 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Methodology},
}

@misc{zhou_conformal_2024-1,
	title = {Conformal inference for random objects},
	url = {http://arxiv.org/abs/2405.00294},
	abstract = {We develop an inferential toolkit for analyzing object-valued responses, which correspond to data situated in general metric spaces, paired with Euclidean predictors within the conformal framework. To this end we introduce conditional profile average transport costs, where we compare distance profiles that correspond to one-dimensional distributions of probability mass falling into balls of increasing radius through the optimal transport cost when moving from one distance profile to another. The average transport cost to transport a given distance profile to all others is crucial for statistical inference in metric spaces and underpins the proposed conditional profile scores. A key feature of the proposed approach is to utilize the distribution of conditional profile average transport costs as conformity score for general metric space-valued responses, which facilitates the construction of prediction sets by the split conformal algorithm. We derive the uniform convergence rate of the proposed conformity score estimators and establish asymptotic conditional validity for the prediction sets. The finite sample performance for synthetic data in various metric spaces demonstrates that the proposed conditional profile score outperforms existing methods in terms of both coverage level and size of the resulting prediction sets, even in the special case of scalar and thus Euclidean responses. We also demonstrate the practical utility of conditional profile scores for network data from New York taxi trips and for compositional data reflecting energy sourcing of U.S. states.},
	language = {en},
	urldate = {2024-05-05},
	publisher = {arXiv},
	author = {Zhou, Hang and Müller, Hans-Georg},
	month = apr,
	year = {2024},
	note = {arXiv:2405.00294 [stat]},
	keywords = {Statistics - Methodology},
}

@misc{calissano_conformal_2024,
	title = {Conformal {Prediction} {Sets} for {Populations} of {Graphs}},
	url = {http://arxiv.org/abs/2404.18862},
	doi = {10.48550/arXiv.2404.18862},
	abstract = {The analysis of data such as graphs has been gaining increasing attention in the past years. This is justified by the numerous applications in which they appear. Several methods are present to predict graphs, but much fewer to quantify the uncertainty of the prediction. The present work proposes an uncertainty quantification methodology for graphs, based on conformal prediction. The method works both for graphs with the same set of nodes (labelled graphs) and graphs with no clear correspondence between the set of nodes across the observed graphs (unlabelled graphs). The unlabelled case is dealt with the creation of prediction sets embedded in a quotient space. The proposed method does not rely on distributional assumptions, it achieves finite-sample validity, and it identifies interpretable prediction sets. To explore the features of this novel forecasting technique, we perform two simulation studies to show the methodology in both the labelled and the unlabelled case. We showcase the applicability of the method in analysing the performance of different teams during the FIFA 2018 football world championship via their player passing networks.},
	urldate = {2024-05-03},
	publisher = {arXiv},
	author = {Calissano, Anna and Fontana, Matteo and Zeni, Gianluca and Vantini, Simone},
	month = apr,
	year = {2024},
	note = {arXiv:2404.18862 [stat]
version: 1},
	keywords = {Statistics - Methodology},
}

@article{gabrielli_computational_2024,
	title = {Computational social science in regional analysis and the {European} real estate market},
	copyright = {© 2024 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group},
	url = {https://www.tandfonline.com/doi/abs/10.1080/00343404.2024.2329238},
	abstract = {The recent so-called ‘data revolution’ offers unprecedented opportunities to innovate regional policies. New data sources are being widely used by the scientific community, however their uptake is ...},
	language = {EN},
	urldate = {2024-04-23},
	journal = {Regional Studies},
	author = {Gabrielli, Lorenzo and Sulis, Patrizia and Fontana, Matteo and Signorelli, Serena and Vespe, Michele and Lavalle, Carlo},
	month = apr,
	year = {2024},
	note = {Publisher: Routledge},
}

@article{yang_selection_2024,
	title = {Selection and {Aggregation} of {Conformal} {Prediction} {Sets}},
	copyright = {© 2024 American Statistical Association},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.2024.2344700},
	abstract = {Conformal prediction is a generic methodology for finite-sample valid distribution-free prediction. This technique has garnered a lot of attention in the literature partly because it can be applied...},
	language = {EN},
	urldate = {2024-04-23},
	journal = {Journal of the American Statistical Association},
	author = {Yang, Yachong and Kuchibhotla, Arun Kumar},
	month = apr,
	year = {2024},
	note = {Publisher: Taylor \& Francis},
}

@misc{sun_copula_2024,
	title = {Copula {Conformal} {Prediction} for {Multi}-step {Time} {Series} {Forecasting}},
	url = {http://arxiv.org/abs/2212.03281},
	doi = {10.48550/arXiv.2212.03281},
	abstract = {Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper, we propose a Copula Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. We prove that CopulaCPTS has finite sample validity guarantee. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Sun, Sophia and Yu, Rose},
	month = mar,
	year = {2024},
	note = {arXiv:2212.03281 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Applications},
}

@misc{xu_conformal_2024,
	title = {Conformal prediction for multi-dimensional time series by ellipsoidal sets},
	url = {http://arxiv.org/abs/2403.03850},
	doi = {10.48550/arXiv.2403.03850},
	abstract = {Conformal prediction (CP) has been a popular method for uncertainty quantification because it is distribution-free, model-agnostic, and theoretically sound. For forecasting problems in supervised learning, most CP methods focus on building prediction intervals for univariate responses. In this work, we develop a sequential CP method called \${\textbackslash}texttt\{MultiDimSPCI\}\$ that builds prediction regions for a multivariate response, especially in the context of multivariate time series, which are not exchangeable. Theoretically, we estimate finite-sample high-probability bounds on the conditional coverage gap. Empirically, we demonstrate that \${\textbackslash}texttt\{MultiDimSPCI\}\$ maintains valid coverage on a wide range of multivariate time series while producing smaller prediction regions than CP and non-CP baselines.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Xu, Chen and Jiang, Hanyang and Xie, Yao},
	month = mar,
	year = {2024},
	note = {arXiv:2403.03850 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{cevid_distributional_2022,
	title = {Distributional {Random} {Forests}: {Heterogeneity} {Adjustment} and {Multivariate} {Distributional} {Regression}},
	volume = {23},
	issn = {1533-7928},
	shorttitle = {Distributional {Random} {Forests}},
	url = {http://jmlr.org/papers/v23/21-0585.html},
	abstract = {Random Forest is a successful and widely used regression and classification algorithm. Part of its appeal and reason for its versatility is its (implicit) construction of a kernel-type weighting function on training data, which can also be used for targets other than the original mean estimation. We propose a novel forest construction for multivariate responses based on their joint conditional distribution, independent of the estimation target and the data model. It uses a new splitting criterion based on the MMD distributional metric, which is suitable for detecting heterogeneity in multivariate distributions. The induced weights define an estimate of the full conditional distribution, which in turn can be used for arbitrary and potentially complicated targets of interest. The method is very versatile and convenient to use, as we illustrate on a wide range of examples. The code is available as Python and R packages drf.},
	number = {333},
	urldate = {2024-04-19},
	journal = {Journal of Machine Learning Research},
	author = {Cevid, Domagoj and Michel, Loris and Näf, Jeffrey and Bühlmann, Peter and Meinshausen, Nicolai},
	year = {2022},
	pages = {1--79},
}

@misc{amoukou_adaptive_2023,
	title = {Adaptive {Conformal} {Prediction} by {Reweighting} {Nonconformity} {Score}},
	url = {http://arxiv.org/abs/2303.12695},
	doi = {10.48550/arXiv.2303.12695},
	abstract = {Despite attractive theoretical guarantees and practical successes, Predictive Interval (PI) given by Conformal Prediction (CP) may not reflect the uncertainty of a given model. This limitation arises from CP methods using a constant correction for all test points, disregarding their individual uncertainties, to ensure coverage properties. To address this issue, we propose using a Quantile Regression Forest (QRF) to learn the distribution of nonconformity scores and utilizing the QRF's weights to assign more importance to samples with residuals similar to the test point. This approach results in PI lengths that are more aligned with the model's uncertainty. In addition, the weights learnt by the QRF provide a partition of the features space, allowing for more efficient computations and improved adaptiveness of the PI through groupwise conformalization. Our approach enjoys an assumption-free finite sample marginal and training-conditional coverage, and under suitable assumptions, it also ensures conditional coverage. Our methods work for any nonconformity score and are available as a Python package. We conduct experiments on simulated and real-world data that demonstrate significant improvements compared to existing methods.},
	urldate = {2024-04-19},
	publisher = {arXiv},
	author = {Amoukou, Salim I. and Brunel, Nicolas J. B.},
	month = may,
	year = {2023},
	note = {arXiv:2303.12695 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{johansson_regression_2014,
	title = {Regression conformal prediction with random forests},
	volume = {97},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-014-5453-0},
	doi = {10.1007/s10994-014-5453-0},
	abstract = {Regression conformal prediction produces prediction intervals that are valid, i.e., the probability of excluding the correct target value is bounded by a predefined confidence level. The most important criterion when comparing conformal regressors is efficiency; the prediction intervals should be as tight (informative) as possible. In this study, the use of random forests as the underlying model for regression conformal prediction is investigated and compared to existing state-of-the-art techniques, which are based on neural networks and k-nearest neighbors. In addition to their robust predictive performance, random forests allow for determining the size of the prediction intervals by using out-of-bag estimates instead of requiring a separate calibration set. An extensive empirical investigation, using 33 publicly available data sets, was undertaken to compare the use of random forests to existing state-of-the-art conformal predictors. The results show that the suggested approach, on almost all confidence levels and using both standard and normalized nonconformity functions, produced significantly more efficient conformal predictors than the existing alternatives.},
	language = {en},
	number = {1},
	urldate = {2024-04-19},
	journal = {Machine Learning},
	author = {Johansson, Ulf and Boström, Henrik and Löfström, Tuve and Linusson, Henrik},
	month = oct,
	year = {2014},
	keywords = {Conformal prediction, Random forests, Regression},
	pages = {155--176},
}

@incollection{papadopoulos_transductive_2013,
	address = {Berlin, Heidelberg},
	title = {Transductive conformal predictors},
	volume = {412},
	isbn = {978-3-642-41141-0 978-3-642-41142-7},
	url = {http://link.springer.com/10.1007/978-3-642-41142-7_36},
	abstract = {This paper discusses a transductive version of conformal predictors. This version is computationally ineﬃcient for big test sets, but it turns out that apparently crude “Bonferroni predictors” are about as good in their information eﬃciency and vastly superior in computational eﬃciency.},
	language = {en},
	urldate = {2024-04-19},
	booktitle = {Artificial {Intelligence} {Applications} and {Innovations}},
	publisher = {Springer Berlin Heidelberg},
	author = {Vovk, Vladimir},
	editor = {Papadopoulos, Harris and Andreou, Andreas S. and Iliadis, Lazaros and Maglogiannis, Ilias},
	year = {2013},
	doi = {10.1007/978-3-642-41142-7_36},
	note = {Series Title: IFIP Advances in Information and Communication Technology},
	pages = {348--360},
}

@misc{noauthor_out--sample_nodate,
	title = {Out-of-sample tests for conditional quantile coverage an application to {Growth}-at-{Risk} - {ScienceDirect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0304407623002063},
	urldate = {2024-04-15},
}

@incollection{diquigiovanni_conformal_2021,
	title = {A {Conformal} approach for functional data prediction},
	copyright = {All rights reserved},
	url = {https://www.research.unipd.it/handle/11577/3405020},
	urldate = {2024-03-17},
	booktitle = {Book of short papers {SIS} 2021, {Pisa}, 21-25 {Giugno} 2021},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	year = {2021},
	pages = {907--910},
}

@book{bertoni_handbook_2023,
	title = {Handbook of {Computational} {Social} {Science} for {Policy}},
	copyright = {All rights reserved},
	isbn = {978-3-031-16624-2},
	url = {https://link.springer.com/book/10.1007/978-3-031-16624-2},
	publisher = {Springer},
	editor = {Bertoni, Eleonora and Fontana, Matteo and Gabrielli, Lorenzo and Signorelli, Serena and Vespe, Michele},
	year = {2023},
}

@misc{centofanti_slasso_2021,
	title = {slasso: {S}-{LASSO} {Estimator} for the {Function}-on-{Function} {Linear} {Regression}},
	copyright = {GPL (≥ 3)},
	shorttitle = {slasso},
	url = {https://cran.r-project.org/web/packages/slasso/index.html},
	abstract = {Implements the smooth LASSO estimator for the function-on-function linear regression model described in Centofanti et al. (2020) {\textless}doi:10.48550/arXiv.2007.00529{\textgreater}.},
	urldate = {2024-04-11},
	author = {Centofanti, Fabio and Lepore, Antonio and Vantini, Simone and Fontana, Matteo},
	month = oct,
	year = {2021},
}

@misc{noauthor_easy_nodate,
	title = {Easy {Uncertainty} {Quantification} ({EasyUQ}): {Generating} {Predictive} {Distributions} from {Single}-{Valued} {Model} {Output}},
	shorttitle = {Easy {Uncertainty} {Quantification} ({EasyUQ})},
	url = {https://epubs.siam.org/doi/epdf/10.1137/22M1541915},
	language = {en},
	urldate = {2024-04-03},
	doi = {10.1137/22M1541915},
}

@article{berger_statistical_2019,
	title = {On the {Statistical} {Formalism} of {Uncertainty} {Quantification}},
	volume = {6},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-030718-105232},
	doi = {10.1146/annurev-statistics-030718-105232},
	abstract = {The use of models to try to better understand reality is ubiquitous. Models have proven useful in testing our current understanding of reality; for instance, climate models of the 1980s were built for science discovery, to achieve a better understanding of the general dynamics of climate systems. Scientific insights often take the form of general qualitative predictions (i.e., “under these conditions, the Earth’s poles will warm more than the rest of the planet”); such use of models differs from making quantitative forecasts of specific events (i.e. “high winds at noon tomorrow at London’s Heathrow Airport”). It is sometimes hoped that, after sufficient model development, any model can be used to make quantitative forecasts for any target system. Even if that were the case, there would always be some uncertainty in the prediction. Uncertainty quantification aims to provide a framework within which that uncertainty can be discussed and, ideally, quantified, in a manner relevant to practitioners using the forecast system. A statistical formalism has developed that claims to be able to accurately assess the uncertainty in prediction. This article is a discussion of if and when this formalism can do so. The article arose from an ongoing discussion between the authors concerning this issue, the second author generally being considerably more skeptical concerning the utility of the formalism in providing quantitative decision-relevant information.},
	language = {en},
	number = {1},
	urldate = {2024-03-26},
	journal = {Annual Review of Statistics and Its Application},
	author = {Berger, James O. and Smith, Leonard A.},
	month = mar,
	year = {2019},
	pages = {433--460},
}

@article{heckman_econometric_2022,
	title = {The {Econometric} {Model} for {Causal} {Policy} {Analysis}},
	volume = {14},
	issn = {1941-1383, 1941-1391},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-economics-051520-015456},
	doi = {10.1146/annurev-economics-051520-015456},
	abstract = {This article discusses the econometric model of causal policy analysis and two alternative frameworks that are popular in statistics and computer science. By employing the alternative frameworks uncritically, economists ignore the substantial advantages of an econometric approach, and this results in less informative analyses of economic policy. We show that the econometric approach to causality enables economists to characterize and analyze a wider range of policy problems than is allowed by alternative approaches.},
	language = {en},
	number = {Volume 14, 2022},
	urldate = {2024-03-26},
	journal = {Annual Review of Economics},
	author = {Heckman, James J. and Pinto, Rodrigo},
	month = aug,
	year = {2022},
	note = {Publisher: Annual Reviews},
	pages = {893--923},
}

@article{baker_household_2022,
	title = {Household {Financial} {Transaction} {Data}},
	volume = {14},
	issn = {1941-1383, 1941-1391},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-economics-051520-023646},
	doi = {10.1146/annurev-economics-051520-023646},
	abstract = {The growth of the availability and use of detailed household financial transaction micro data has dramatically expanded the ability of researchers to understand both household decision making and aggregate economic fluctuations across a wide range of fields. This class of transaction data is derived from a myriad of sources, including financial institutions, FinTech apps, and payment intermediaries. We review how these detailed data have been utilized in finance and economics research and analyze both their benefits and limitations as compared to more traditional measures of income, spending, and wealth. Finally, we discuss the future potential of this flexible class of data in firm-focused research, real-time policy analysis, and macro statistics.},
	language = {en},
	number = {Volume 14, 2022},
	urldate = {2024-03-26},
	journal = {Annual Review of Economics},
	author = {Baker, Scott R. and Kueng, Lorenz},
	month = aug,
	year = {2022},
	note = {Publisher: Annual Reviews},
	pages = {47--67},
}

@article{blomquist_econometrics_2023,
	title = {The {Econometrics} of {Nonlinear} {Budget} {Sets}},
	volume = {15},
	issn = {1941-1383, 1941-1391},
	url = {https://www.annualreviews.org/content/journals/10.1146/annurev-economics-082222-065323},
	doi = {10.1146/annurev-economics-082222-065323},
	abstract = {This article surveys the development of nonparametric models and methods for estimation of choice models with nonlinear budget sets. The discussion focuses on the budget set regression, that is, the conditional expectation of a choice variable given the budget set. Utility maximization in a nonparametric model with general heterogeneity reduces the curse of dimensionality in this regression. Empirical results using this regression are different from maximum likelihood and give informative inference. The article also considers the information provided by kink probabilities for nonparametric utility with general heterogeneity. Instrumental variable estimation and the evidence it provides of heterogeneity in preferences are also discussed.},
	language = {en},
	number = {Volume 15, 2023},
	urldate = {2024-03-26},
	journal = {Annual Review of Economics},
	author = {Blomquist, Sören and Hausman, Jerry A. and Newey, Whitney K.},
	month = sep,
	year = {2023},
	note = {Publisher: Annual Reviews},
	pages = {287--306},
}

@article{gneiting_model_2023,
	title = {Model {Diagnostics} and {Forecast} {Evaluation} for {Quantiles}},
	volume = {10},
	copyright = {http://creativecommons.org/licenses/by/4.0/},
	issn = {2326-8298, 2326-831X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-statistics-032921-020240},
	doi = {10.1146/annurev-statistics-032921-020240},
	abstract = {Model diagnostics and forecast evaluation are closely related tasks, with the former concerning in-sample goodness (or lack) of fit and the latter addressing predictive performance out-of-sample. We review the ubiquitous setting in which forecasts are cast in the form of quantiles or quantile-bounded prediction intervals. We distinguish unconditional calibration, which corresponds to classical coverage criteria, from the stronger notion of conditional calibration, as can be visualized in quantile reliability diagrams. Consistent scoring functions—including, but not limited to, the widely used asymmetricpiecewise linear score or pinball loss—provide for comparative assessment and ranking, and link to the coefficient of determination and skill scores. We illustrate the use of these tools on Engel's food expenditure data, the Global Energy Forecasting Competition 2014, and the US COVID-19 Forecast Hub.},
	language = {en},
	number = {1},
	urldate = {2024-03-26},
	journal = {Annual Review of Statistics and Its Application},
	author = {Gneiting, Tilmann and Wolffram, Daniel and Resin, Johannes and Kraus, Kristof and Bracher, Johannes and Dimitriadis, Timo and Hagenmeyer, Veit and Jordan, Alexander I. and Lerch, Sebastian and Phipps, Kaleb and Schienle, Melanie},
	month = mar,
	year = {2023},
	pages = {597--621},
}

@phdthesis{wang_machine_2023,
	type = {{PhD} {Thesis}},
	title = {Machine {Learning} in {Functional} {Data} {Analysis}},
	language = {en},
	school = {Simon Fraser University},
	author = {Wang, Haixu},
	year = {2023},
}

@article{giraldo_ordinary_2011,
	title = {Ordinary kriging for function-valued spatial data},
	volume = {18},
	issn = {1352-8505, 1573-3009},
	url = {http://link.springer.com/10.1007/s10651-010-0143-y},
	doi = {10.1007/s10651-010-0143-y},
	abstract = {In various scientiﬁc ﬁelds properties are represented by functions varying over space. In this paper, we present a methodology to make spatial predictions at non-data locations when the data values are functions. In particular, we propose both an estimator of the spatial correlation and a functional kriging predictor. We adapt an optimization criterion used in multivariable spatial prediction in order to estimate the kriging parameters. The curves are pre-processed by a non-parametric ﬁtting, where the smoothing parameters are chosen by cross-validation. The approach is illustrated by analyzing real data based on soil penetration resistances.},
	language = {en},
	number = {3},
	urldate = {2024-03-25},
	journal = {Environmental and Ecological Statistics},
	author = {Giraldo, R. and Delicado, P. and Mateu, J.},
	month = sep,
	year = {2011},
	pages = {411--426},
}

@misc{lundtorp_olsen_local_2023,
	title = {Local inference for functional data on manifold domains using permutation tests},
	url = {https://ui.adsabs.harvard.edu/abs/2023arXiv230607738L},
	doi = {10.48550/arXiv.2306.07738},
	abstract = {Pini and Vantini (2017) introduced the interval-wise testing procedure which performs local inference for functional data defined on an interval domain, where the output is an adjusted p-value function that controls for type I errors. We extend this idea to a general setting where domain is a Riemannian manifolds. This requires new methodology such as how to define adjustment sets on product manifolds and how to approximate the test statistic when the domain has non-zero curvature. We propose to use permutation tests for inference and apply the procedure in three settings: a simulation on a "chameleon-shaped" manifold and two applications related to climate change where the manifolds are a complex subset of \$S{\textasciicircum}2\$ and \$S{\textasciicircum}2 {\textbackslash}times S{\textasciicircum}1\$, respectively. We note the tradeoff between type I and type II errors: increasing the adjustment set reduces the type I error but also results in smaller areas of significance. However, some areas still remain significant even at maximal adjustment.},
	urldate = {2024-03-25},
	author = {Lundtorp Olsen, Niels and Pini, Alessia and Vantini, Simone},
	month = jun,
	year = {2023},
	note = {Publication Title: arXiv e-prints
ADS Bibcode: 2023arXiv230607738L},
	keywords = {62G10, 62H99, Statistics - Methodology},
}

@article{rimalova_inference_2022,
	title = {Inference for spatial regression models with functional response using a permutational approach},
	volume = {189},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001718},
	doi = {10.1016/j.jmva.2021.104893},
	abstract = {The aim of this work is to introduce an approach to null hypothesis significance testing in a functional linear model for spatial data. The proposed method is capable of dealing with the spatial structure of data by building a permutation testing procedure on spatially filtered residuals of a spatial regression model. Indeed, due to the spatial dependence existing among the data, the residuals of the regression model are not exchangeable, breaking the basic assumptions of the Freedman and Lane permutation scheme. Instead, it is proposed here to estimate the variance–covariance structure of the residuals by variography, remove this correlation by spatial filtering residuals and base the permutation test on these approximately exchangeable residuals. A simulation study is conducted to evaluate the performance of the proposed method in terms of empirical size and power, examining its behavior under different covariance settings. We show that neglecting the residuals spatial structure in the permutation scheme (thus permuting the correlated residuals directly) yields a very liberal testing procedures, whereas the proposed procedure is close to the nominal size of the test. The methodology is demonstrated on a real world data set on the amount of waste production in the Venice province of Italy.},
	urldate = {2024-03-25},
	journal = {Journal of Multivariate Analysis},
	author = {Římalová, Veronika and Fišerová, Eva and Menafoglio, Alessandra and Pini, Alessia},
	month = may,
	year = {2022},
	keywords = {Functional geostatistics, Nonparametric inference, Permutation tests, Spatial correlation, Spatial functional regression model},
	pages = {104893},
}

@article{maranzano_adaptive_2023,
	title = {Adaptive {LASSO} estimation for functional hidden dynamic geostatistical models},
	issn = {1436-3240},
	doi = {10.1007/s00477-023-02466-5},
	abstract = {We propose a novel model selection algorithm based on a penalized maximum likelihood estimator (PMLE) for functional hidden dynamic geostatistical models (f-HDGM). These models employ a classic mixed-effect regression structure with embedded spatiotemporal dynamics to model georeferenced data observed in a functional domain. Thus, the regression coefficients are functions. The algorithm simultaneously selects the relevant spline basis functions and regressors that are used to model the fixed effects. In this way, it automatically shrinks to zero irrelevant parts of the functional coefficients or the entire function for an irrelevant regressor. The algorithm is based on an adaptive LASSO penalty function, with weights obtained by the unpenalised f-HDGM maximum likelihood estimators. The computational burden of maximisation is drastically reduced by a local quadratic approximation of the log-likelihood. A Monte Carlo simulation study provides insight in prediction ability and parameter estimate precision, considering increasing spatiotemporal dependence and cross-correlations among predictors. Further, the algorithm behaviour is investigated when modelling air quality functional data with several weather and land cover covariates. Within this application, we also explore some scalability properties of our algorithm. Both simulations and empirical results show that the prediction ability of the penalised estimates are equivalent to those provided by the maximum likelihood estimates. However, adopting the so-called one-standard-error rule, we obtain estimates closer to the real ones, as well as simpler and more interpretable models.
SUPPLEMENTARY INFORMATION: The online version contains supplementary material available at 10.1007/s00477-023-02466-5.},
	language = {eng},
	journal = {Stochastic Environmental Research and Risk Assessment: Research Journal},
	author = {Maranzano, Paolo and Otto, Philipp and Fassò, Alessandro},
	month = may,
	year = {2023},
	pmid = {37362848},
	pmcid = {PMC10189237},
	keywords = {Adaptive LASSO, Air quality Lombardy, Functional HDGM, Geostatistical models, Model selection, Penalized maximum likelihood},
	pages = {1--23},
}

@article{ballari_spatial_2018,
	title = {Spatial functional data analysis for regionalizing precipitation seasonality and intensity in a sparsely monitored region: {Unveiling} the spatio-temporal dependencies of precipitation in {Ecuador}},
	volume = {38},
	copyright = {© 2018 Royal Meteorological Society},
	issn = {1097-0088},
	shorttitle = {Spatial functional data analysis for regionalizing precipitation seasonality and intensity in a sparsely monitored region},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/joc.5504},
	doi = {10.1002/joc.5504},
	abstract = {The identification of area-wise homogeneous precipitation regions helps to unveil similar precipitation patterns and amounts, where similar atmospheric processes at diverse temporal scales are likely to occur. However, although scientifically and socially relevant, the regionalization of precipitation is challenging, specially in areas of complex orography and with sparse monitoring. This limits our understanding of complex spatio-temporal dependencies and hinders any information-based resource management decision-making. Gridded satellite precipitation products are useful in this context, even though they contain bias errors. Spatial functional data analysis (sFDA) is a novel technique that considers time as well as space dependencies by means of spatial autocorrelation and complete time functions, one for each spatial point. Therefore, the aim of this study is to evaluate sFDA as a tool to regionalize seasonality and intensity precipitation patterns, having Ecuador as a case study. The Tropical Rainfall Measuring Mission (TRMM 3B43) satellite precipitation is used to create an exhaustive spatial delineation. To the best of our knowledge, this is the first time that a sFDA regionalization approach is performed on gridded satellite precipitation. The complex orography and heat-driven atmospheric processes in Ecuador's latitude make it a highly non-trivial case to test the aforementioned technique. As a result, five relevant regions of precipitation seasonality were spatially delineated and temporally characterized. Three of them were zonally oriented, and two meridional-wise in the coast. In addition, 20 relevant intensity regions across Ecuador were identified specially in regions with sparse monitoring. The regions were related to regional climate processes. However, limitations were found in regions with important orographic precipitation and locally variability patterns, probably due to the shortcomings of TRMM precipitation quantification. After the successful application of hierarchical regionalization using sFDA in a tropical region with sparse monitoring, it is reasonable to conclude that sFDA is a robust method to detect compact and meaningful homogeneous areas.},
	language = {en},
	number = {8},
	urldate = {2024-03-25},
	journal = {International Journal of Climatology},
	author = {Ballari, Daniela and Giraldo, Ramón and Campozano, Lenin and Samaniego, Esteban},
	year = {2018},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/joc.5504},
	keywords = {Ecuador, functional data analysis, geostatistic, intensity, precipitation, regionalization, seasonality, ungaged basins},
	pages = {3337--3354},
}

@article{ruiz-medina_functional_2014,
	title = {Functional time series analysis of spatio–temporal epidemiological data},
	volume = {28},
	issn = {1436-3259},
	url = {https://doi.org/10.1007/s00477-013-0794-y},
	doi = {10.1007/s00477-013-0794-y},
	abstract = {Spatio–temporal statistical models have been proposed for the analysis of the temporal evolution of the geographical pattern of mortality (or incidence) risks in disease mapping. However, as far as we know, functional approaches based on Hilbert-valued processes have not been used so far in this area. In this paper, the autoregressive Hilbertian process framework is adopted to estimate the functional temporal evolution of mortality relative risk maps. Specifically, the penalized functional estimation of log-relative risk maps is considered to smooth the classical standardized mortality ratio. The reproducing kernel Hilbert space (RKHS) norm is selected for definition of the penalty term. This RKHS-based approach is combined with the Kalman filtering algorithm for the spatio–temporal estimation of risk. Functional confidence intervals are also derived for detecting high risk areas. The proposed methodology is illustrated analyzing breast cancer mortality data in the provinces of Spain during the period 1975–2005. A simulation study is performed to compare the ARH(1) based estimation with the classical spatio–temporal conditional autoregressive approach.},
	language = {en},
	number = {4},
	urldate = {2024-03-25},
	journal = {Stochastic Environmental Research and Risk Assessment},
	author = {Ruiz-Medina, M. D. and Espejo, R. M. and Ugarte, M. D. and Militino, A. F.},
	month = may,
	year = {2014},
	keywords = {Disease mapping, Hilbert-valued processes, Kalman filtering, Penalized estimation, Spatio–temporal random fields},
	pages = {943--954},
}

@article{piter_helsinki_2022,
	title = {The {Helsinki} {Bike}-{Sharing} {System}—{Insights} {Gained} from a {Spatiotemporal} {Functional} {Model}},
	volume = {185},
	issn = {0964-1998, 1467-985X},
	url = {https://academic.oup.com/jrsssa/article/185/3/1294/7068886},
	doi = {10.1111/rssa.12834},
	abstract = {Abstract
            Understanding the usage patterns for bike-sharing systems is essential in terms of supporting and enhancing operational planning for such schemes. Studies have demonstrated how factors such as weather conditions influence the number of bikes that should be available at bike-sharing stations at certain times during the day. However, the influences of these factors usually vary over the course of a day, and if there is good temporal resolution, there could also be significant effects only for some hours/minutes (rush hours, the hours when shops are open and so forth). Thus, in this paper, an analysis of Helsinki's bike-sharing data from 2017 is conducted that considers full temporal and spatial resolutions. The station hire data are analysed in a spatiotemporal functional setting, where the number of bikes at a station is defined as a continuous function of the time of day. For this completely novel approach, we apply a functional spatiotemporal hierarchical model to investigate the effect of environmental factors and the magnitude of the spatial and temporal dependence. Challenges in computational complexity are faced using a Monte Carlo subsampling approach. The results show the necessity of splitting the bike-sharing stations into two clusters based on the similarity of their spatiotemporal functional observations in order to model the station hire data of Helsinki's bike-sharing system effectively.},
	language = {en},
	number = {3},
	urldate = {2024-03-25},
	journal = {Journal of the Royal Statistical Society Series A: Statistics in Society},
	author = {Piter, Andreas and Otto, Philipp and Alkhatib, Hamza},
	month = jul,
	year = {2022},
	pages = {1294--1318},
}

@article{otto_statistical_2021,
	title = {Statistical analysis of beach profiles – {A} spatiotemporal functional approach},
	volume = {170},
	issn = {0378-3839},
	url = {https://www.sciencedirect.com/science/article/pii/S0378383921001526},
	doi = {10.1016/j.coastaleng.2021.103999},
	abstract = {Beach profile data sets provide valuable insight into the morphological evolution of sandy shorelines. However, beach monitoring schemes often show large variability in temporal and spatial intervals between beach profiles. Moreover, beach profiles are often incomplete (i.e. only a part of the profile is measured) and data gaps are unavoidable. The resulting irregular sets of beach profiles complicate statistical analysis and previous studies on the morphological evolution and the effects of external influences have often omitted incomplete beach profiles. In this perspective, a statistical model is suggested to study beach profiles and to identify the effects of external influences. To be precise, the statistical model can be used (1) to determine the temporal and spatial variability of beach profiles while accounting for autoregressive dependencies in space and time, (2) to identify effects of external influences, (3) to predict complete beach profiles at unknown locations (i.e., interpolation between beach profiles), and (4) to forecast complete beach profiles accounting for external influences, such as storm events or nourishments. To illustrate the applicability of this model to irregular beach profile data, this state-of-the-art functional, spatiotemporal model was applied to beach profiles of the island of Sylt, Germany. In a first case study on submerged beach profiles, a decreasing temporal dependency between the profiles in the offshore direction was revealed, highlighting that less frequent measurements of offshore areas would suffice. A second analysis of the emerged beach profiles revealed the general effect of storm conditions (wave heights {\textgreater} 5 m) on subsequently measured beach profiles, which was statistically significant, and the profiles eroded with approximately 0.2–0.7 m in height. In summary, this study proposes and explores the application of a state-of-the-art statistical model to investigate beach profile changes from increasingly diverse and large profile data in coastal engineering and management.},
	urldate = {2024-03-25},
	journal = {Coastal Engineering},
	author = {Otto, Philipp and Piter, Andreas and Gijsman, Rik},
	month = dec,
	year = {2021},
	keywords = {Beach nourishment, Beach profiles, Coastal erosion, Functional data-driven model, Spatiotemporal statistics},
	pages = {103999},
}

@article{nerini_cokriging_2010,
	series = {Statistical {Methods} and {Problems} in {Infinite}-dimensional {Spaces}},
	title = {Cokriging for spatial functional data},
	volume = {101},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X0900061X},
	doi = {10.1016/j.jmva.2009.03.005},
	abstract = {This work proposes to generalize the method of kriging when data are spatially sampled curves. A spatial functional linear model is constructed including spatial dependencies between curves. Under some regularity conditions of the curves, an ordinary kriging system is established in the infinite dimensional case. From a practical point-of-view, the decomposition of the curves into a functional basis boils down the problem of kriging in infinite dimension to a standard cokriging on basis coefficients. The methodological developments are illustrated with temperature profiles sampled with dives of elephant seals in the Antarctic Ocean. The projection of sampled profiles into a Legendre polynomial basis is performed with a regularization procedure based on spline smoothing which uses the variance of the sampling devices in order to estimate coefficients by quadrature.},
	number = {2},
	urldate = {2024-03-25},
	journal = {Journal of Multivariate Analysis},
	author = {Nerini, David and Monestiez, Pascal and Manté, Claude},
	month = feb,
	year = {2010},
	keywords = {Cokriging, Coregionalization, Functional data analysis, Functional linear model, Legendre polynomials, RKHS},
	pages = {409--418},
}

@article{king_functional_2018,
	title = {A functional data analysis of spatiotemporal trends and variation in fine particulate matter},
	volume = {184},
	issn = {1352-2310},
	url = {https://www.sciencedirect.com/science/article/pii/S1352231018302267},
	doi = {10.1016/j.atmosenv.2018.04.001},
	abstract = {In this paper we illustrate the application of modern functional data analysis methods to study the spatiotemporal variability of particulate matter components across the United States. The approach models the pollutant annual profiles in a way that describes the dynamic behavior over time and space. This new technique allows us to predict yearly profiles for locations and years at which data are not available and also offers dimension reduction for easier visualization of the data. Additionally it allows us to study changes of pollutant levels annually or for a particular season. We apply our method to daily concentrations of two particular components of PM2.5 measured by two networks of monitoring sites across the United States from 2003 to 2015. Our analysis confirms existing findings and additionally reveals new trends in the change of the pollutants across seasons and years that may not be as easily determined from other common approaches such as Kriging.},
	urldate = {2024-03-25},
	journal = {Atmospheric Environment},
	author = {King, Meredith C. and Staicu, Ana-Maria and Davis, Jerry M. and Reich, Brian J. and Eder, Brian},
	month = jul,
	year = {2018},
	keywords = {Air pollution, Functional data, Functional principal component analysis, Kriging, Particulate matter},
	pages = {233--243},
}

@article{madonna_new_2022,
	title = {The {New} {Radiosounding} {HARMonization} ({RHARM}) {Data} {Set} of {Homogenized} {Radiosounding} {Temperature}, {Humidity}, and {Wind} {Profiles} {With} {Uncertainties}},
	volume = {127},
	copyright = {© 2021. The Authors.},
	issn = {2169-8996},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2021JD035220},
	doi = {10.1029/2021JD035220},
	abstract = {Observational records are more often than not influenced by residual non-climatic factors which must be detected and adjusted for prior to their usage. In this work, we present a novel approach, named Radiosounding HARMonization (RHARM), providing a homogenized data set of temperature, humidity and wind profiles along with an estimation of the measurement uncertainties for 697 radiosounding stations globally. The RHARM method has been used to adjust twice daily (0000 and 1200 UTC) radiosonde data holdings at 16 pressure levels in the range 1,000–10 hPa, from 1978 to present, provided by the Integrated Global Radiosonde Archive. Relative humidity (RH) data are limited to 250 hPa. The applied adjustments are interpolated to all reported levels. RHARM is the first data set to provide homogenized time series with an estimation of the observational uncertainty at each sounding pressure level. By construction, RHARM adjusted fields are not affected by cross-contamination of biases across stations and are fully independent of reanalysis data. Analysis of trends for temperature, RH and winds highlights increased geographical coherency of trends over 1978–2000 globally, but especially in the Northern Hemisphere and South America. RHARM shows warming trends of 0.39 K/decade at 300 hPa in the Northern Hemisphere and of 0.25 K/decade in the tropics. The RHARM adjustments also reduce differences with the European Centre for Medium-Range Weather Forecast ERA5 reanalysis, with the strongest effect in the Northern Hemisphere for temperature and relative humidity. For wind speed, the comparison indicates a good agreement with ERA5 in the troposphere.},
	language = {en},
	number = {2},
	urldate = {2024-03-25},
	journal = {Journal of Geophysical Research: Atmospheres},
	author = {Madonna, Fabio and Tramutola, Emanuele and Sy, Souleymane and Serva, Federico and Proto, Monica and Rosoldi, Marco and Gagliardi, Simone and Amato, Francesco and Marra, Fabrizio and Fassò, Alessandro and Gardiner, Tom and Thorne, Peter William},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2021JD035220},
	keywords = {homogenized data, observational uncertainty, relative humidity, temperature, upper-air data, wind},
	pages = {e2021JD035220},
}

@article{delicado_statistics_2010,
	title = {Statistics for spatial functional data: some recent contributions},
	volume = {21},
	copyright = {Copyright © 2009 John Wiley \& Sons, Ltd.},
	issn = {1099-095X},
	shorttitle = {Statistics for spatial functional data},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.1003},
	doi = {10.1002/env.1003},
	abstract = {Functional data analysis (FDA) is a relatively new branch in statistics. Experiments where a complete function is observed for each individual give rise to functional data. In this work we focus on the case of functional data presenting spatial dependence. The three classic types of spatial data structures (geostatistical data, point patterns, and areal data) can be combined with functional data as it is shown in the examples of each situation provided here. We also review some contributions in the literature on spatial functional data. Copyright © 2009 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {3-4},
	urldate = {2024-03-25},
	journal = {Environmetrics},
	author = {Delicado, P. and Giraldo, R. and Comas, C. and Mateu, J.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.1003},
	keywords = {areal data, functional data analysis, geostatistical data, point processes, smoothing},
	pages = {224--239},
}

@article{petropoulos_forecasting_2022,
	title = {Forecasting: theory and practice},
	volume = {38},
	issn = {0169-2070},
	shorttitle = {Forecasting},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207021001758},
	doi = {10.1016/j.ijforecast.2021.11.001},
	abstract = {Forecasting has always been at the forefront of decision making and planning. The uncertainty that surrounds the future is both exciting and challenging, with individuals and organisations seeking to minimise risks and maximise utilities. The large number of forecasting applications calls for a diverse set of forecasting methods to tackle real-life challenges. This article provides a non-systematic review of the theory and the practice of forecasting. We provide an overview of a wide range of theoretical, state-of-the-art models, methods, principles, and approaches to prepare, produce, organise, and evaluate forecasts. We then demonstrate how such theoretical concepts are applied in a variety of real-life contexts. We do not claim that this review is an exhaustive list of methods and applications. However, we wish that our encyclopedic presentation will offer a point of reference for the rich work that has been undertaken over the last decades, with some key insights for the future of forecasting theory and practice. Given its encyclopedic nature, the intended mode of reading is non-linear. We offer cross-references to allow the readers to navigate through the various topics. We complement the theoretical concepts and applications covered by large lists of free or open-source software implementations and publicly-available databases.},
	number = {3},
	urldate = {2024-03-20},
	journal = {International Journal of Forecasting},
	author = {Petropoulos, Fotios and Apiletti, Daniele and Assimakopoulos, Vassilios and Babai, Mohamed Zied and Barrow, Devon K. and Ben Taieb, Souhaib and Bergmeir, Christoph and Bessa, Ricardo J. and Bijak, Jakub and Boylan, John E. and Browell, Jethro and Carnevale, Claudio and Castle, Jennifer L. and Cirillo, Pasquale and Clements, Michael P. and Cordeiro, Clara and Cyrino Oliveira, Fernando Luiz and De Baets, Shari and Dokumentov, Alexander and Ellison, Joanne and Fiszeder, Piotr and Franses, Philip Hans and Frazier, David T. and Gilliland, Michael and Gönül, M. Sinan and Goodwin, Paul and Grossi, Luigi and Grushka-Cockayne, Yael and Guidolin, Mariangela and Guidolin, Massimo and Gunter, Ulrich and Guo, Xiaojia and Guseo, Renato and Harvey, Nigel and Hendry, David F. and Hollyman, Ross and Januschowski, Tim and Jeon, Jooyoung and Jose, Victor Richmond R. and Kang, Yanfei and Koehler, Anne B. and Kolassa, Stephan and Kourentzes, Nikolaos and Leva, Sonia and Li, Feng and Litsiou, Konstantia and Makridakis, Spyros and Martin, Gael M. and Martinez, Andrew B. and Meeran, Sheik and Modis, Theodore and Nikolopoulos, Konstantinos and Önkal, Dilek and Paccagnini, Alessia and Panagiotelis, Anastasios and Panapakidis, Ioannis and Pavía, Jose M. and Pedio, Manuela and Pedregal, Diego J. and Pinson, Pierre and Ramos, Patrícia and Rapach, David E. and Reade, J. James and Rostami-Tabar, Bahman and Rubaszek, Michał and Sermpinis, Georgios and Shang, Han Lin and Spiliotis, Evangelos and Syntetos, Aris A. and Talagala, Priyanga Dilini and Talagala, Thiyanga S. and Tashman, Len and Thomakos, Dimitrios and Thorarinsdottir, Thordis and Todini, Ezio and Trapero Arenas, Juan Ramón and Wang, Xiaoqian and Winkler, Robert L. and Yusupova, Alisa and Ziel, Florian},
	month = jul,
	year = {2022},
	keywords = {Applications, Encyclopedia, Methods, Prediction, Principles, Review, Time series},
	pages = {705--871},
}

@book{marron_object_2021,
	address = {Boca Raton},
	title = {Object oriented data analysis},
	isbn = {978-0-8153-9282-8 978-1-03-211480-4},
	abstract = {"Object Oriented Data Analysis (OODA) provides a useful general framework for the consideration of many types of Complex Data. It is deliberately intended to be particularly useful in the analysis of data in complicated situations which are typically not easily represented as an unconstrained matrix of numbers"--},
	publisher = {Taylor \& Francis Group, LLC},
	author = {Marron, James Stephen and Dryden, I. L.},
	year = {2021},
	keywords = {Methodology, Object-oriented methods (Computer science), Quantitative research, Statistics},
}

@incollection{centofanti_sparse_2019,
	title = {A sparse estimator for the function-on-function linear regression model},
	copyright = {All rights reserved},
	url = {https://www.iris.unina.it/handle/11588/763509},
	urldate = {2024-03-17},
	booktitle = {Smart statistics for smart applications},
	publisher = {ITA},
	author = {Centofanti, Fabio and Fontana, Matteo and Lepore, Antonio and Vantini, Simone},
	year = {2019},
	pages = {763--767},
}

@inproceedings{centofanti_sparse_2019-1,
	title = {Sparse estimation of function-on-function non-concurrent linear regression models},
	copyright = {All rights reserved},
	url = {https://www.iris.unina.it/handle/11588/763512},
	urldate = {2024-03-17},
	booktitle = {Abstract at the {Final} {CRoNoS} {Events} and {Workshop} on {Multivariate} {Data} {Analysis} 2019},
	author = {Centofanti, F. and Fontana, M. and Lepore, A. and Vantini, S.},
	year = {2019},
}

@misc{noauthor_conformal_nodate,
	title = {‪{A} {Conformal} approach for multivariate functional data prediction‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=it&user=U7jODH8AAAAJ&cstart=20&pagesize=80&citation_for_view=U7jODH8AAAAJ:qxL8FJ1GzNcC},
	abstract = {‪J Diquigiovanni, M Fontana, S Vantini‬, ‪IWFOS 2021: Book of Abstracts‬},
	urldate = {2024-03-17},
}

@incollection{fontana_functional_2020,
	title = {Functional {Data} {Analysis} for {Interferometric} {Synthetic} {Aperture} {Radar} {Data} {Post}-{Processing}: {The} case of {Santa} {Barbara} mud volcano},
	copyright = {All rights reserved},
	shorttitle = {Functional {Data} {Analysis} for {Interferometric} {Synthetic} {Aperture} {Radar} {Data} {Post}-{Processing}},
	url = {https://re.public.polimi.it/handle/11311/1155724},
	urldate = {2024-03-17},
	booktitle = {Book of {Short} {Papers}-{SIS} 2020},
	publisher = {Pearson},
	author = {Fontana, Matteo and Menafoglio, Alessandra and Cigna, Francesca and Tapete, Deodato},
	year = {2020},
	pages = {164--169},
}

@misc{noauthor_conformal_nodate-1,
	title = {‪{Conformal} clustering for functional variables, with an application to electricity consumption curves‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=it&user=U7jODH8AAAAJ&cstart=20&pagesize=80&citation_for_view=U7jODH8AAAAJ:5nxA0vEk-isC},
	abstract = {‪I Nouretdinov, M Fontana, J Gammerman, L Shelmit, D Rehal, P Giovannotti‬, ‪Conformal and Probabilistic Prediction and Applications, 2019‬},
	urldate = {2024-03-17},
}

@misc{noauthor_conformal_nodate-2,
	title = {‪{Conformal} {Prediction} {Sets} for {Populations} of {Graphs}‬},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=it&user=U7jODH8AAAAJ&cstart=20&pagesize=80&citation_for_view=U7jODH8AAAAJ:mVmsd5A6BfQC},
	abstract = {‪A Calissano, M Fontana, G Zeni, S Vantini‬, ‪MOX-Report, 2021‬},
	urldate = {2024-03-17},
}

@misc{noauthor_slasso_nodate,
	title = {‪slasso: {Smooth} {LASSO} {Estimator} for the {Function}-on-{Function} {Linear} {Regression}‬},
	copyright = {All rights reserved},
	shorttitle = {‪slasso},
	url = {https://scholar.google.com/citations?view_op=view_citation&hl=it&user=U7jODH8AAAAJ&citation_for_view=U7jODH8AAAAJ:ZeXyd9-uunAC},
	abstract = {‪F Centofanti, A Lepore, S Vantini, M Fontana‬, ‪https://cran.r-project.org/package=slasso, 2022‬},
	urldate = {2024-03-17},
}

@misc{ghirri_evaluation_2023,
	title = {An {Evaluation} of {Researchers}' {Migration} {Patterns} in {Europe} using {Digital} {Trace} {Data}},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/2302.07764},
	abstract = {The comprehension of the mechanisms behind the mobility of skilled workers is of paramount importance for policy making. The lacking nature of official measurements motivates the use of digital trace data extracted from ORCID public records. We use such data to investigate European regions, studied at NUTS2 level, over the time horizon of 2009 to 2020. We present a novel perspective where regions roles are dictated by the overall activity of the research community, contradicting the common brain drain interpretation of the phenomenon. We find that a high mobility is usually correlated with strong university prestige, high magnitude of investments and an overall good schooling level in a region.},
	urldate = {2024-03-17},
	publisher = {arXiv},
	author = {Ghirri, Jacopo and Mastropietro, Marta and Vantini, Simone and Ieva, Francesca and Fontana, Matteo},
	month = feb,
	year = {2023},
	note = {arXiv:2302.07764 [stat]},
	keywords = {Statistics - Applications},
}

@incollection{bertoni_challenges_2023,
	address = {Cham},
	title = {Challenges and {Opportunities} of {Computational} {Social} {Science} for {Official} {Statistics}},
	copyright = {All rights reserved},
	isbn = {978-3-031-16623-5 978-3-031-16624-2},
	url = {https://link.springer.com/10.1007/978-3-031-16624-2_10},
	abstract = {Abstract
            The vast amount of data produced everyday (so-called digital traces) and available nowadays represent a gold mine for the social sciences, especially in a computational context, that allows to fully extract their informational and knowledge value. In the latest years, statistical offices have made efforts to profit from harnessing the potential offered by these new sources of data, with promising results. But how difficult is this integration process? What are the challenges that statistical offices would likely face to profit from new data sources and analytical methods? This chapter will start by setting the scene of the current official statistics system, with a focus on its fundamental principles and dimensions relevant to the use of non-traditional data. It will then present some experiments and proofs of concept in the context of data innovation for official statistics, followed by a discussion on prospective challenges related to sustainable data access, new technical and methodological approaches and effective use of new sources of data.},
	language = {en},
	urldate = {2024-03-17},
	booktitle = {Handbook of {Computational} {Social} {Science} for {Policy}},
	publisher = {Springer International Publishing},
	author = {Signorelli, Serena and Fontana, Matteo and Gabrielli, Lorenzo and Vespe, Michele},
	editor = {Bertoni, Eleonora and Fontana, Matteo and Gabrielli, Lorenzo and Signorelli, Serena and Vespe, Michele},
	year = {2023},
	doi = {10.1007/978-3-031-16624-2_10},
	pages = {195--211},
}

@incollection{fontana_functional_2019,
	title = {Functional {Data} {Analysis}-based sensitivity analysis of {Integrated} {Assessment} {Models} for {Climate} {Change} {Modelling}},
	url = {https://re.public.polimi.it/handle/11311/1102920},
	urldate = {2024-03-17},
	booktitle = {Smart {Statistics} for {Smart} {Applications}-{Books} of {Short} {Papers} {SIS2019}},
	publisher = {Pearson},
	author = {Fontana, Matteo and Tavoni, Massimo and Vantini, Simone},
	year = {2019},
	pages = {313--317},
}

@article{diquigiovanni_importance_2025,
	title = {The {Importance} of {Being} a {Band}: {Finite}-{Sample} {Exact} {Distribution}-{Free} {Prediction} {Sets} for {Functional} {Data}},
	copyright = {All rights reserved},
	issn = {10170405},
	shorttitle = {The {Importance} of {Being} a {Band}},
	url = {https://www3.stat.sinica.edu.tw/ss_newpaper/SS-2022-0087_na.pdf},
	doi = {10.5705/ss.202022.0087},
	language = {en},
	urldate = {2024-03-17},
	journal = {Statistica Sinica},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	year = {2025},
}

@article{paries_ecb_nodate,
	title = {{ECB} macroeconometric models for forecasting and policy analysis},
	language = {en},
	number = {344},
	author = {Pariès, Matthieu Darracq and Priftis, Romanos},
}

@article{gattuso_mud_2021,
	title = {The mud volcanoes at {Santa} {Barbara} and {Aragona} ({Sicily}, {Italy}): a contribution to risk assessment},
	volume = {21},
	issn = {1561-8633},
	shorttitle = {The mud volcanoes at {Santa} {Barbara} and {Aragona} ({Sicily}, {Italy})},
	url = {https://nhess.copernicus.org/articles/21/3407/2021/},
	doi = {10.5194/nhess-21-3407-2021},
	abstract = {The Santa Barbara and Aragona areas are affected by mud volcanism (MV) phenomena, consisting of continuous or intermittent emission of mud, water, and gases. This activity could be interrupted by paroxysmal events, with an eruptive column composed mainly of clay material, water, and gases. They are the most hazardous phenomena, and today it is impossible to define the potential parameters for modelling the phenomenon. In 2017, two digital surface models (DSMs) were performed by drone in both areas, thus allowing the mapping of the emission zones and the covered areas by the previous events.

 Detailed information about past paroxysms was obtained from historical sources, and, with the analysis of the 2017 DSMs, a preliminary hazard assessment was carried out for the first time at two sites. Two potentially hazardous paroxysm surfaces of 0.12 and 0.20 km2 for Santa Barbara and Aragona respectively were defined. In May 2020, at Aragona, a new paroxysm covered a surface of 8721 m2. After this, a new detailed DSM was collected with the aim to make a comparison with the 2017 one. Since 2017, a seismic station was installed in Santa Barbara. From preliminary results, both seismic events and ambient noise showed a frequency of 5–10 Hz.},
	language = {English},
	number = {11},
	urldate = {2024-03-13},
	journal = {Natural Hazards and Earth System Sciences},
	author = {Gattuso, Alessandro and Italiano, Francesco and Capasso, Giorgio and D'Alessandro, Antonino and Grassa, Fausto and Pisciotta, Antonino Fabio and Romano, Davide},
	month = nov,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {3407--3419},
}

@misc{catania_economic_2021,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Economic {Vulnerability} {Is} {State} {Dependent}},
	url = {https://papers.ssrn.com/abstract=3821668},
	doi = {10.2139/ssrn.3821668},
	abstract = {A novel dynamic model for joint estimation of multiple quantiles of a time series conditionally on a set of covariates is presented. The model preserves quantile monotonicity and allows for a clear interpretation of covariate effects across quantiles. Model parameters are estimated using a two-step M-estimator. The resulting estimator is consistent, and its finite sample properties are analysed through simulations. The new model is used to study the impact of different levels of stress in the financial system  on GDP growth rate. The analysis shows that worsened financial conditions imply a more pessimistic economic outlook when the financial scenario is already severely distressed, and an overall increased macroeconomic uncertainty. Additionally, past information on GDP growth is found to be critical in studying and predicting economic vulnerability.  These findings hold true even when alternative measures of real economic activity are considered.},
	language = {en},
	urldate = {2024-03-13},
	author = {Catania, Leopoldo and Luati, Alessandra and Vallarino, Pierluigi},
	month = apr,
	year = {2021},
	keywords = {Dynamic quantiles, Growth-at-Risk, Macro-Finance, Score-driven models},
}

@misc{ingv_comunicato_2008,
	title = {Comunicato sull’eruzione di fango in {C}.da {Terrapelata} {Santa} {Barbara} ({Cl}) 11 {Agosto} 2008 – {Aggiornamento} del 16 {Agosto}},
	url = {Availble at: http://www.pa.ingv.it/},
	urldate = {2017-03-01},
	publisher = {Istituto Nazionale di Geofisica e Vulcanologia, Sezione di Palermo},
	author = {{INGV}},
	year = {2008},
}

@techreport{regione_siciliana_emergenza_2008,
	title = {Emergenza “{Maccalube}” dell’11 {Agosto} 2008 nel {Comune} di {Caltanissetta}, {Descrizione} dell’evento e dei danni},
	institution = {Regione Siciliana, Presidenza, Dipartimento della Protezione Civile, Servizio di Caltanissetta},
	author = {{Regione Siciliana}},
	month = sep,
	year = {2008},
	pages = {30},
}

@article{bonini_mud_2012,
	title = {Mud volcanoes: {Indicators} of stress orientation and tectonic controls},
	volume = {115},
	issn = {0012-8252},
	shorttitle = {Mud volcanoes},
	url = {https://www.sciencedirect.com/science/article/pii/S0012825212001134},
	doi = {10.1016/j.earscirev.2012.09.002},
	abstract = {This study examines the use of specific mud volcano features (i.e., elongated calderas, aligned vents and elongated volcanoes) as potential indicators of tectonic stress orientation. The stress indicator principles, widely recognised for magmatic systems, have been discussed and applied to mud volcano settings such as in the Northern Apennines and the Azerbaijan Greater Caucasus, as well as in other instances where the analysis was fully based on a remote sensing study. The results of these applications are promising, the obtained maximum horizontal stress (SH) directions generally showing a good correlation with those determined in the upper crust by classical methods (i.e., earthquake focal mechanism solutions, well bore breakouts). Therefore, stress information from mud volcanoes could be used as a proxy for stress orientation (1) where stress data is lacking, (2) where settings are inaccessible (i.e., underwater or the surface of planets), or simply (3) as supplementary stress indicators. This study also pays special attention to structural elements that may control fluid expulsion at various length scales, and pathways that should have spawned the mud volcanoes and controlled their paroxysmal events and eruptions. Different types of sub-planar brittle elements have been found to focus fluid flow rising up-through fold cores, where the vertical zonation of stresses may take part in this process by creating distinctive feeder fracture/fault sets. On a regional scale, mud volcanoes in active fold-and-thrust belts may occur over wider areas, such as the prolific mud volcanism in Azerbaijan, or may cluster along discrete structures like the steep Pede-Apennine thrust in the Northern Apennines, where the generation of overpressures is expected to establish a positive feedback loop allowing for fault movement and mud volcanism.},
	number = {3},
	urldate = {2024-03-11},
	journal = {Earth-Science Reviews},
	author = {Bonini, Marco},
	month = nov,
	year = {2012},
	keywords = {Azerbaijan Greater Caucasus, Brittle elements, Fold-and-thrust belts, Mud volcanism, Northern Apennines, Stress indicators},
	pages = {121--152},
}

@article{brighenti_uav_2021,
	title = {{UAV} survey method to monitor and analyze geological hazards: the case study of the mud volcano of {Villaggio} {Santa} {Barbara}, {Caltanissetta} ({Sicily})},
	volume = {21},
	issn = {1561-8633},
	shorttitle = {{UAV} survey method to monitor and analyze geological hazards},
	url = {https://nhess.copernicus.org/articles/21/2881/2021/},
	doi = {10.5194/nhess-21-2881-2021},
	abstract = {Active geological processes often generate a ground surface response such as uplift, subsidence and faulting/fracturing. Nowadays remote sensing represents a key tool for the evaluation and monitoring of natural hazards. The use of unmanned aerial vehicles (UAVs) in relation to observations of natural hazards encompasses three main stages: pre- and post-event data acquisition, monitoring, and risk assessment. The mud volcano of Santa Barbara (Municipality of Caltanissetta, Italy) represents a dangerous site because on 11 August 2008 a paroxysmal event caused serious damage to infrastructures within a range of about 2 km. The main precursors to mud volcano paroxysmal events are uplift and the development of structural features with dimensions ranging from centimeters to decimeters. Here we present a methodology for monitoring deformation processes that may be precursory to paroxysmal events at the Santa Barbara mud volcano. This methodology is based on (i) the data collection, (ii) the structure from motion (SfM) processing chain and (iii) the M3C2-PM algorithm for the comparison between point clouds and uncertainty analysis with a statistical approach. The objective of this methodology is to detect precursory activity by monitoring deformation processes with centimeter-scale precision and a temporal frequency of 1–2 months.},
	language = {English},
	number = {9},
	urldate = {2024-03-11},
	journal = {Natural Hazards and Earth System Sciences},
	author = {Brighenti, Fabio and Carnemolla, Francesco and Messina, Danilo and De Guidi, Giorgio},
	month = sep,
	year = {2021},
	note = {Publisher: Copernicus GmbH},
	pages = {2881--2898},
}

@article{moro_new_2017,
	title = {New insights into earthquake precursors from {InSAR}},
	volume = {7},
	copyright = {2017 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-017-12058-3},
	doi = {10.1038/s41598-017-12058-3},
	abstract = {We measured ground displacements before and after the 2009 L’Aquila earthquake using multi-temporal InSAR techniques to identify seismic precursor signals. We estimated the ground deformation and its temporal evolution by exploiting a large dataset of SAR imagery that spans seventy-two months before and sixteen months after the mainshock. These satellite data show that up to 15 mm of subsidence occurred beginning three years before the mainshock. This deformation occurred within two Quaternary basins that are located close to the epicentral area and are filled with sediments hosting multi-layer aquifers. After the earthquake, the same basins experienced up to 12 mm of uplift over approximately nine months. Before the earthquake, the rocks at depth dilated, and fractures opened. Consequently, fluids migrated into the dilated volume, thereby lowering the groundwater table in the carbonate hydrostructures and in the hydrologically connected multi-layer aquifers within the basins. This process caused the elastic consolidation of the fine-grained sediments within the basins, resulting in the detected subsidence. After the earthquake, the fractures closed, and the deep fluids were squeezed out. The pre-seismic ground displacements were then recovered because the groundwater table rose and natural recharge of the shallow multi-layer aquifers occurred, which caused the observed uplift.},
	language = {en},
	number = {1},
	urldate = {2024-03-07},
	journal = {Scientific Reports},
	author = {Moro, Marco and Saroli, Michele and Stramondo, Salvatore and Bignami, Christian and Albano, Matteo and Falcucci, Emanuela and Gori, Stefano and Doglioni, Carlo and Polcari, Marco and Tallini, Marco and Macerola, Luca and Novali, Fabrizio and Costantini, Mario and Malvarosa, Fabio and Wegmüller, Urs},
	month = sep,
	year = {2017},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {Natural hazards, Seismology},
	pages = {12035},
}

@article{cigna_land_2022,
	title = {Land {Subsidence} and {Aquifer}-{System} {Storage} {Loss} in {Central} {Mexico}: {A} {Quasi}-{Continental} {Investigation} {With} {Sentinel}-1 {InSAR}},
	volume = {49},
	copyright = {© 2022. The Authors.},
	issn = {1944-8007},
	shorttitle = {Land {Subsidence} and {Aquifer}-{System} {Storage} {Loss} in {Central} {Mexico}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022GL098923},
	doi = {10.1029/2022GL098923},
	abstract = {Aquifers play an important role in addressing water needs worldwide. When overexploited, they may lose storage and compact, causing land subsidence and impacts on urban landscapes. Using Sentinel-1 satellite imagery, we perform the largest ever-made Interferometric Synthetic Aperture Radar (InSAR) survey over Mexico, across a 700,000 km2 area hosting {\textgreater}85.2 million inhabitants. We estimate present-day subsidence rates for yet unmapped and well-known hotspots (e.g., −45 cm/year in Mexico City), and compute compaction volumes at {\textgreater}300 aquifer-systems (up to −60 hm3/year at Mexico City Metropolitan Area). InSAR-derived aquifer-system compaction generally correlates well with groundwater deficits, extractions and storage changes from management reports. Semi-theoretical relationships for the whole Central Mexico and hydrological-administrative regions VII, VIII, and XIII, enable the assessment of compaction rates and volumes resulting from groundwater exploitation. These could be used to inform groundwater management strategies towards adaptation to climate change and future needs of a growing population.},
	language = {en},
	number = {15},
	urldate = {2024-03-07},
	journal = {Geophysical Research Letters},
	author = {Cigna, F. and Tapete, D.},
	year = {2022},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022GL098923},
	pages = {e2022GL098923},
}

@article{lanari_automatic_2020,
	title = {Automatic {Generation} of {Sentinel}-1 {Continental} {Scale} {DInSAR} {Deformation} {Time} {Series} through an {Extended} {P}-{SBAS} {Processing} {Pipeline} in a {Cloud} {Computing} {Environment}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/18/2961},
	doi = {10.3390/rs12182961},
	abstract = {We present in this work an advanced processing pipeline for continental scale differential synthetic aperture radar (DInSAR) deformation time series generation, which is based on the parallel small baseline subset (P-SBAS) approach and on the joint exploitation of Sentinel-1 (S-1) interferometric wide swath (IWS) SAR data, continuous global navigation satellite system (GNSS) position time-series, and cloud computing (CC) resources. We first briefly describe the basic rationale of the adopted P-SBAS processing approach, tailored to deal with S-1 IWS SAR data and to be implemented in a CC environment, highlighting the innovative solutions that have been introduced in the processing chain we present. They mainly consist in a series of procedures that properly exploit the available GNSS time series with the aim of identifying and filtering out possible residual atmospheric artifacts that may affect the DInSAR measurements. Moreover, significant efforts have been carried out to improve the P-SBAS processing pipeline automation and robustness, which represent crucial issues for interferometric continental scale analysis. Then, a massive experimental analysis is presented. In this case, we exploit: (i) the whole archive of S-1 IWS SAR images acquired over a large portion of Europe, from descending orbits, (ii) the continuous GNSS position time series provided by the Nevada Geodetic Laboratory at the University of Nevada, Reno, USA (UNR-NGL) available for the investigated area, and (iii) the ONDA platform, one of the Copernicus Data and Information Access Services (DIAS). The achieved results demonstrate the capability of the proposed solution to successfully retrieve the DInSAR time series relevant to such a huge area, opening new scenarios for the analysis and interpretation of these ground deformation measurements.},
	language = {en},
	number = {18},
	urldate = {2024-03-07},
	journal = {Remote Sensing},
	author = {Lanari, Riccardo and Bonano, Manuela and Casu, Francesco and Luca, Claudio De and Manunta, Michele and Manzo, Mariarosaria and Onorato, Giovanni and Zinno, Ivana},
	month = jan,
	year = {2020},
	note = {Number: 18
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {DIAS, DInSAR, GNSS, P-SBAS, Sentinel-1, deformation time series},
	pages = {2961},
}

@article{yao_landslide_2022,
	title = {Landslide {Detection} and {Mapping} {Based} on {SBAS}-{InSAR} and {PS}-{InSAR}: {A} {Case} {Study} in {Gongjue} {County}, {Tibet}, {China}},
	volume = {14},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	shorttitle = {Landslide {Detection} and {Mapping} {Based} on {SBAS}-{InSAR} and {PS}-{InSAR}},
	url = {https://www.mdpi.com/2072-4292/14/19/4728},
	doi = {10.3390/rs14194728},
	abstract = {The rock mass along the Jinsha River is relatively broken under complex geological action. Many ancient landslides were distributed along the Jinsha River in Gongjue County, which is very dangerous under the action of gravity, tectonic stress and river erosion. Efficient and accurate identification and monitoring of landslides is important for disaster monitoring and early warning. Interferometric synthetic aperture radar (InSAR) technology has been proved to be an effective technology for landslide hazard identification and mapping. However, great uncertainty inevitably exists due to the single deformation observation method, resulting in wrong judgment during the process of landslide detection. Therefore, to address the uncertainties arising from single observations, a cross-comparison method is put forward using SBAS-InSAR (small baseline subset InSAR) and PS-InSAR (permanent scatterers InSAR) technology. Comparative analysis of the spatial complementarity of interference points and temporal deformation refined the deformation characteristics and verified the reliability of the InSAR results, aiding in the comprehensive identification and further mapping of landslides. Landslides along the Jinsha River in Gongjue County were studied in this paper. Firstly, 14 landslides with a total area of 20 km2 were identified by using two time-series InSAR methods. Then, the deformation characteristics of these landslides were validated by UAV (unmanned aerial vehicle) images, multiresource remote sensing data and field investigation. Further, the precipitation data were introduced to analyze the temporal deformation characteristics of two large landslides. Lastly, the influence of fault activity on landslide formation is further discussed. Our results demonstrate that the cross-comparison of the time-series InSAR method can effectively verify the accuracy of landslide identification.},
	language = {en},
	number = {19},
	urldate = {2024-03-07},
	journal = {Remote Sensing},
	author = {Yao, Jiaming and Yao, Xin and Liu, Xinghong},
	month = jan,
	year = {2022},
	note = {Number: 19
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {InSAR, landslide detection, rainfall, remote sensing, time-series deformation},
	pages = {4728},
}

@article{li_review_2022,
	series = {Advances in {InSAR} algorithms, deformation monitoring and source parameter inversion},
	title = {Review of the {SBAS} {InSAR} {Time}-series algorithms, applications, and challenges},
	volume = {13},
	issn = {1674-9847},
	url = {https://www.sciencedirect.com/science/article/pii/S1674984721000860},
	doi = {10.1016/j.geog.2021.09.007},
	abstract = {In the past 30 years, the small baseline subset (SBAS) InSAR time-series technique has emerged as an essential tool for measuring slow surface displacement and estimating geophysical parameters. Because of its ability to monitor large-scale deformation with millimeter accuracy, the SBAS method has been widely used in various geodetic fields, such as ground subsidence, landslides, and seismic activity. The obtained long-term time-series cumulative deformation is vital for studying the deformation mechanism. This article reviews the algorithms, applications, and challenges of the SBAS method. First, we recall the fundamental principle and analyze the shortcomings of the traditional SBAS algorithm, which provides a basic framework for the following improved time series methods. Second, we classify the current improved SBAS techniques from different perspectives: solving the ill-posed equation, increasing the density of high-coherence points, improving the accuracy of monitoring deformation and measuring the multi-dimensional deformation. Third, we summarize the application of the SBAS method in monitoring ground subsidence, permafrost degradation, glacier movement, volcanic activity, landslides, and seismic activity. Finally, we discuss the difficulties faced by the SBAS method and explore its future development direction.},
	number = {2},
	urldate = {2024-03-07},
	journal = {Geodesy and Geodynamics},
	author = {Li, Shaowei and Xu, Wenbin and Li, Zhiwei},
	month = mar,
	year = {2022},
	keywords = {Deformation, InSAR, Small baseline subset, Time-series InSAR},
	pages = {114--126},
}

@article{cigna_accuracy_2021,
	title = {Accuracy of {Sentinel}-1 {PSI} and {SBAS} {InSAR} {Displacement} {Velocities} against {GNSS} and {Geodetic} {Leveling} {Monitoring} {Data}},
	volume = {13},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/13/23/4800},
	doi = {10.3390/rs13234800},
	abstract = {Correct use of multi-temporal Interferometric Synthetic Aperture Radar (InSAR) datasets to complement geodetic surveying for geo-hazard applications requires rigorous assessment of their precision and accuracy. Published inter-comparisons are mostly limited to ground displacement estimates obtained from different algorithms belonging to the same family of InSAR approaches, either Persistent Scatterer Interferometry (PSI) or Small BAseline Subset (SBAS); and accuracy assessments are mainly focused on vertical displacements or based on few Global Navigation Satellite System (GNSS) or geodetic leveling points. To fill this demonstration gap, two years of Sentinel-1 SAR ascending and descending mode data are processed with both PSI and SBAS consolidated algorithms to extract vertical and horizontal displacement velocity datasets, whose accuracy is then assessed against a wealth of contextual geodetic data. These include permanent GNSS records, static GNSS benchmark repositioning, and geodetic leveling monitoring data that the National Institute of Statistics, Geography, and Informatics (INEGI) of Mexico collected in 2014−2016 in the Aguascalientes Valley, where structurally-controlled land subsidence exhibits fast vertical rates (up to −150 mm/year) and a non-negligible east-west component (up to ±30 mm/year). Despite the temporal constraint of the data selected, the PSI-SBAS inter-comparison reveals standard deviation of 6 mm/year and 4 mm/year for the vertical and east-west rate differences, respectively, thus reassuring about the similarity between the two types of InSAR outputs. Accuracy assessment shows that the standard deviations in vertical velocity differences are 9−10 mm/year against GNSS benchmarks, and 8 mm/year against leveling data. Relative errors are below 20\% for any locations subsiding faster than −15 mm/year. Differences in east-west velocity estimates against GNSS are on average −0.1 mm/year for PSI and +0.2 mm/year for SBAS, with standard deviations of 8 mm/year. When discrepancies are found between InSAR and geodetic data, these mostly occur at benchmarks located in proximity to the main normal faults, thus falling within the same SBAS ground pixel or closer to the same PSI target, regardless of whether they are in the footwall or hanging wall of the fault. Establishing new benchmarks at higher distances from the fault traces or exploiting higher resolution SAR scenes and/or InSAR datasets may improve the detection of the benchmarks and thus consolidate the statistics of the InSAR accuracy assessments.},
	language = {en},
	number = {23},
	urldate = {2024-03-07},
	journal = {Remote Sensing},
	author = {Cigna, Francesca and Esquivel Ramírez, Rubén and Tapete, Deodato},
	month = jan,
	year = {2021},
	note = {Number: 23
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {GNSS, InSAR, Mexico, PSI-SBAS inter-comparison, Sentinel-1, accuracy, ground deformation, land subsidence, leveling, validation},
	pages = {4800},
}

@article{duan_multi-temporal_2020,
	title = {Multi-{Temporal} {InSAR} {Parallel} {Processing} for {Sentinel}-1 {Large}-{Scale} {Surface} {Deformation} {Mapping}},
	volume = {12},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/12/22/3749},
	doi = {10.3390/rs12223749},
	abstract = {Interferometric synthetic aperture radar (InSAR) has achieved great success in various geodetic applications, and its potential for ground deformation measurements on the large scale has attracted increasingly more attention in recent years. The increasing number of synthetic aperture radar (SAR) satellite systems have steadily provided a large amount of SAR data. Among these systems, the Sentinel-1 mission can be considered a milestone in the development of InSAR techniques, offering new opportunities to monitor global surface deformation with high precision, due to its wide coverage, short revisit time, and free access. However, conventional InSAR techniques have encountered great challenges in large-scale InSAR processing over wide areas because of the large computational burden and complexity. In this work, we present a novel parallel computing-based coherent scatterer InSAR (P-CSInSAR) method for automatic and efficient generation of deformation results from Sentinel-1 raw data. To achieve high parallelization performance for the overall InSAR processing chain, parallelization strategies at different levels have been adopted in the P-CSInSAR method, which has been fully addressed in this work. To evaluate the efficiency and accuracy of the proposed method, P-CSInSAR has been tested on the North China Plain regions with three adjacent frames of Sentinel-1 images, and the deformation results have been validated by GPS measurements. The experimental results confirm the effectiveness of the proposed parallel computing-based P-CSInSAR method. The proposed method can also play an important role in exploiting Sentinel-1 InSAR big data for disaster prevention and reduction.},
	language = {en},
	number = {22},
	urldate = {2024-03-07},
	journal = {Remote Sensing},
	author = {Duan, Wei and Zhang, Hong and Wang, Chao and Tang, Yixian},
	month = jan,
	year = {2020},
	note = {Number: 22
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {InSAR, Sentinel-1, big data, deformation mapping, parallel computing},
	pages = {3749},
}

@article{manunta_parallel_2019,
	title = {The {Parallel} {SBAS} {Approach} for {Sentinel}-1 {Interferometric} {Wide} {Swath} {Deformation} {Time}-{Series} {Generation}: {Algorithm} {Description} and {Products} {Quality} {Assessment}},
	volume = {57},
	issn = {1558-0644},
	shorttitle = {The {Parallel} {SBAS} {Approach} for {Sentinel}-1 {Interferometric} {Wide} {Swath} {Deformation} {Time}-{Series} {Generation}},
	url = {https://ieeexplore.ieee.org/document/8721519},
	doi = {10.1109/TGRS.2019.2904912},
	abstract = {We present an advanced differential synthetic aperture radar (SAR) interferometry (DInSAR) processing chain, based on the Parallel Small BAseline Subset (P-SBAS) technique, for the efficient generation of deformation time series from Sentinel-1 (S-1) interferometric wide (IW) swath SAR data sets. We first discuss an effective solution for the generation of high-quality interferograms, which properly accounts for the peculiarities of the terrain observation with progressive scans (TOPS) acquisition mode used to collect S-1 IW SAR data. These data characteristics are also properly accounted within the developed processing chain, taking full advantage from the burst partitioning. Indeed, such data structure represents a key element in the proposed P-SBAS implementation of the S-1 IW processing chain, whose migration into a cloud computing (CC) environment is also envisaged. An extensive experimental analysis, which allows us to assess the quality of the obtained interferometric products, is presented. To do this, we apply the developed S-1 IW P-SBAS processing chain to the overall archive acquired from descending orbits during the March 2015-April 2017 time span over the whole Italian territory, consisting in 2740 S-1 slices. In particular, the quality of the final results is assessed through a large-scale comparison with the GPS measurements relevant to nearly 500 stations. The mean standard deviation value of the differences between the DInSAR and the GPS time series (projected in the radar line of sight) is less than 0.5 cm, thus confirming the effectiveness of the implemented solution. Finally, a discussion about the performance achieved by migrating the developed processing chain within the Amazon Web Services CC environment is addressed, highlighting that a two-year data set relevant to a standard S-1 IW slice can be reliably processed in about 30 h.The presented results demonstrate the capability of the implemented P-SBAS approach to efficiently and effectively process large S-1 IW data sets relevant to extended portions of the earth surface, paving the way to the systematic generation of advanced DInSAR products to monitor ground displacements at a very wide spatial scale.},
	number = {9},
	urldate = {2024-03-07},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Manunta, Michele and De Luca, Claudio and Zinno, Ivana and Casu, Francesco and Manzo, Mariarosaria and Bonano, Manuela and Fusco, Adele and Pepe, Antonio and Onorato, Giovanni and Berardino, Paolo and De Martino, Prospero and Lanari, Riccardo},
	month = sep,
	year = {2019},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	keywords = {Cloud computing (CC), Distributed databases, GPS, Interferometry, Orbits, Parallel Small BAseline Subset (P-SBAS), Sentinel-1, Strain, Synthetic aperture radar, Time series analysis, deformation time series, differential synthetic aperture radar interferometry (DInSAR)},
	pages = {6259--6281},
}

@article{cigna_relationship_2017,
	series = {Big {Remotely} {Sensed} {Data}: tools, applications and experiences},
	title = {The relationship between intermittent coherence and precision of {ISBAS} {InSAR} ground motion velocities: {ERS}-1/2 case studies in the {UK}},
	volume = {202},
	issn = {0034-4257},
	shorttitle = {The relationship between intermittent coherence and precision of {ISBAS} {InSAR} ground motion velocities},
	url = {https://www.sciencedirect.com/science/article/pii/S0034425717302146},
	doi = {10.1016/j.rse.2017.05.016},
	abstract = {Retrieving ground motion information for non-urban and semi-vegetated areas using differential Interferometric Synthetic Aperture Radar (InSAR) and Small Baseline Subset (SBAS) approaches with C-band satellite radar imagery is challenging due to temporal decorrelation. By exploiting six stacks of medium resolution ERS-1/2 SAR images acquired between 1992 and 2000 over four regions of interest in the UK, this paper demonstrates the performance of the recently developed processing method Intermittent SBAS (ISBAS). This approach builds upon the conventional low-resolution SBAS method and, by relaxing the approach to selecting image pixels to process and accounting for the intermittent nature of non-urban targets, is capable to extend the coverage of motion results across the full range of land cover types, even those typically unfavourable for InSAR. On average, the new ISBAS implementation provides 4 to 26 times more coverage than SBAS for the processed regions, with the spatial coverage of ground motion solutions increasing from only 4–12\% land pixels with SBAS, to 39–99\% with ISBAS. Despite relying only on temporal subsets of the networks of small baseline interferograms, intermittently coherent pixels show velocity standard errors of 0.8–1.4mm/year on average, hence retain sub-millimetre to millimetre precision. The empirical relationships between intermittent coherence and standard errors in the estimated ground motion velocity are computed for each of the six datasets, and confirm that errors are controlled by the number of independent observations used for each image pixel to extract the ISBAS solution. In particular, velocity standard errors εvel for the intermittently coherent pixels are inversely proportional to the square root of the number of best coherence interferograms used, ni, and can be modelled as εvel=11/ni mm/year on average for the six datasets. The established empirical relationship also allows informed decisions on the ISBAS threshold for ni to be made. This is achieved by setting the maximum acceptable error in the velocity estimate εMAX, and then computing the corresponding minimum ni to accept an intermittently coherent pixel that will guarantee the desired precision. In the present era of ‘big SAR data’ and their derived ‘big InSAR data’, we discuss perspectives on the use of huge datasets of thousands or even millions of ground deformation time series – such as those produced using ISBAS, with a particular focus on the veracity of big data and the need for a quality assessment check-point in the ‘big InSAR data’ cycle.},
	urldate = {2024-03-07},
	journal = {Remote Sensing of Environment},
	author = {Cigna, Francesca and Sowter, Andrew},
	month = dec,
	year = {2017},
	keywords = {Big InSAR data, Ground motion, ISBAS, InSAR, Intermittent coherence, SBAS, Small baseline, Temporal decorrelation, Time series},
	pages = {177--198},
}

@article{raucoules_validation_2009,
	title = {Validation and intercomparison of {Persistent} {Scatterers} {Interferometry}: {PSIC4} project results},
	volume = {68},
	issn = {0926-9851},
	shorttitle = {Validation and intercomparison of {Persistent} {Scatterers} {Interferometry}},
	url = {https://www.sciencedirect.com/science/article/pii/S0926985109000226},
	doi = {10.1016/j.jappgeo.2009.02.003},
	abstract = {This article presents the main results of the Persistent Scatterer Interferometry Codes Cross Comparison and Certification for long term differential interferometry (PSIC4) project. The project was based on the validation of the PSI (Persistent Scatterer Interferometry) data with respect to levelling data on a subsiding mining area near Gardanne, in the South of France. Eight PSI participant teams processed the SAR data without any a priori information, as a blind test. Intercomparison of the different teams' results was then carried out in order to assess any similarities and discrepancies. The subsidence velocity intercomparison results obtained from the PSI data showed a standard deviation between 0.6 and 1.9 mm/year between the teams. The velocity validation against rates measured on the ground showed a standard deviation between 5 and 7 mm/year. A comparison of the PSI time series and levelling time series shows that if the displacement is larger than about 2 cm in between two consecutive SAR-images, PS-InSAR starts to seriously deviate from the levelling time series. Non-linear deformation rates up to several cm/year appear to be the main reason for these reduced performances, as no prior information was used to adjust the processing parameters. Under such testing conditions and without good ground-truth information, the phase-unwrapping errors for this type of work are a major issue. This point illustrates the importance of having ground truth information and a strong interaction with the end-user of the data, in order to properly understand the type and speed of the deformation that is to be measured, and thus determine the applicability of the technique.},
	number = {3},
	urldate = {2024-03-07},
	journal = {Journal of Applied Geophysics},
	author = {Raucoules, D. and Bourgine, B. and de Michele, M. and Le Cozannet, G. and Closset, L. and Bremmer, C. and Veldkamp, H. and Tragheim, D. and Bateson, L. and Crosetto, M. and Agudo, M. and Engdahl, M.},
	month = jul,
	year = {2009},
	keywords = {Mining subsidence, Persistent Scatterers Interferometry, Validation},
	pages = {335--347},
}

@article{strumbelj_past_2024,
	title = {Past, {Present} and {Future} of {Software} for {Bayesian} {Inference}},
	volume = {39},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/journals/statistical-science/volume-39/issue-1/Past-Present-and-Future-of-Software-for-Bayesian-Inference/10.1214/23-STS907.full},
	doi = {10.1214/23-STS907},
	abstract = {Software tools for Bayesian inference have undergone rapid evolution in the past three decades, following popularisation of the first generation MCMC-sampler implementations. More recently, exponential growth in the number of users has been stimulated both by the active development of new packages by the machine learning community and popularity of specialist software for particular applications. This review aims to summarize the most popular software and provide a useful map for a reader to navigate the world of Bayesian computation. We anticipate a vigorous continued development of algorithms and corresponding software in multiple research fields, such as probabilistic programming, likelihood-free inference and Bayesian neural networks, which will further broaden the possibilities for employing the Bayesian paradigm in exciting applications.},
	number = {1},
	urldate = {2024-02-23},
	journal = {Statistical Science},
	author = {Štrumbelj, Erik and Bouchard-Côté, Alexandre and Corander, Jukka and Gelman, Andrew and Rue, Håvard and Murray, Lawrence and Pesonen, Henri and Plummer, Martyn and Vehtari, Aki},
	month = feb,
	year = {2024},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {MCMC, computation, data analysis, probabilistic programming, statistics},
	pages = {46--61},
}

@misc{gibbs_conformal_2023,
	title = {Conformal {Prediction} {With} {Conditional} {Guarantees}},
	url = {http://arxiv.org/abs/2305.12616},
	doi = {10.48550/arXiv.2305.12616},
	abstract = {We consider the problem of constructing distribution-free prediction sets with finite-sample conditional guarantees. Prior work has shown that it is impossible to provide exact conditional coverage universally in finite samples. Thus, most popular methods only provide marginal coverage over the covariates. This paper bridges this gap by defining a spectrum of problems that interpolate between marginal and conditional validity. We motivate these problems by reformulating conditional coverage as coverage over a class of covariate shifts. When the target class of shifts is finite dimensional, we show how to simultaneously obtain exact finite sample coverage over all possible shifts. For example, given a collection of protected subgroups, our algorithm outputs intervals with exact coverage over each group. For more flexible, infinite dimensional classes where exact coverage is impossible, we provide a simple procedure for quantifying the gap between the coverage of our algorithm and the target level. Moreover, by tuning a single hyperparameter, we allow the practitioner to control the size of this gap across shifts of interest. Our methods can be easily incorporated into existing split conformal inference pipelines, and thus can be used to quantify the uncertainty of modern black-box algorithms without distributional assumptions.},
	urldate = {2024-02-22},
	publisher = {arXiv},
	author = {Gibbs, Isaac and Cherian, John J. and Candès, Emmanuel J.},
	month = dec,
	year = {2023},
	note = {arXiv:2305.12616 [stat]},
	keywords = {Statistics - Methodology},
}

@article{foygel_barber_limits_2021,
	title = {The limits of distribution-free conditional predictive inference},
	volume = {10},
	issn = {2049-8772},
	url = {https://doi.org/10.1093/imaiai/iaaa017},
	doi = {10.1093/imaiai/iaaa017},
	abstract = {We consider the problem of distribution-free predictive inference, with the goal of producing predictive coverage guarantees that hold conditionally rather than marginally. Existing methods such as conformal prediction offer marginal coverage guarantees, where predictive coverage holds on average over all possible test points, but this is not sufficient for many practical applications where we would like to know that our predictions are valid for a given individual, not merely on average over a population. On the other hand, exact conditional inference guarantees are known to be impossible without imposing assumptions on the underlying distribution. In this work, we aim to explore the space in between these two and examine what types of relaxations of the conditional coverage property would alleviate some of the practical concerns with marginal coverage guarantees while still being possible to achieve in a distribution-free setting.},
	number = {2},
	urldate = {2024-02-22},
	journal = {Information and Inference: A Journal of the IMA},
	author = {Foygel Barber, Rina and Candès, Emmanuel J and Ramdas, Aaditya and Tibshirani, Ryan J},
	month = jun,
	year = {2021},
	pages = {455--482},
}

@article{shen_prediction_2018,
	series = {Confidence distributions},
	title = {Prediction with confidence—{A} general framework for predictive inference},
	volume = {195},
	issn = {0378-3758},
	url = {https://www.sciencedirect.com/science/article/pii/S0378375817301696},
	doi = {10.1016/j.jspi.2017.09.012},
	abstract = {This paper proposes a general framework for prediction in which a prediction is presented in the form of a distribution function, called predictive distribution function. This predictive distribution function is well suited for the notion of confidence subscribed in the frequentist interpretation, and it can provide meaningful answers for questions related to prediction. A general approach under this framework is formulated and illustrated by using the so-called confidence distributions (CDs). This CD-based prediction approach inherits many desirable properties of CD, including its capacity for serving as a common platform for connecting and unifying the existing procedures of predictive inference in Bayesian, fiducial and frequentist paradigms. The theory underlying the CD-based predictive distribution is developed and some related efficiency and optimality issues are addressed. Moreover, a simple yet broadly applicable Monte Carlo algorithm is proposed for the implementation of the proposed approach. This concrete algorithm together with the proposed definition and associated theoretical development produce a comprehensive statistical inference framework for prediction. Finally, the approach is applied to simulation studies, and a real project on predicting the incoming volume of application submissions to a government agency. The latter shows the applicability of the proposed approach to dependence data settings.},
	urldate = {2024-02-22},
	journal = {Journal of Statistical Planning and Inference},
	author = {Shen, Jieli and Liu, Regina Y. and Xie, Min-ge},
	month = may,
	year = {2018},
	keywords = {Confidence distribution, Distributional inference, Frequentist coverage, Prediction, Predictive distribution},
	pages = {126--140},
}

@misc{noauthor_reviews_nodate,
	title = {Reviews: {Tree}-to-tree {Neural} {Networks} for {Program} {Translation}},
	url = {https://proceedings.neurips.cc/paper/2018/file/d759175de8ea5b1d9a2660e45554894f-Reviews.html},
	urldate = {2024-02-22},
}

@misc{cabezas_regression_2024,
	title = {Regression {Trees} for {Fast} and {Adaptive} {Prediction} {Intervals}},
	url = {http://arxiv.org/abs/2402.07357},
	doi = {10.48550/arXiv.2402.07357},
	abstract = {Predictive models make mistakes. Hence, there is a need to quantify the uncertainty associated with their predictions. Conformal inference has emerged as a powerful tool to create statistically valid prediction regions around point predictions, but its naive application to regression problems yields non-adaptive regions. New conformal scores, often relying upon quantile regressors or conditional density estimators, aim to address this limitation. Although they are useful for creating prediction bands, these scores are detached from the original goal of quantifying the uncertainty around an arbitrary predictive model. This paper presents a new, model-agnostic family of methods to calibrate prediction intervals for regression problems with local coverage guarantees. Our approach is based on pursuing the coarsest partition of the feature space that approximates conditional coverage. We create this partition by training regression trees and Random Forests on conformity scores. Our proposal is versatile, as it applies to various conformity scores and prediction settings and demonstrates superior scalability and performance compared to established baselines in simulated and real-world datasets. We provide a Python package clover that implements our methods using the standard scikit-learn interface.},
	urldate = {2024-02-20},
	publisher = {arXiv},
	author = {Cabezas, Luben M. C. and Otto, Mateus P. and Izbicki, Rafael and Stern, Rafael B.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.07357 [cs, stat]},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{vovk_computationally_2020,
	title = {Computationally efficient versions of conformal predictive distributions},
	volume = {397},
	issn = {0925-2312},
	url = {https://www.sciencedirect.com/science/article/pii/S0925231219316042},
	doi = {10.1016/j.neucom.2019.10.110},
	abstract = {Conformal predictive systems are a recent modification of conformal predictors that output, in regression problems, probability distributions for labels of test observations rather than set predictions. The extra information provided by conformal predictive systems may be useful, e.g., in decision making problems. Conformal predictive systems inherit the relative computational inefficiency of conformal predictors. In this paper we discuss two computationally efficient versions of conformal predictive systems, which we call split conformal predictive systems and cross-conformal predictive systems. The main advantage of split conformal predictive systems is their guaranteed validity, whereas for cross-conformal predictive systems validity only holds empirically and in the absence of excessive randomization. The main advantage of cross-conformal predictive systems is their greater predictive efficiency.},
	urldate = {2024-02-19},
	journal = {Neurocomputing},
	author = {Vovk, Vladimir and Petej, Ivan and Nouretdinov, Ilia and Manokhin, Valery and Gammerman, Alexander},
	month = jul,
	year = {2020},
	keywords = {Conformal prediction, Cross-conformal prediction, Inductive conformal prediction, Predictive distributions, Regression, Split conformal prediction},
	pages = {292--308},
}

@article{geyer_fuzzy_2005,
	title = {Fuzzy and {Randomized} {Confidence} {Intervals} and {P}-{Values}},
	volume = {20},
	issn = {0883-4237},
	url = {https://www.jstor.org/stable/20061193},
	abstract = {The optimal hypothesis tests for the binomial distribution and some other discrete distributions are uniformly most powerful (UMP) one-tailed and UMP unbiased (UMPU) two-tailed randomized tests. Conventional confidence intervals are not dual to randomized tests and perform badly on discrete data at small and moderate sample sizes. We introduce a new confidence interval notion, called fuzzy confidence intervals, that is dual to and inherits the exactness and optimality of UMP and UMPU tests. We also introduce a new P-value notion, called fuzzy P-values or abstract randomized P-values, that also inherits the same exactness and optimality.},
	number = {4},
	urldate = {2024-02-19},
	journal = {Statistical Science},
	author = {Geyer, Charles J. and Meeden, Glen D.},
	year = {2005},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {358--366},
}

@book{schweder_confidence_2016,
	address = {Cambridge},
	series = {Cambridge {Series} in {Statistical} and {Probabilistic} {Mathematics}},
	title = {Confidence, {Likelihood}, {Probability}: {Statistical} {Inference} with {Confidence} {Distributions}},
	isbn = {978-0-521-86160-1},
	shorttitle = {Confidence, {Likelihood}, {Probability}},
	url = {https://www.cambridge.org/core/books/confidence-likelihood-probability/143A34F11FB3D6F611F78E27C6D2CA5A},
	abstract = {This lively book lays out a methodology of confidence distributions and puts them through their paces. Among other merits, they lead to optimal combinations of confidence from different sources of information, and they can make complex models amenable to objective and indeed prior-free analysis for less subjectively inclined statisticians. The generous mixture of theory, illustrations, applications and exercises is suitable for statisticians at all levels of experience, as well as for data-oriented scientists. Some confidence distributions are less dispersed than their competitors. This concept leads to a theory of risk functions and comparisons for distributions of confidence. Neyman–Pearson type theorems leading to optimal confidence are developed and richly illustrated. Exact and optimal confidence distribution is the gold standard for inferred epistemic distributions. Confidence distributions and likelihood functions are intertwined, allowing prior distributions to be made part of the likelihood. Meta-analysis in likelihood terms is developed and taken beyond traditional methods, suiting it in particular to combining information across diverse data sources.},
	urldate = {2024-02-19},
	publisher = {Cambridge University Press},
	author = {Schweder, Tore and Hjort, Nils Lid},
	year = {2016},
	doi = {10.1017/CBO9781139046671},
}

@article{wolf_bootstrap_2015,
	title = {Bootstrap {Joint} {Prediction} {Regions}},
	volume = {36},
	issn = {1467-9892},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/jtsa.12099},
	doi = {10.1111/jtsa.12099},
	abstract = {Many statistical applications require the forecast of a random variable of interest over several periods into the future. The sequence of individual forecasts, one period at a time, is called a path forecast, where the term path refers to the sequence of individual future realizations of the random variable. The problem of constructing a corresponding joint prediction region has been rather neglected in the literature so far: such a region is supposed to contain the entire future path with a prespecified probability. We develop bootstrap methods to construct joint prediction regions. The resulting regions are proven to be asymptotically consistent under a mild high-level assumption. We compare the finite-sample performance of our joint prediction regions with some previous proposals via Monte Carlo simulations. An empirical application to a real data set is also provided.},
	language = {en},
	number = {3},
	urldate = {2024-02-19},
	journal = {Journal of Time Series Analysis},
	author = {Wolf, Michael and Wunderli, Dan},
	year = {2015},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/jtsa.12099},
	keywords = {Generalized error rates, path forecast, simultaneous prediction intervals},
	pages = {352--376},
}

@article{storlie_implementation_2009,
	title = {Implementation and evaluation of nonparametric regression procedures for sensitivity analysis of computationally demanding models},
	volume = {94},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832009001112},
	doi = {10.1016/j.ress.2009.05.007},
	abstract = {The analysis of many physical and engineering problems involves running complex computational models (simulation models, computer codes). With problems of this type, it is important to understand the relationships between the input variables (whose values are often imprecisely known) and the output. The goal of sensitivity analysis (SA) is to study this relationship and identify the most significant factors or variables affecting the results of the model. In this presentation, an improvement on existing methods for SA of complex computer models is described for use when the model is too computationally expensive for a standard Monte-Carlo analysis. In these situations, a meta-model or surrogate model can be used to estimate the necessary sensitivity index for each input. A sensitivity index is a measure of the variance in the response that is due to the uncertainty in an input. Most existing approaches to this problem either do not work well with a large number of input variables and/or they ignore the error involved in estimating a sensitivity index. Here, a new approach to sensitivity index estimation using meta-models and bootstrap confidence intervals is described that provides solutions to these drawbacks. Further, an efficient yet effective approach to incorporate this methodology into an actual SA is presented. Several simulated and real examples illustrate the utility of this approach. This framework can be extended to uncertainty analysis as well.},
	language = {en},
	number = {11},
	urldate = {2020-06-25},
	journal = {Reliability Engineering \& System Safety},
	author = {Storlie, Curtis B. and Swiler, Laura P. and Helton, Jon C. and Sallaberry, Cedric J.},
	month = nov,
	year = {2009},
	keywords = {Bootstrap, Confidence intervals, Meta-model, Nonparametric regression, Sensitivity analysis, Surrogate model, Uncertainty analysis, Variance decomposition},
	pages = {1735--1763},
}

@misc{del_barrio_nonparametric_2022,
	title = {Nonparametric {Multiple}-{Output} {Center}-{Outward} {Quantile} {Regression}},
	url = {http://arxiv.org/abs/2204.11756},
	doi = {10.48550/arXiv.2204.11756},
	abstract = {Based on the novel concept of multivariate center-outward quantiles introduced recently in Chernozhukov et al. (2017) and Hallin et al. (2021), we are considering the problem of nonparametric multiple-output quantile regression. Our approach defines nested conditional center-outward quantile regression contours and regions with given conditional probability content irrespective of the underlying distribution; their graphs constitute nested center-outward quantile regression tubes. Empirical counterparts of these concepts are constructed, yielding interpretable empirical regions and contours which are shown to consistently reconstruct their population versions in the Pompeiu-Hausdorff topology. Our method is entirely non-parametric and performs well in simulations including heteroskedasticity and nonlinear trends; its power as a data-analytic tool is illustrated on some real datasets.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {del Barrio, Eustasio and Sanz, Alberto Gonzalez and Hallin, Marc},
	month = apr,
	year = {2022},
	note = {arXiv:2204.11756 [math, stat]},
	keywords = {Center-outward quantiles, G.3, Mathematics - Statistics Theory, Multiple-output regression, Optimal transport, Statistics - Methodology},
}

@article{secchi_bagging_2013,
	series = {Spatial {Statistics} for {Mapping} the {Environment}},
	title = {Bagging {Voronoi} classifiers for clustering spatial functional data},
	volume = {22},
	issn = {0303-2434},
	url = {http://www.sciencedirect.com/science/article/pii/S0303243412000505},
	doi = {10.1016/j.jag.2012.03.006},
	abstract = {We propose a bagging strategy based on random Voronoi tessellations for the exploration of geo-referenced functional data, suitable for different purposes (e.g., classification, regression, dimensional reduction, …). Urged by an application to environmental data contained in the Surface Solar Energy database, we focus in particular on the problem of clustering functional data indexed by the sites of a spatial finite lattice. We thus illustrate our strategy by implementing a specific algorithm whose rationale is to (i) replace the original data set with a reduced one, composed by local representatives of neighborhoods covering the entire investigated area; (ii) analyze the local representatives; (iii) repeat the previous analysis many times for different reduced data sets associated to randomly generated different sets of neighborhoods, thus obtaining many different weak formulations of the analysis; (iv) finally, bag together the weak analyses to obtain a conclusive strong analysis. Through an extensive simulation study, we show that this new procedure – which does not require an explicit model for spatial dependence – is statistically and computationally efficient.},
	urldate = {2019-09-16},
	journal = {International Journal of Applied Earth Observation and Geoinformation},
	author = {Secchi, Piercesare and Vantini, Simone and Vitelli, Valeria},
	month = jun,
	year = {2013},
	keywords = {Bagging, Clustering, Functional data analysis, Irradiance data, Spatial statistics, Voronoi tessellation},
	pages = {53--64},
}

@article{kuelbs_convergence_2016,
	series = {In {Memoriam}: {Evarist} {Giné}},
	title = {Convergence of quantile and depth regions},
	volume = {126},
	issn = {0304-4149},
	url = {http://www.sciencedirect.com/science/article/pii/S0304414916300333},
	doi = {10.1016/j.spa.2016.04.011},
	abstract = {Since contours of multi-dimensional depth functions often characterize the distribution, it has become of interest to consider structural properties and limit theorems for the sample contours (see Zuo and Serfling (2000)). For finite dimensional data Massé and Theodorescu (1994) [14] and Kong and Mizera (2012) have made connections of directional quantile envelopes to level sets of half-space (Tukey) depth. In the recent paper (Kuelbs and Zinn, 2014) we showed that half-space depth regions determined by evaluation maps of a stochastic process are not only uniquely determined by related upper and lower quantile functions for the process, but limit theorems have also been obtained. In this paper we study the consequences of these results when applied to finite dimensional data in greater detail. The methods we employ here are based on Kuelbs and Zinn (2015) and Kuelbs and Zinn (2013).},
	number = {12},
	urldate = {2019-09-05},
	journal = {Stochastic Processes and their Applications},
	author = {Kuelbs, James and Zinn, Joel},
	month = dec,
	year = {2016},
	keywords = {Central limit theorems, Consistency, Convergence of empirical quantile and depth regions, Depth, Empirical processes, Tukey depth},
	pages = {3681--3700},
}

@article{secchi_analysis_2015,
	title = {Analysis of spatio-temporal mobile phone data: a case study in the metropolitan area of {Milan}},
	volume = {24},
	issn = {1613-981X},
	shorttitle = {Analysis of spatio-temporal mobile phone data},
	url = {https://doi.org/10.1007/s10260-014-0294-3},
	doi = {10.1007/s10260-014-0294-3},
	abstract = {We analyze geo-referenced high-dimensional data describing the use over time of the mobile-phone network in the urban area of Milan, Italy. Aim of the analysis is to identify subregions of the metropolitan area of Milan sharing a similar pattern along time, and possibly related to activities taking place in specific locations and/or times within the city. To tackle this problem, we develop a non-parametric method for the analysis of spatially dependent functional data, named Bagging Voronoi Treelet analysis. This novel approach integrates the treelet decomposition with a proper treatment of spatial dependence, obtained through a Bagging Voronoi strategy. The latter relies on the aggregation of different replicates of the analysis, each involving a set of functional local representatives associated to random Voronoi-based neighborhoods covering the investigated area. Results clearly point out some interesting temporal patterns interpretable in terms of population density mobility (e.g., daily work activities in the tertiary district, leisure activities in residential areas in the evenings and in the weekend, commuters movements along the highways during rush hours, and localized mob concentrations related to occasional events). Moreover we perform simulation studies, aimed at investigating the properties and performances of the method, and whose description is available online as Supplementary material.},
	language = {en},
	number = {2},
	urldate = {2019-09-16},
	journal = {Statistical Methods \& Applications},
	author = {Secchi, Piercesare and Vantini, Simone and Vitelli, Valeria},
	month = jul,
	year = {2015},
	keywords = {Bagging, Erlang data, Functional data analysis, Spatial statistics, Treelet analysis, Voronoi tessellation},
	pages = {279--300},
}

@article{lamboni_multivariate_2011,
	title = {Multivariate sensitivity analysis to measure global contribution of input factors in dynamic models},
	volume = {96},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832010002504},
	doi = {10.1016/j.ress.2010.12.002},
	abstract = {Many dynamic models are used for risk assessment and decision support in ecology and crop science. Such models generate time-dependent model predictions, with time either discretised or continuous. Their global sensitivity analysis is usually applied separately on each time output, but Campbell et al. (2006 [1]) advocated global sensitivity analyses on the expansion of the dynamics in a well-chosen functional basis. This paper focuses on the particular case when principal components analysis is combined with analysis of variance. In addition to the indices associated with the principal components, generalised sensitivity indices are proposed to synthesize the influence of each parameter on the whole time series output. Index definitions are given when the uncertainty on the input factors is either discrete or continuous and when the dynamic model is either discrete or functional. A general estimation algorithm is proposed, based on classical methods of global sensitivity analysis. The method is applied to a dynamic wheat crop model with 13 uncertain parameters. Three methods of global sensitivity analysis are compared: the Sobol'–Saltelli method, the extended FAST method, and the fractional factorial design of resolution 6.},
	language = {en},
	number = {4},
	urldate = {2020-04-15},
	journal = {Reliability Engineering \& System Safety},
	author = {Lamboni, Matieyendou and Monod, Hervé and Makowski, David},
	month = apr,
	year = {2011},
	keywords = {Dynamic model, Factorial design, Latin hypercube sampling, Principal components analysis, RKHS, Sensitivity analysis, Sobol' decomposition},
	pages = {450--459},
}

@article{cella_valid_2020,
	title = {Valid distribution-free inferential models for prediction},
	url = {http://arxiv.org/abs/2001.09225},
	abstract = {A fundamental problem in statistics and machine learning is that of using observed data to predict future observations. This is particularly challenging for model-based approaches because often the goal is to carry out this prediction with no or minimal model assumptions. For example, the inferential model (IM) approach is attractive because it has certain validity guarantees, but requires speciﬁcation of a parametric model. Here we show that a new perspective on a recently developed generalized IM approach can be applied to construct an IM for prediction that satisﬁes the desirable validity guarantees without speciﬁcation of a model. One important special case of this approach corresponds to the powerful conformal prediction framework and, consequently, the desirable properties of conformal prediction follow immediately from the general IM validity theory. Several numerical examples are presented to illustrate the theory and highlight the method’s performance and ﬂexibility.},
	language = {en},
	urldate = {2020-07-08},
	journal = {arXiv:2001.09225 [math, stat]},
	author = {Cella, Leonardo and Martin, Ryan},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.09225},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning},
}

@article{mao_valid_2020,
	title = {Valid model-free spatial prediction},
	url = {http://arxiv.org/abs/2006.15640},
	abstract = {Predicting the response at an unobserved location is a fundamental problem in spatial statistics. Given the difﬁculty in modeling spatial dependence, especially in non-stationary cases, model-based prediction intervals are at risk of misspeciﬁcation bias that can negatively affect their validity. Here we present a new approach for model-free spatial prediction based on the conformal prediction machinery. Our key observation is that spatial data can be treated as exactly or approximately exchangeable in a wide range of settings. For example, when the spatial locations are deterministic, we prove that the response values are, in a certain sense, locally approximately exchangeable for a broad class of spatial processes, and we develop a local spatial conformal prediction algorithm that yields valid prediction intervals without model assumptions. Numerical examples with both real and simulated data conﬁrm that the proposed conformal prediction intervals are valid and generally more efﬁcient than existing model-based procedures across a range of non-stationary and non-Gaussian settings.},
	language = {en},
	urldate = {2020-07-08},
	journal = {arXiv:2006.15640 [math, stat]},
	author = {Mao, Huiying and Martin, Ryan and Reich, Brian},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.15640},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
}

@article{barber_conformal_2023,
	title = {Conformal prediction beyond exchangeability},
	volume = {51},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-51/issue-2/Conformal-prediction-beyond-exchangeability/10.1214/23-AOS2276.full},
	doi = {10.1214/23-AOS2276},
	abstract = {Conformal prediction is a popular, modern technique for providing valid predictive inference for arbitrary machine learning models. Its validity relies on the assumptions of exchangeability of the data, and symmetry of the given model fitting algorithm as a function of the data. However, exchangeability is often violated when predictive models are deployed in practice. For example, if the data distribution drifts over time, then the data points are no longer exchangeable; moreover, in such settings, we might want to use a nonsymmetric algorithm that treats recent observations as more relevant. This paper generalizes conformal prediction to deal with both aspects: we employ weighted quantiles to introduce robustness against distribution drift, and design a new randomization technique to allow for algorithms that do not treat data points symmetrically. Our new methods are provably robust, with substantially less loss of coverage when exchangeability is violated due to distribution drift or other challenging features of real data, while also achieving the same coverage guarantees as existing conformal prediction methods if the data points are in fact exchangeable. We demonstrate the practical utility of these new tools with simulations and real-data experiments on electricity and election forecasting.},
	number = {2},
	urldate = {2024-02-17},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Candès, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	month = apr,
	year = {2023},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F40, 62G35, 62G86, conformal prediction, distribution-free inference, exchangeability, jackknife, robust statistics},
	pages = {816--845},
}

@article{gupta_nested_2020,
	title = {Nested conformal prediction and quantile out-of-bag ensemble methods},
	url = {http://arxiv.org/abs/1910.10562},
	abstract = {Conformal prediction is a popular tool for distribution-free uncertainty quantiﬁcation in statistical learning. While the traditional description of conformal prediction starts with a nonconformity score, we provide an alternate (but equivalent) view that starts with a sequence of nested sets and calibrates them to ﬁnd a valid prediction region. The nested framework easily subsumes all recent score functions including those based on quantile regression and density estimation. While these ideas were originally derived based on sample splitting, our framework seamlessly extends them to leaveone-out techniques like cross-conformal and the jackknife+. We use the framework to derive a new algorithm (QOOB, pronounced cube) that combines four ideas: quantile regression, cross-conformalization, ensemble methods and out-of-bag predictions. In a detailed numerical investigation, QOOB performs either the best or close to the best on all simulated and real datasets.},
	language = {en},
	urldate = {2020-10-13},
	journal = {arXiv:1910.10562 [math, stat]},
	author = {Gupta, Chirag and Kuchibhotla, Arun K. and Ramdas, Aaditya K.},
	month = may,
	year = {2020},
	note = {arXiv: 1910.10562},
	keywords = {Mathematics - Statistics Theory, Statistics - Machine Learning, Statistics - Methodology},
}

@article{paparoditis_bootstrap_2021,
	title = {Bootstrap {Prediction} {Bands} for {Functional} {Time} {Series}},
	url = {http://arxiv.org/abs/2004.03971},
	abstract = {A bootstrap procedure for constructing prediction bands for a stationary functional time series is proposed. The procedure exploits a general vector autoregressive representation of the time-reversed series of Fourier coefficients appearing in the Karhunen-Loeve representation of the functional process. It generates backward-in-time, functional replicates that adequately mimic the dependence structure of the underlying process in a model-free way and have the same conditionally fixed curves at the end of each functional pseudo-time series. The bootstrap prediction error distribution is then calculated as the difference between the model-free, bootstrap-generated future functional observations and the functional forecasts obtained from the model used for prediction. This allows the estimated prediction error distribution to account for the innovation and estimation errors associated with prediction and the possible errors due to model misspecification. We establish the asymptotic validity of the bootstrap procedure in estimating the conditional prediction error distribution of interest, and we also show that the procedure enables the construction of prediction bands that achieve (asymptotically) the desired coverage. Prediction bands based on a consistent estimation of the conditional distribution of the studentized prediction error process also are introduced. Such bands allow for taking more appropriately into account the local uncertainty of prediction. Through a simulation study and the analysis of two data sets, we demonstrate the capabilities and the good finite-sample performance of the proposed method.},
	urldate = {2021-06-26},
	journal = {arXiv:2004.03971 [math, stat]},
	author = {Paparoditis, Efstathios and Shang, Han Lin},
	month = may,
	year = {2021},
	note = {arXiv: 2004.03971},
	keywords = {62M15, Mathematics - Statistics Theory},
}

@article{shang_bootstrap_2018,
	title = {Bootstrap methods for stationary functional time series},
	volume = {28},
	issn = {1573-1375},
	url = {https://doi.org/10.1007/s11222-016-9712-8},
	doi = {10.1007/s11222-016-9712-8},
	abstract = {Bootstrap methods for estimating the long-run covariance of stationary functional time series are considered. We introduce a versatile bootstrap method that relies on functional principal component analysis, where principal component scores can be bootstrapped by maximum entropy. Two other bootstrap methods resample error functions, after the dependence structure being modeled linearly by a sieve method or nonlinearly by a functional kernel regression. Through a series of Monte-Carlo simulation, we evaluate and compare the finite-sample performances of these three bootstrap methods for estimating the long-run covariance in a functional time series. Using the intraday particulate matter (\$\${\textbackslash}hbox \{PM\}\_\{10\}\$\$) dataset in Graz, the proposed bootstrap methods provide a way of constructing the distribution of estimated long-run covariance for functional time series.},
	language = {en},
	number = {1},
	urldate = {2021-06-21},
	journal = {Statistics and Computing},
	author = {Shang, Han Lin},
	month = jan,
	year = {2018},
	pages = {1--10},
}

@article{diquigiovanni_conformal_2021,
	title = {Conformal {Prediction} bands for multivariate functional data},
	copyright = {All rights reserved},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X21001573},
	doi = {https://doi.org/10.1016/j.jmva.2021.104879},
	abstract = {Motivated by the pressing request of methods able to create prediction sets in a general regression framework for a multivariate functional response, we propose a set of conformal predictors that produce finite-sample either valid or exact multivariate simultaneous prediction bands under the mild assumption of exchangeable regression pairs. The fact that the prediction bands can be built around any regression estimator and that can be easily found in closed form yields a very widely usable method, which is fairly straightforward to implement. In addition, we first introduce and then describe a specific conformal predictor that guarantees an asymptotic result in terms of efficiency and inducing prediction bands able to modulate their width based on the local behavior and magnitude of the functional data. The method is investigated and analyzed through a simulation study and a real-world application in the field of urban mobility.},
	journal = {Journal of Multivariate Analysis},
	author = {Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	year = {2021},
	keywords = {Conformal Prediction, Distribution-free prediction set, Exact prediction set, Finite-sample prediction set, Functional data, Prediction band, Statistics - Methodology},
	pages = {104879},
}

@incollection{horvath_functional_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Functional time series},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_16},
	abstract = {Functional data often arise from measurements obtained by separating an almost continuous time record into natural consecutive intervals, for example days. The functions thus obtained form a functional time series, and the central issue in the analysis of such data is to take into account the temporal dependence of these functional observations. In the previous chapters we have seen many examples, which include daily curves of financial transaction data and daily patterns of geophysical and environmental data.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_16},
	keywords = {Adjusted Power, Brownian Bridge, Change Point, Change Point Detection, Kernel Estimator, autoregressive process, functional data, prediction, principal components, time series},
	pages = {289--341},
}

@misc{noauthor_conference_nodate,
	title = {Conference {Management} {Toolkit} - {View} reviews},
	url = {https://cmt3.research.microsoft.com/AISTATS2024/Submission/ReviewsSnapshot/1256},
	urldate = {2023-11-28},
}

@inproceedings{tapete_testing_2013,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Testing {Computational} {Methods} to {Identify} {Deformation} {Trends} in {RADARSAT} {Persistent} {Scatterers} {Time} {Series} for {Structural} {Assessment} of {Archaeological} {Heritage}},
	isbn = {978-3-642-39643-4},
	doi = {10.1007/978-3-642-39643-4_50},
	abstract = {RADARSAT-1 data stacks processed by means of Persistent Scatterer Interferometry (PSI) over the central archaeological area of Rome, Italy, and radar-interpreted according to the procedure of radar mapping by Tapete and Cigna (2012), were re-analyzed by applying the Deviation Index DI1 defined by Cigna et al. (2012). Our tests aimed to assess how an early computational identification of deformation trends within the displacement time series can support strategies of preventive conservation. The suitability of such semi-automated method for trend recognition is discussed with regard to a traditional approach of manual check of PSI time series, at the scale of single measurement point. Results from the case studies of Palatine and Oppian Hills are presented in this paper, examining both advantages and drawbacks offered by the implementation of such type of computational approach.},
	language = {en},
	booktitle = {Computational {Science} and {Its} {Applications} – {ICCSA} 2013},
	publisher = {Springer},
	author = {Tapete, Deodato and Casagli, Nicola},
	editor = {Murgante, Beniamino and Misra, Sanjay and Carlini, Maurizio and Torre, Carmelo M. and Nguyen, Hong-Quang and Taniar, David and Apduhan, Bernady O. and Gervasi, Osvaldo},
	year = {2013},
	keywords = {Archaeological Remote Sensing, Conservation, Deformation Analysis, Deformation Trend, Persistent Scatterer Interferometry, Synthetic Aperture Radar},
	pages = {693--707},
}

@misc{ulmer_non-exchangeable_2024,
	title = {Non-{Exchangeable} {Conformal} {Language} {Generation} with {Nearest} {Neighbors}},
	url = {http://arxiv.org/abs/2402.00707},
	doi = {10.48550/arXiv.2402.00707},
	abstract = {Quantifying uncertainty in automatically generated text is important for letting humans check potential hallucinations and making systems more reliable. Conformal prediction is an attractive framework to provide predictions imbued with statistical guarantees, however, its application to text generation is challenging since any i.i.d. assumptions are not realistic. In this paper, we bridge this gap by leveraging recent results on non-exchangeable conformal prediction, which still ensures bounds on coverage. The result, non-exchangeable conformal nucleus sampling, is a novel extension of the conformal prediction framework to generation based on nearest neighbors. Our method can be used post-hoc for an arbitrary model without extra training and supplies token-level, calibrated prediction sets equipped with statistical guarantees. Experiments in machine translation and language modeling show encouraging results in generation quality. By also producing tighter prediction sets with good coverage, we thus give a more theoretically principled way to perform sampling with conformal guarantees.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Ulmer, Dennis and Zerva, Chrysoula and Martins, André F. T.},
	month = feb,
	year = {2024},
	note = {arXiv:2402.00707 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{barber_predictive_2021,
	title = {Predictive inference with the jackknife+},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-1/Predictive-inference-with-the-jackknife/10.1214/20-AOS1965.full},
	doi = {10.1214/20-AOS1965},
	abstract = {This paper introduces the jackknife+, which is a novel method for constructing predictive confidence intervals. Whereas the jackknife outputs an interval centered at the predicted response of a test point, with the width of the interval determined by the quantiles of leave-one-out residuals, the jackknife+ also uses the leave-one-out predictions at the test point to account for the variability in the fitted regression function. Assuming exchangeable training samples, we prove that this crucial modification permits rigorous coverage guarantees regardless of the distribution of the data points, for any algorithm that treats the training points symmetrically. Such guarantees are not possible for the original jackknife and we demonstrate examples where the coverage rate may actually vanish. Our theoretical and empirical analysis reveals that the jackknife and the jackknife+ intervals achieve nearly exact coverage and have similar lengths whenever the fitting algorithm obeys some form of stability. Further, we extend the jackknife+ to \$K\$-fold cross validation and similarly establish rigorous coverage properties. Our methods are related to cross-conformal prediction proposed by Vovk (Ann. Math. Artif. Intell. 74 (2015) 9–28) and we discuss connections.},
	number = {1},
	urldate = {2023-11-24},
	journal = {The Annals of Statistics},
	author = {Barber, Rina Foygel and Candès, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	month = feb,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62F40, 62G08, 62G09, Statistics - Methodology, conformal inference, cross-validation, distribution-free, jackknife, leave-one-out, stability},
	pages = {486--507},
}

@misc{ghosal_multivariate_2023,
	title = {Multivariate {Scalar} on {Multidimensional} {Distribution} {Regression}},
	url = {http://arxiv.org/abs/2310.10494},
	abstract = {We develop a new method for multivariate scalar on multidimensional distribution regression. Traditional approaches typically analyze isolated univariate scalar outcomes or consider unidimensional distributional representations as predictors. However, these approaches are sub-optimal because: i) they fail to utilize the dependence between the distributional predictors: ii) neglect the correlation structure of the response. To overcome these limitations, we propose a multivariate distributional analysis framework that harnesses the power of multivariate density functions and multitask learning. We develop a computationally efficient semiparametric estimation method for modelling the effect of the latent joint density on multivariate response of interest. Additionally, we introduce a new conformal algorithm for quantifying the uncertainty of regression models with multivariate responses and distributional predictors, providing valuable insights into the conditional distribution of the response. We have validated the effectiveness of our proposed method through comprehensive numerical simulations, clearly demonstrating its superior performance compared to traditional methods. The application of the proposed method is demonstrated on tri-axial accelerometer data from the National Health and Nutrition Examination Survey (NHANES) 2011-2014 for modelling the association between cognitive scores across various domains and distributional representation of physical activity among older adult population. Our results highlight the advantages of the proposed approach, emphasizing the significance of incorporating complete spatial information derived from the accelerometer device.},
	urldate = {2023-10-17},
	publisher = {arXiv},
	author = {Ghosal, Rahul and Matabuena, Marcos},
	month = oct,
	year = {2023},
	note = {arXiv:2310.10494 [stat]},
	keywords = {Statistics - Applications, Statistics - Methodology},
}

@book{kokoszka_introduction_2021,
	address = {Boca Raton London New York},
	edition = {First issued in paperback},
	series = {Texts in statistical science series},
	title = {Introduction to functional data analysis},
	isbn = {978-1-03-209659-9 978-1-4987-4634-2},
	language = {eng},
	publisher = {CRC Press},
	author = {Kokoszka, Piotr and Reimherr, Matthew},
	year = {2021},
}

@article{cigna_semi-automated_2012,
	title = {Semi-automated extraction of {Deviation} {Indexes} ({DI}) from satellite {Persistent} {Scatterers} time series: tests on sedimentary volcanism and tectonically-induced motions},
	volume = {19},
	issn = {1023-5809},
	shorttitle = {Semi-automated extraction of {Deviation} {Indexes} ({DI}) from satellite {Persistent} {Scatterers} time series},
	url = {https://npg.copernicus.org/articles/19/643/2012/},
	doi = {10.5194/npg-19-643-2012},
	abstract = {We develop a methodology based on satellite Persistent Scatterers (PS) time series and aimed to calculate two indexes which are capable to depict the deviation from a deformation model defined a priori. Through a simple mathematical approach, these indexes reproduce the visual process of identification of trend deviations that is usually performed manually by the radar-interpreter, and guide the prioritization of further interpretation for those areas recording significant variations within their motion history. First tests on semi-automated extraction of the Deviation Indexes (DI) from RADARSAT-1 PS data available over Southern Italy allowed the quantification of tectonically-induced land motions which occurred in February 2005 within the town of Naro, and also the clear recognition of the precursors to mud volcano eruptions which occurred in August 2008 in the village of St. Barbara. For these areas, the information level brought by the DI increases and adds onto that of other PS parameters, such as yearly velocity, standard deviation and coherence. Factors exerting influence on the DI are critically tackled within the discussions, together with the analysis of the potentials of these indexes for monitoring and warning activities of geohazards.},
	language = {English},
	number = {6},
	urldate = {2024-01-11},
	journal = {Nonlinear Processes in Geophysics},
	author = {Cigna, F. and Tapete, D. and Casagli, N.},
	month = nov,
	year = {2012},
	note = {Publisher: Copernicus GmbH},
	pages = {643--655},
}

@misc{tsai_efficient_2024,
	title = {Efficient {Non}-{Parametric} {Uncertainty} {Quantification} for {Black}-{Box} {Large} {Language} {Models} and {Decision} {Planning}},
	url = {http://arxiv.org/abs/2402.00251},
	doi = {10.48550/arXiv.2402.00251},
	abstract = {Step-by-step decision planning with large language models (LLMs) is gaining attention in AI agent development. This paper focuses on decision planning with uncertainty estimation to address the hallucination problem in language models. Existing approaches are either white-box or computationally demanding, limiting use of black-box proprietary LLMs within budgets. The paper's first contribution is a non-parametric uncertainty quantification method for LLMs, efficiently estimating point-wise dependencies between input-decision on the fly with a single inference, without access to token logits. This estimator informs the statistical interpretation of decision trustworthiness. The second contribution outlines a systematic design for a decision-making agent, generating actions like ``turn on the bathroom light'' based on user prompts such as ``take a bath''. Users will be asked to provide preferences when more than one action has high estimated point-wise dependencies. In conclusion, our uncertainty estimation and decision-making agent design offer a cost-efficient approach for AI agent development.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Tsai, Yao-Hung Hubert and Talbott, Walter and Zhang, Jian},
	month = jan,
	year = {2024},
	note = {arXiv:2402.00251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{dutta_estimating_2023,
	title = {Estimating {Uncertainty} in {Multimodal} {Foundation} {Models} using {Public} {Internet} {Data}},
	url = {http://arxiv.org/abs/2310.09926},
	doi = {10.48550/arXiv.2310.09926},
	abstract = {Foundation models are trained on vast amounts of data at scale using self-supervised learning, enabling adaptation to a wide range of downstream tasks. At test time, these models exhibit zero-shot capabilities through which they can classify previously unseen (user-specified) categories. In this paper, we address the problem of quantifying uncertainty in these zero-shot predictions. We propose a heuristic approach for uncertainty estimation in zero-shot settings using conformal prediction with web data. Given a set of classes at test time, we conduct zero-shot classification with CLIP-style models using a prompt template, e.g., "an image of a {\textless}category{\textgreater}", and use the same template as a search query to source calibration data from the open web. Given a web-based calibration set, we apply conformal prediction with a novel conformity score that accounts for potential errors in retrieved web data. We evaluate the utility of our proposed method in Biomedical foundation models; our preliminary results show that web-based conformal prediction sets achieve the target coverage with satisfactory efficiency on a variety of biomedical datasets.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Dutta, Shiladitya and Wei, Hongbo and van der Laan, Lars and Alaa, Ahmed M.},
	month = nov,
	year = {2023},
	note = {arXiv:2310.09926 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
}

@misc{ren_robots_2023,
	title = {Robots {That} {Ask} {For} {Help}: {Uncertainty} {Alignment} for {Large} {Language} {Model} {Planners}},
	shorttitle = {Robots {That} {Ask} {For} {Help}},
	url = {http://arxiv.org/abs/2307.01928},
	doi = {10.48550/arXiv.2307.01928},
	abstract = {Large language models (LLMs) exhibit a wide range of promising capabilities -- from step-by-step planning to commonsense reasoning -- that may provide utility for robots, but remain prone to confidently hallucinated predictions. In this work, we present KnowNo, which is a framework for measuring and aligning the uncertainty of LLM-based planners such that they know when they don't know and ask for help when needed. KnowNo builds on the theory of conformal prediction to provide statistical guarantees on task completion while minimizing human help in complex multi-step planning settings. Experiments across a variety of simulated and real robot setups that involve tasks with different modes of ambiguity (e.g., from spatial to numeric uncertainties, from human preferences to Winograd schemas) show that KnowNo performs favorably over modern baselines (which may involve ensembles or extensive prompt tuning) in terms of improving efficiency and autonomy, while providing formal assurances. KnowNo can be used with LLMs out of the box without model-finetuning, and suggests a promising lightweight approach to modeling uncertainty that can complement and scale with the growing capabilities of foundation models. Website: https://robot-help.github.io},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Ren, Allen Z. and Dixit, Anushri and Bodrova, Alexandra and Singh, Sumeet and Tu, Stephen and Brown, Noah and Xu, Peng and Takayama, Leila and Xia, Fei and Varley, Jake and Xu, Zhenjia and Sadigh, Dorsa and Zeng, Andy and Majumdar, Anirudha},
	month = sep,
	year = {2023},
	note = {arXiv:2307.01928 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics, Statistics - Applications},
}

@misc{liu_trustworthy_2023,
	title = {Trustworthy {LLMs}: a {Survey} and {Guideline} for {Evaluating} {Large} {Language} {Models}' {Alignment}},
	shorttitle = {Trustworthy {LLMs}},
	url = {http://arxiv.org/abs/2308.05374},
	doi = {10.48550/arXiv.2308.05374},
	abstract = {Ensuring alignment, which refers to making models behave in accordance with human intentions [1,2], has become a critical task before deploying large language models (LLMs) in real-world applications. For instance, OpenAI devoted six months to iteratively aligning GPT-4 before its release [3]. However, a major challenge faced by practitioners is the lack of clear guidance on evaluating whether LLM outputs align with social norms, values, and regulations. This obstacle hinders systematic iteration and deployment of LLMs. To address this issue, this paper presents a comprehensive survey of key dimensions that are crucial to consider when assessing LLM trustworthiness. The survey covers seven major categories of LLM trustworthiness: reliability, safety, fairness, resistance to misuse, explainability and reasoning, adherence to social norms, and robustness. Each major category is further divided into several sub-categories, resulting in a total of 29 sub-categories. Additionally, a subset of 8 sub-categories is selected for further investigation, where corresponding measurement studies are designed and conducted on several widely-used LLMs. The measurement results indicate that, in general, more aligned models tend to perform better in terms of overall trustworthiness. However, the effectiveness of alignment varies across the different trustworthiness categories considered. This highlights the importance of conducting more fine-grained analyses, testing, and making continuous improvements on LLM alignment. By shedding light on these key dimensions of LLM trustworthiness, this paper aims to provide valuable insights and guidance to practitioners in the field. Understanding and addressing these concerns will be crucial in achieving reliable and ethically sound deployment of LLMs in various applications.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Liu, Yang and Yao, Yuanshun and Ton, Jean-Francois and Zhang, Xiaoying and Guo, Ruocheng and Cheng, Hao and Klochkov, Yegor and Taufiq, Muhammad Faaiz and Li, Hang},
	month = aug,
	year = {2023},
	note = {arXiv:2308.05374 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
}

@misc{huang_conformal_2023,
	title = {Conformal {Prediction} for {Deep} {Classifier} via {Label} {Ranking}},
	url = {http://arxiv.org/abs/2310.06430},
	doi = {10.48550/arXiv.2310.06430},
	abstract = {Conformal prediction is a statistical framework that generates prediction sets containing ground-truth labels with a desired coverage guarantee. The predicted probabilities produced by machine learning models are generally miscalibrated, leading to large prediction sets in conformal prediction. In this paper, we empirically and theoretically show that disregarding the probabilities' value will mitigate the undesirable effect of miscalibrated probability values. Then, we propose a novel algorithm named \${\textbackslash}textit\{Sorted Adaptive prediction sets\}\$ (SAPS), which discards all the probability values except for the maximum softmax probability. The key idea behind SAPS is to minimize the dependence of the non-conformity score on the probability values while retaining the uncertainty information. In this manner, SAPS can produce sets of small size and communicate instance-wise uncertainty. Theoretically, we provide a finite-sample coverage guarantee of SAPS and show that the expected value of set size from SAPS is always smaller than APS. Extensive experiments validate that SAPS not only lessens the prediction sets but also broadly enhances the conditional coverage rate and adaptation of prediction sets.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Huang, Jianguo and Xi, Huajun and Zhang, Linjun and Yao, Huaxiu and Qiu, Yue and Wei, Hongxin},
	month = oct,
	year = {2023},
	note = {arXiv:2310.06430 [cs, math, stat]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Mathematics - Statistics Theory},
}

@misc{wang_conformal_2023,
	title = {Conformal {Temporal} {Logic} {Planning} using {Large} {Language} {Models}: {Knowing} {When} to {Do} {What} and {When} to {Ask} for {Help}},
	shorttitle = {Conformal {Temporal} {Logic} {Planning} using {Large} {Language} {Models}},
	url = {http://arxiv.org/abs/2309.10092},
	doi = {10.48550/arXiv.2309.10092},
	abstract = {This paper addresses a new motion planning problem for mobile robots tasked with accomplishing multiple high-level sub-tasks, expressed using natural language (NL). These sub-tasks should be accomplished in a temporal and logical order. To formally define the overarching mission, we leverage Linear Temporal Logic (LTL) defined over atomic predicates modeling these NL-based sub-tasks. This is in contrast to related planning approaches that define LTL tasks over atomic predicates capturing desired low-level system configurations. Our goal is to design robot plans that satisfy LTL tasks defined over NL-based atomic propositions. A novel technical challenge arising in this setup lies in reasoning about correctness of a robot plan with respect to such LTL-encoded tasks. To address this problem, we propose HERACLEs, a hierarchical conformal natural language planner, that relies on (i) automata theory to determine what NL-specified sub-tasks should be accomplished next to make mission progress; (ii) Large Language Models to design robot plans satisfying these sub-tasks; and (iii) conformal prediction to reason probabilistically about correctness of the designed plans and to determine if external assistance is required. We provide theoretical probabilistic mission satisfaction guarantees as well as extensive comparative experiments on mobile manipulation tasks.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Wang, Jun and Tong, Jiaming and Tan, Kaiyuan and Vorobeychik, Yevgeniy and Kantaros, Yiannis},
	month = dec,
	year = {2023},
	note = {arXiv:2309.10092 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Robotics},
}

@inproceedings{zelenka_simple_2023,
	title = {A {Simple} and {Explainable} {Method} for {Uncertainty} {Estimation} {Using} {Attribute} {Prototype} {Networks}},
	url = {https://openaccess.thecvf.com/content/ICCV2023W/UnCV/html/Zelenka_A_Simple_and_Explainable_Method_for_Uncertainty_Estimation_Using_Attribute_ICCVW_2023_paper.html},
	language = {en},
	urldate = {2024-02-17},
	author = {Zelenka, Claudius and Göhring, Andrea and Kazempour, Daniyal and Hünemörder, Maximilian and Schmarje, Lars and Kröger, Peer},
	year = {2023},
	pages = {4570--4579},
}

@misc{zollo_prompt_2023,
	title = {Prompt {Risk} {Control}: {A} {Rigorous} {Framework} for {Responsible} {Deployment} of {Large} {Language} {Models}},
	shorttitle = {Prompt {Risk} {Control}},
	url = {http://arxiv.org/abs/2311.13628},
	doi = {10.48550/arXiv.2311.13628},
	abstract = {The recent explosion in the capabilities of large language models has led to a wave of interest in how best to prompt a model to perform a given task. While it may be tempting to simply choose a prompt based on average performance on a validation set, this can lead to a deployment where unexpectedly poor responses are generated, especially for the worst-off users. To mitigate this prospect, we propose Prompt Risk Control, a lightweight framework for selecting a prompt based on rigorous upper bounds on families of informative risk measures. We offer methods for producing bounds on a diverse set of metrics, including quantities that measure worst-case responses and disparities in generation quality across the population of users. In addition, we extend the underlying statistical bounding techniques to accommodate the possibility of distribution shifts in deployment. Experiments on applications such as open-ended chat, medical question summarization, and code generation highlight how such a framework can foster responsible deployment by reducing the risk of the worst outcomes.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Zollo, Thomas P. and Morrill, Todd and Deng, Zhun and Snell, Jake C. and Pitassi, Toniann and Zemel, Richard},
	month = nov,
	year = {2023},
	note = {arXiv:2311.13628 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{zerva_conformalizing_2023,
	title = {Conformalizing {Machine} {Translation} {Evaluation}},
	url = {http://arxiv.org/abs/2306.06221},
	doi = {10.48550/arXiv.2306.06221},
	abstract = {Several uncertainty estimation methods have been recently proposed for machine translation evaluation. While these methods can provide a useful indication of when not to trust model predictions, we show in this paper that the majority of them tend to underestimate model uncertainty, and as a result they often produce misleading confidence intervals that do not cover the ground truth. We propose as an alternative the use of conformal prediction, a distribution-free method to obtain confidence intervals with a theoretically established guarantee on coverage. First, we demonstrate that split conformal prediction can ``correct'' the confidence intervals of previous methods to yield a desired coverage level. Then, we highlight biases in estimated confidence intervals, both in terms of the translation language pairs and the quality of translations. We apply conditional conformal prediction techniques to obtain calibration subsets for each data subgroup, leading to equalized coverage.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Zerva, Chrysoula and Martins, André F. T.},
	month = jun,
	year = {2023},
	note = {arXiv:2306.06221 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{israelsen_llms_2023,
	title = {{LLMs} for {Multi}-{Modal} {Knowledge} {Extraction} and {Analysis} in {Intelligence}/{Safety}-{Critical} {Applications}},
	url = {http://arxiv.org/abs/2312.03088},
	doi = {10.48550/arXiv.2312.03088},
	abstract = {Large Language Models have seen rapid progress in capability in recent years; this progress has been accelerating and their capabilities, measured by various benchmarks, are beginning to approach those of humans. There is a strong demand to use such models in a wide variety of applications but, due to unresolved vulnerabilities and limitations, great care needs to be used before applying them to intelligence and safety-critical applications. This paper reviews recent literature related to LLM assessment and vulnerabilities to synthesize the current research landscape and to help understand what advances are most critical to enable use of of these technologies in intelligence and safety-critical applications. The vulnerabilities are broken down into ten high-level categories and overlaid onto a high-level life cycle of an LLM. Some general categories of mitigations are reviewed.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Israelsen, Brett and Sarkar, Soumalya},
	month = dec,
	year = {2023},
	note = {arXiv:2312.03088 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@misc{pelucchi_chatgpt_2023,
	title = {{ChatGPT} {Prompting} {Cannot} {Estimate} {Predictive} {Uncertainty} in {High}-{Resource} {Languages}},
	url = {http://arxiv.org/abs/2311.06427},
	doi = {10.48550/arXiv.2311.06427},
	abstract = {ChatGPT took the world by storm for its impressive abilities. Due to its release without documentation, scientists immediately attempted to identify its limits, mainly through its performance in natural language processing (NLP) tasks. This paper aims to join the growing literature regarding ChatGPT's abilities by focusing on its performance in high-resource languages and on its capacity to predict its answers' accuracy by giving a confidence level. The analysis of high-resource languages is of interest as studies have shown that low-resource languages perform worse than English in NLP tasks, but no study so far has analysed whether high-resource languages perform as well as English. The analysis of ChatGPT's confidence calibration has not been carried out before either and is critical to learn about ChatGPT's trustworthiness. In order to study these two aspects, five high-resource languages and two NLP tasks were chosen. ChatGPT was asked to perform both tasks in the five languages and to give a numerical confidence value for each answer. The results show that all the selected high-resource languages perform similarly and that ChatGPT does not have a good confidence calibration, often being overconfident and never giving low confidence values.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Pelucchi, Martino and Valdenegro-Toro, Matias},
	month = nov,
	year = {2023},
	note = {arXiv:2311.06427 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{tuwani_safe_2023,
	title = {Safe and reliable transport of prediction models to new healthcare settings without the need to collect new labeled data},
	copyright = {© 2023, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NoDerivs 4.0 International), CC BY-ND 4.0, as described at http://creativecommons.org/licenses/by-nd/4.0/},
	url = {https://www.medrxiv.org/content/10.1101/2023.12.13.23299899v2},
	doi = {10.1101/2023.12.13.23299899},
	abstract = {How can practitioners and clinicians know if a prediction model trained at a different institution can be safely used on their patient population? There is a large body of evidence showing that small changes in the distribution of the covariates used by prediction models may cause them to fail when deployed to new settings. This specific kind of dataset shift, known as covariate shift, is a central challenge to implementing existing prediction models in new healthcare environments. One solution is to collect additional labels in the target population and then fine tune the prediction model to adapt it to the characteristics of the new healthcare setting, which is often referred to as localization. However, collecting new labels can be expensive and time-consuming. To address these issues, we recast the core problem of model transportation in terms of uncertainty quantification, which allows one to know when a model trained in one setting may be safely used in a new healthcare environment of interest. Using methods from conformal prediction, we show how to transport models safely between different settings in the presence of covariate shift, even when all one has access to are covariates from the new setting of interest (e.g. no new labels). Using this approach, the model returns a prediction set that quantifies its uncertainty and is guaranteed to contain the correct label with a user-specified probability (e.g. 90\%), a property that is also known as coverage. We show that a weighted conformal inference procedure based on density ratio estimation between the source and target populations can produce prediction sets with the correct level of coverage on real-world data. This allows users to know if a model’s predictions can be trusted on their population without the need to collect new labeled data.},
	language = {en},
	urldate = {2024-02-17},
	publisher = {medRxiv},
	author = {Tuwani, Rudraksh and Beam, Andrew},
	month = dec,
	year = {2023},
	note = {Pages: 2023.12.13.23299899},
}

@misc{kumar_conformal_2023,
	title = {Conformal {Prediction} with {Large} {Language} {Models} for {Multi}-{Choice} {Question} {Answering}},
	url = {http://arxiv.org/abs/2305.18404},
	doi = {10.48550/arXiv.2305.18404},
	abstract = {As large language models continue to be widely developed, robust uncertainty quantification techniques will become crucial for their safe deployment in high-stakes scenarios. In this work, we explore how conformal prediction can be used to provide uncertainty quantification in language models for the specific task of multiple-choice question-answering. We find that the uncertainty estimates from conformal prediction are tightly correlated with prediction accuracy. This observation can be useful for downstream applications such as selective classification and filtering out low-quality predictions. We also investigate the exchangeability assumption required by conformal prediction to out-of-subject questions, which may be a more realistic scenario for many practical applications. Our work contributes towards more trustworthy and reliable usage of large language models in safety-critical situations, where robust guarantees of error rate are required.},
	urldate = {2024-02-17},
	publisher = {arXiv},
	author = {Kumar, Bhawesh and Lu, Charlie and Gupta, Gauri and Palepu, Anil and Bellamy, David and Raskar, Ramesh and Beam, Andrew},
	month = jul,
	year = {2023},
	note = {arXiv:2305.18404 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{carlier_vector_2016,
	title = {Vector quantile regression: {An} optimal transport approach},
	volume = {44},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Vector quantile regression},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-44/issue-3/Vector-quantile-regression-An-optimal-transport-approach/10.1214/15-AOS1401.full},
	doi = {10.1214/15-AOS1401},
	abstract = {We propose a notion of conditional vector quantile function and a vector quantile regression. A conditional vector quantile function (CVQF) of a random vector \$Y\$, taking values in \${\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$ given covariates \$Z=z\$, taking values in \${\textbackslash}mathbb\{R\}{\textasciicircum}\{k\}\$, is a map \$u{\textbackslash}longmapsto Q\_\{Y{\textbar}Z\}(u,z)\$, which is monotone, in the sense of being a gradient of a convex function, and such that given that vector \$U\$ follows a reference non-atomic distribution \$F\_\{U\}\$, for instance uniform distribution on a unit cube in \${\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$, the random vector \$Q\_\{Y{\textbar}Z\}(U,z)\$ has the distribution of \$Y\$ conditional on \$Z=z\$. Moreover, we have a strong representation, \$Y=Q\_\{Y{\textbar}Z\}(U,Z)\$ almost surely, for some version of \$U\$. The vector quantile regression (VQR) is a linear model for CVQF of \$Y\$ given \$Z\$. Under correct specification, the notion produces strong representation, \$Y={\textbackslash}beta (U){\textasciicircum}\{{\textbackslash}top\}f(Z)\$, for \$f(Z)\$ denoting a known set of transformations of \$Z\$, where \$u{\textbackslash}longmapsto{\textbackslash}beta(u){\textasciicircum}\{{\textbackslash}top\}f(Z)\$ is a monotone map, the gradient of a convex function and the quantile regression coefficients \$u{\textbackslash}longmapsto{\textbackslash}beta(u)\$ have the interpretations analogous to that of the standard scalar quantile regression. As \$f(Z)\$ becomes a richer class of transformations of \$Z\$, the model becomes nonparametric, as in series modelling. A key property of VQR is the embedding of the classical Monge–Kantorovich’s optimal transportation problem at its core as a special case. In the classical case, where \$Y\$ is scalar, VQR reduces to a version of the classical QR, and CVQF reduces to the scalar conditional quantile function. An application to multiple Engel curve estimation is considered.},
	number = {3},
	urldate = {2024-02-15},
	journal = {The Annals of Statistics},
	author = {Carlier, Guillaume and Chernozhukov, Victor and Galichon, Alfred},
	month = jun,
	year = {2016},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G05, 62H05, 62J99, Monge–Kantorovich–Brenier, Vector quantile regression, vector conditional quantile function},
	pages = {1165--1192},
}

@inproceedings{huang_convex_2020,
	title = {Convex {Potential} {Flows}: {Universal} {Probability} {Distributions} with {Optimal} {Transport} and {Convex} {Optimization}},
	shorttitle = {Convex {Potential} {Flows}},
	url = {https://openreview.net/forum?id=te7PVH1sPxJ},
	abstract = {Flow-based models are powerful tools for designing probabilistic models with tractable density. This paper introduces Convex Potential Flows (CP-Flow), a natural and efficient parameterization of invertible models inspired by the optimal transport (OT) theory. CP-Flows are the gradient map of a strongly convex neural potential function. The convexity implies invertibility and allows us to resort to convex optimization to solve the convex conjugate for efficient inversion. To enable maximum likelihood training, we derive a new gradient estimator of the log-determinant of the Jacobian, which involves solving an inverse-Hessian vector product using the conjugate gradient method. The gradient estimator has constant-memory cost, and can be made effectively unbiased by reducing the error tolerance level of the convex optimization routine. Theoretically, we prove that CP-Flows are universal density approximators and are optimal in the OT sense. Our empirical results show that CP-Flow performs competitively on standard benchmarks of density estimation and variational inference.},
	language = {en},
	urldate = {2024-02-15},
	author = {Huang, Chin-Wei and Chen, Ricky T. Q. and Tsirigotis, Christos and Courville, Aaron},
	month = oct,
	year = {2020},
}

@article{guan_localized_2023,
	title = {Localized conformal prediction: a generalized inference framework for conformal prediction},
	volume = {110},
	issn = {1464-3510},
	shorttitle = {Localized conformal prediction},
	url = {https://doi.org/10.1093/biomet/asac040},
	doi = {10.1093/biomet/asac040},
	abstract = {We propose a new inference framework called localized conformal prediction. It generalizes the framework of conformal prediction by offering a single-test-sample adaptive construction that emphasizes a local region around this test sample, and can be combined with different conformal scores. The proposed framework enjoys an assumption-free finite sample marginal coverage guarantee, and it also offers additional local coverage guarantees under suitable assumptions. We demonstrate how to change from conformal prediction to localized conformal prediction using several conformal scores, and we illustrate a potential gain via numerical examples.},
	number = {1},
	urldate = {2024-02-14},
	journal = {Biometrika},
	author = {Guan, Leying},
	month = mar,
	year = {2023},
	pages = {33--50},
}

@article{breiman_statistical_2001,
	title = {Statistical {Modeling}: {The} {Two} {Cultures}},
	volume = {16},
	issn = {0883-4237},
	shorttitle = {Statistical {Modeling}},
	url = {https://www.jstor.org/stable/2676681},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2024-02-14},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	pages = {199--215},
}

@article{zhu_autoregressive_2023,
	title = {Autoregressive optimal transport models},
	volume = {85},
	issn = {1369-7412},
	url = {https://doi.org/10.1093/jrsssb/qkad051},
	doi = {10.1093/jrsssb/qkad051},
	abstract = {Series of univariate distributions indexed by equally spaced time points are ubiquitous in applications and their analysis constitutes one of the challenges of the emerging field of distributional data analysis. To quantify such distributional time series, we propose a class of intrinsic autoregressive models that operate in the space of optimal transport maps. The autoregressive transport models that we introduce here are based on regressing optimal transport maps on each other, where predictors can be transport maps from an overall barycenter to a current distribution or transport maps between past consecutive distributions of the distributional time series. Autoregressive transport models and their associated distributional regression models specify the link between predictor and response transport maps by moving along geodesics in Wasserstein space. These models emerge as natural extensions of the classical autoregressive models in Euclidean space. Unique stationary solutions of autoregressive transport models are shown to exist under a geometric moment contraction condition of Wu \&amp; Shao [(2004) Limit theorems for iterated random functions. Journal of Applied Probability 41, 425–436)], using properties of iterated random functions. We also discuss an extension to a varying coefficient model for first-order autoregressive transport models. In addition to simulations, the proposed models are illustrated with distributional time series of house prices across U.S. counties and annual summer temperature distributions.},
	number = {3},
	urldate = {2024-02-08},
	journal = {Journal of the Royal Statistical Society Series B: Statistical Methodology},
	author = {Zhu, Changbo and Müller, Hans-Georg},
	month = jul,
	year = {2023},
	pages = {1012--1033},
}

@inproceedings{kan_multivariate_2022,
	title = {Multivariate {Quantile} {Function} {Forecaster}},
	url = {https://proceedings.mlr.press/v151/kan22a.html},
	abstract = {We propose Multivariate Quantile Function Forecaster (MQF2), a global probabilistic forecasting method constructed using a multivariate quantile function and investigate its application to multi-horizon forecasting. Prior approaches are either autoregressive, implicitly capturing the dependency structure across time but exhibiting error accumulation with increasing forecast horizons, or multi-horizon sequence-to-sequence models, which do not exhibit error accumulation, but also do typically not model the dependency structure across time steps. MQF2 combines the benefits of both approaches, by directly making predictions in the form of a multivariate quantile function, defined as the gradient of a convex function which we parametrize using input-convex neural networks. By design, the quantile function is monotone with respect to the input quantile levels and hence avoids quantile crossing. We provide two options to train MQF2: with energy score or with maximum likelihood. Experimental results on real-world and synthetic datasets show that our model has comparable performance with state-of-the-art methods in terms of single time step metrics while capturing the time dependency structure.},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {Proceedings of {The} 25th {International} {Conference} on {Artificial} {Intelligence} and {Statistics}},
	publisher = {PMLR},
	author = {Kan, Kelvin and Aubet, François-Xavier and Januschowski, Tim and Park, Youngsuk and Benidis, Konstantinos and Ruthotto, Lars and Gasthaus, Jan},
	month = may,
	year = {2022},
	note = {ISSN: 2640-3498},
	pages = {10603--10621},
}

@article{rodriguez_casal_set_2007,
	title = {Set estimation under convexity type assumptions},
	volume = {43},
	issn = {02460203},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0246020307000027},
	doi = {10.1016/j.anihpb.2006.11.001},
	abstract = {The problem of estimating a set S from a random sample of points taken within S is considered. It is assumed that S is r-convex, which means that a ball of radius r can go around from outside the set boundary. Under this assumption, the r-convex hull of the sample is a natural estimator of S. We obtain convergence rates for this estimator under both the distance in measure and the Hausdorff metric between sets. It is also proved that the boundary of the estimator consistently estimates the boundary of S, in Hausdorff’s sense.},
	language = {en},
	number = {6},
	urldate = {2024-02-01},
	journal = {Annales de l'Institut Henri Poincare (B) Probability and Statistics},
	author = {Rodriguez Casal, Alberto},
	month = nov,
	year = {2007},
	pages = {763--774},
}

@article{pateiro-lopez_generalizing_2010,
	title = {Generalizing the {Convex} {Hull} of a {Sample}: {The} {R} {Package} alphahull},
	volume = {34},
	copyright = {Copyright (c) 2009 Beatriz  Pateiro-López, Alberto Rodríguez-Casal},
	issn = {1548-7660},
	shorttitle = {Generalizing the {Convex} {Hull} of a {Sample}},
	url = {https://doi.org/10.18637/jss.v034.i05},
	doi = {10.18637/jss.v034.i05},
	abstract = {This paper presents the R package alphahull which implements the α-convex hull and the α-shape of a finite set of points in the plane. These geometric structures provide an informative overview of the shape and properties of the point set. Unlike the convex hull, the α-convex hull and the α-shape are able to reconstruct non-convex sets. This flexibility make them specially useful in set estimation. Since the implementation is based on the intimate relation of theses constructs with Delaunay triangulations, the R package alphahull also includes functions to compute Voronoi and Delaunay tesselations. The usefulness of the package is illustrated with two small simulation studies on boundary length estimation.},
	language = {en},
	urldate = {2024-02-01},
	journal = {Journal of Statistical Software},
	author = {Pateiro-López, Beatriz and Rodríguez-Casal, Alberto},
	month = apr,
	year = {2010},
	pages = {1--28},
}

@misc{figalli_mongeampere_2017,
	title = {The {Monge}–{Ampère} {Equation} and {Its} {Applications}},
	url = {https://ems.press/books/zlam/140},
	abstract = {The Monge–Ampère Equation and Its Applications, by Alessio Figalli. Published by EMS Press},
	language = {en},
	urldate = {2024-02-01},
	author = {Figalli, Alessio},
	month = jan,
	year = {2017},
	doi = {10.4171/170},
	note = {ISBN: 9783037191705 9783037196700},
}

@article{mccann_existence_1995,
	title = {Existence and uniqueness of monotone measure-preserving maps},
	volume = {80},
	issn = {0012-7094},
	url = {https://projecteuclid.org/journals/duke-mathematical-journal/volume-80/issue-2/Existence-and-uniqueness-of-monotone-measure-preserving-maps/10.1215/S0012-7094-95-08013-2.full},
	doi = {10.1215/S0012-7094-95-08013-2},
	abstract = {Introduction. Given a pair of Borel probability measures \# and v on Rd, it is natural to ask whether v can be obtained from \# by redistributing its mass in a canonical way. In the case of the line d 1 the answer is clear: as long as both measures are free from atoms--\#[\{x\}] v[\{x\}] 0mthere is a map y(x) of the line to itself for which Uniquely determined /-almost everywhere, this map may be taken to be nondecreasing by a suitable choice of y(x)e Rw \{\_\_\_o\} at the remaining points. Interpreting \# and v as the initial and final distribution of a one-dimensional fluid, the transformation y(x) gives a rearrangement of fluid particles yielding final state v from the initial state \#; this rearrangement is characterized by the fact that it preserves particle ordering, obviating any need for two particles to cross. Although the generalization of this construction to higher dimensions is the focus of this paper, the one-dimensional case will be pursued slightly further: when the measures are absolutely continuous with respect to Lebesgue--d\#(x)= f(x)dx and dr(y) g(y)dymthen, formally at least (neglecting regularity issues), the fundamental theorem of calculus yields},
	number = {2},
	urldate = {2024-01-31},
	journal = {Duke Mathematical Journal},
	author = {McCann, Robert J.},
	month = nov,
	year = {1995},
}

@article{hallin_distribution_2017,
	title = {On {Distribution} and {Quantile} {Functions}, {Ranks} and {Signs} in {R}\_d},
	url = {https://ideas.repec.org//p/eca/wpaper/2013-258262.html},
	abstract = {Unlike the real line, the d-dimensional space Rd, for d ≥ 2, is not canonically ordered. As a consequence, such fundamental and strongly order-related univariate concepts as quantile and distribution functions, and their empirical counterparts, involving ranks and signs, do not canonically extend to the multivariate context. Palliating that lack of a canonical ordering has remained an open problem for more than half a century, and has generated an abundant literature, motivating, among others, the development of statistical depth and copula-based methods. We show here that, unlike the many definitions that have been proposed in the literature, the measure transportation-based ones introduced in Chernozhukov et al. (2017) enjoy all the properties (distribution-freeness and preservation of semiparametric efficiency) that make univariate quantiles and ranks successful tools for semiparametric statistical inference. We therefore propose a new center-outward definition of multivariate distribution and quantile functions, along with their empirical counterparts, for which we establish a Glivenko-Cantelli result. Our approach, based on results by McCann (1995), is geometric rather than analytical and, contrary to the Monge-Kantorovich one in Chernozhukov et al. (2017) (which assumes compact supports or finite second-order moments), does not require any moment assumptions. The resulting ranks and signs are shown to be strictly distribution-free, and maximal invariant under the action of transformations (namely, the gradients of convex functions, which thus are playing the role of order-preserving transformations) generating the family of absolutely continuous distributions; this, in view of a general result by Hallin and Werker (2003), implies preservation of semiparametric efficiency. The resulting quantiles are equivariant under the same transformations, which confirms the order-preserving nature of gradients of convex function.},
	language = {en},
	urldate = {2024-01-31},
	journal = {Working Papers ECARES},
	author = {Hallin, Marc},
	month = sep,
	year = {2017},
	note = {Number: ECARES 2017-34
Publisher: ULB -- Universite Libre de Bruxelles},
	keywords = {glivenko-cantelli, gradient of convex function, invariance/equivariance, multivariate distribution function, multivariate order-preserving transformation, multivariate quantiles, multivariate ranks, multivariate signs},
}

@article{edelsbrunner_shape_1983,
	title = {On the shape of a set of points in the plane},
	volume = {29},
	issn = {0018-9448},
	url = {http://ieeexplore.ieee.org/document/1056714/},
	doi = {10.1109/TIT.1983.1056714},
	abstract = {A generalization of the convex hull of a finite set of points in the plane is introduced and analyzed. This generalization leads to a family of straight-line graphs, “o-shapes,” which seem to capture the intuitive notions of “fine shape” and “crude shape” of point sets. It is shown that a-shapes are subgraphs of the closest point or furthest point Delaunay triangulation. Relying on this result an optimal O( n log n) algorithm that constructs o-shapes is developed.},
	language = {en},
	number = {4},
	urldate = {2024-01-26},
	journal = {IEEE Transactions on Information Theory},
	author = {Edelsbrunner, H. and Kirkpatrick, D. and Seidel, R.},
	month = jul,
	year = {1983},
	pages = {551--559},
}

@inproceedings{sohn_learning_2015,
	title = {Learning {Structured} {Output} {Representation} using {Deep} {Conditional} {Generative} {Models}},
	volume = {28},
	url = {https://papers.nips.cc/paper_files/paper/2015/hash/8d55a249e6baa5c06772297520da2051-Abstract.html},
	abstract = {Supervised deep learning has been successfully applied for many recognition problems in machine learning and computer vision. Although it can approximate a complex many-to-one function very well when large number of training data is provided, the lack of probabilistic inference of the current supervised deep learning methods makes it difficult to model a complex structured output representations. In this work, we develop a scalable deep conditional generative model for structured output variables using Gaussian latent variables. The model is trained efficiently in the framework of stochastic gradient variational Bayes, and allows a fast prediction using stochastic feed-forward inference. In addition, we provide novel strategies to build a robust structured prediction algorithms, such as recurrent prediction network architecture, input noise-injection and multi-scale prediction training methods. In experiments, we demonstrate the effectiveness of our proposed algorithm in comparison to the deterministic deep neural network counterparts in generating diverse but realistic output representations using stochastic inference. Furthermore, the proposed schemes in training methods and architecture design were complimentary, which leads to achieve strong pixel-level object segmentation and semantic labeling performance on Caltech-UCSD Birds 200 and the subset of Labeled Faces in the Wild dataset.},
	urldate = {2024-01-26},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Sohn, Kihyuk and Lee, Honglak and Yan, Xinchen},
	year = {2015},
}

@article{hyndman_computing_1996,
	title = {Computing and {Graphing} {Highest} {Density} {Regions}},
	volume = {50},
	issn = {00031305},
	url = {https://www.jstor.org/stable/2684423?origin=crossref},
	doi = {10.2307/2684423},
	abstract = {Highest density regions.},
	language = {en},
	number = {2},
	urldate = {2024-01-26},
	journal = {The American Statistician},
	author = {Hyndman, Rob J.},
	month = may,
	year = {1996},
	pages = {120},
}

@article{chernozhukov_mongekantorovich_2017,
	title = {Monge–{Kantorovich} depth, quantiles, ranks and signs},
	volume = {45},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-45/issue-1/MongeKantorovich-depth-quantiles-ranks-and-signs/10.1214/16-AOS1450.full},
	doi = {10.1214/16-AOS1450},
	abstract = {We propose new concepts of statistical depth, multivariate quantiles, vector quantiles and ranks, ranks and signs, based on canonical transportation maps between a distribution of interest on \${\textbackslash}mathbb\{R\}{\textasciicircum}\{d\}\$ and a reference distribution on the \$d\$-dimensional unit ball. The new depth concept, called Monge–Kantorovich depth, specializes to halfspace depth for \$d=1\$ and in the case of spherical distributions, but for more general distributions, differs from the latter in the ability for its contours to account for non-convex features of the distribution of interest. We propose empirical counterparts to the population versions of those Monge–Kantorovich depth contours, quantiles, ranks, signs and vector quantiles and ranks, and show their consistency by establishing a uniform convergence property for empirical (forward and reverse) transport maps, which is the main theoretical result of this paper.},
	number = {1},
	urldate = {2024-01-25},
	journal = {The Annals of Statistics},
	author = {Chernozhukov, Victor and Galichon, Alfred and Hallin, Marc and Henry, Marc},
	month = feb,
	year = {2017},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62G35, 62M15, Statistical depth, empirical transport maps, multivariate signs, uniform convergence of empirical transport, vector quantiles, vector ranks},
	pages = {223--256},
}

@article{li_trend_2024,
	title = {Trend {Classification} of {InSAR} {Displacement} {Time} {Series} {Using} {SAE}–{CNN}},
	volume = {16},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2072-4292},
	url = {https://www.mdpi.com/2072-4292/16/1/54},
	doi = {10.3390/rs16010054},
	abstract = {Multi-temporal Interferometric Synthetic Aperture Radar technique (MTInSAR) has emerged as a valuable tool for measuring ground motion in a wide area. However, interpreting displacement time series and identifying dangerous signals from millions of InSAR coherent targets is challenging. In this study, we propose a method combining stacked autoencoder (SAE) and convolutional neural network (CNN) to classify InSAR time series and ease the interpretation of movements. The InSAR time series are classified into five categories, including stable, linear, accelerating, deceleration, and phase unwrapping error (PUE). The accuracy of labeled samples reaches 95.1\%, reflecting the performance of the proposed method. This method was applied to the InSAR results for Kunming extracted from 171 ascending Sentinel-1 images from January 2017 to September 2022. The classification map of the InSAR time series shows that stable coherent points dominate around 79.28\% of the area, with linear patterns at 10.70\%, decelerating at 5.30\%, accelerating at 4.72\%, and PUE patterns at 3.60\%. The results demonstrate that this method can distinguish different ground motion features and detect nonlinear deformation signals on a large scale without human intervention.},
	language = {en},
	number = {1},
	urldate = {2024-01-23},
	journal = {Remote Sensing},
	author = {Li, Menghua and Wu, Hanfei and Yang, Mengshi and Huang, Cheng and Tang, Bo-Hui},
	month = jan,
	year = {2024},
	note = {Number: 1
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {CNN, InSAR, displacement time-series classification, machine learning},
	pages = {54},
}

@article{fontana_conformal_2023,
	title = {Conformal prediction: {A} {Unified} {Review} of {Theory} and {New} {Challenges}},
	volume = {29},
	copyright = {All rights reserved},
	issn = {1350-7265},
	shorttitle = {Conformal prediction},
	url = {https://projecteuclid.org/journals/bernoulli/volume-29/issue-1/Conformal-prediction--A-unified-review-of-theory-and-new/10.3150/21-BEJ1447.full},
	doi = {10.3150/21-BEJ1447},
	number = {1},
	urldate = {2022-11-02},
	journal = {Bernoulli},
	author = {Fontana, Matteo and Zeni, Gianluca and Vantini, Simone},
	month = feb,
	year = {2023},
}

@article{vincenty_direct_1975,
	title = {Direct and {Inverse} {Solutions} of {Geodesics} on the {Ellipsoid} with {Application} of {Nested} {Equations}},
	volume = {23},
	issn = {0039-6265},
	url = {https://doi.org/10.1179/sre.1975.23.176.88},
	doi = {10.1179/sre.1975.23.176.88},
	abstract = {This paper gives compact formulae for the direct and inverse solutions of geodesics of any length. Existing formulae have been recast for efficient programming to conserve space and reduce execution time. The main feature of the new formulae is the use of nested equations for elliptic terms. Both solutions are iterative.},
	number = {176},
	urldate = {2024-01-23},
	journal = {Survey Review},
	author = {Vincenty, T.},
	month = apr,
	year = {1975},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1179/sre.1975.23.176.88},
	pages = {88--93},
}

@article{notti_user-oriented_2015,
	title = {A {User}-{Oriented} {Methodology} for {DInSAR} {Time} {Series} {Analysis} and {Interpretation}: {Landslides} and {Subsidence} {Case} {Studies}},
	volume = {172},
	issn = {1420-9136},
	shorttitle = {A {User}-{Oriented} {Methodology} for {DInSAR} {Time} {Series} {Analysis} and {Interpretation}},
	url = {https://doi.org/10.1007/s00024-015-1071-4},
	doi = {10.1007/s00024-015-1071-4},
	abstract = {Recent advances in multi-temporal Differential Synthetic Aperture Radar (SAR) Interferometry (DInSAR) have greatly improved our capability to monitor geological processes. Ground motion studies using DInSAR require both the availability of good quality input data and rigorous approaches to exploit the retrieved Time Series (TS) at their full potential. In this work we present a methodology for DInSAR TS analysis, with particular focus on landslides and subsidence phenomena. The proposed methodology consists of three main steps: (1) pre-processing, i.e., assessment of a SAR Dataset Quality Index (SDQI) (2) post-processing, i.e., application of empirical/stochastic methods to improve the TS quality, and (3) trend analysis, i.e., comparative implementation of methodologies for automatic TS analysis. Tests were carried out on TS datasets retrieved from processing of SAR imagery acquired by different radar sensors (i.e., ERS-1/2 SAR, RADARSAT-1, ENVISAT ASAR, ALOS PALSAR, TerraSAR-X, COSMO-SkyMed) using advanced DInSAR techniques (i.e., SqueeSAR™, PSInSAR™, SPN and SBAS). The obtained values of SDQI are discussed against the technical parameters of each data stack (e.g., radar band, number of SAR scenes, temporal coverage, revisiting time), the retrieved coverage of the DInSAR results, and the constraints related to the characterization of the investigated geological processes. Empirical and stochastic approaches were used to demonstrate how the quality of the TS can be improved after the SAR processing, and examples are discussed to mitigate phase unwrapping errors, and remove regional trends, noise and anomalies. Performance assessment of recently developed methods of trend analysis (i.e., PS-Time, Deviation Index and velocity TS) was conducted on two selected study areas in Northern Italy affected by land subsidence and landslides. Results show that the automatic detection of motion trends enhances the interpretation of DInSAR data, since it provides an objective picture of the deformation behaviour recorded through TS and therefore contributes to the understanding of the on-going geological processes.},
	language = {en},
	number = {11},
	urldate = {2024-01-22},
	journal = {Pure and Applied Geophysics},
	author = {Notti, Davide and Calò, Fabiana and Cigna, Francesca and Manunta, Michele and Herrera, Gerardo and Berti, Matteo and Meisina, Claudia and Tapete, Deodato and Zucca, Francesco},
	month = nov,
	year = {2015},
	keywords = {Persistent scatterers, SAR interferometry, landslides, quality assessment, small baseline subset, subsidence, time series analysis},
	pages = {3081--3105},
}

@article{chang_probabilistic_2016,
	title = {A {Probabilistic} {Approach} for {InSAR} {Time}-{Series} {Postprocessing}},
	volume = {54},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/abstract/document/7208887},
	doi = {10.1109/TGRS.2015.2459037},
	abstract = {Monitoring the kinematic behavior of enormous amounts of points and objects anywhere on Earth is now feasible on a weekly basis using radar interferometry from Earth-orbiting satellites. An increasing number of satellite missions are capable of delivering data that can be used to monitor geophysical processes, mining and construction activities, public infrastructure, or even individual buildings. The parameters estimated from these data are used to better understand various natural hazards, improve public safety, or enhance asset management activities. Yet, the mathematical estimation of kinematic parameters from interferometric data is an ill-posed problem as there is no unique solution, and small changes in the data may lead to significantly different parameter estimates. This problem results in multiple possible outcomes given the same data, hampering public acceptance, particularly in critical conditions. Here, we propose a method to address this problem in a probabilistic way, which is based on multiple hypotheses testing. We demonstrate that it is possible to systematically evaluate competing kinematic models in order to find an optimal model and to assign likelihoods to the results. Using the B-method of testing, a numerically efficient implementation is achieved, which is able to evaluate hundreds of competing models per point. Our approach will not solve the nonuniqueness problem of interferometric synthetic aperture radar (InSAR), but it will allow users to critically evaluate (conflicting) results, avoid overinterpretation, and thereby consolidate InSAR as a geodetic technique.},
	number = {1},
	urldate = {2024-01-22},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Chang, Ling and Hanssen, Ramon F.},
	month = jan,
	year = {2016},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	pages = {421--430},
}

@article{berti_automated_2013,
	title = {Automated classification of {Persistent} {Scatterers} {Interferometry} time series},
	volume = {13},
	issn = {1561-8633},
	url = {https://nhess.copernicus.org/articles/13/1945/2013/},
	doi = {10.5194/nhess-13-1945-2013},
	abstract = {We present a new method for the automatic classification of Persistent Scatters Interferometry (PSI) time series based on a conditional sequence of statistical tests. Time series are classified into distinctive predefined target trends, such as uncorrelated, linear, quadratic, bilinear and discontinuous, that describe different styles of ground deformation. Our automatic analysis overcomes limits related to the visual classification of PSI time series, which cannot be carried out systematically for large datasets. The method has been tested with reference to landslides using PSI datasets covering the northern Apennines of Italy. The clear distinction between the relative frequency of uncorrelated, linear and non-linear time series with respect to mean velocity distribution suggests that different target trends are related to different physical processes that are likely to control slope movements. The spatial distribution of classified time series is also consistent with respect the known distribution of flat areas, slopes and landslides in the tests area. Classified time series enhances the radar interpretation of slope movements at the site scale, pointing out significant advantages in comparison with the conventional analysis based solely on the mean velocity. The test application also warns against potentially misleading classification outputs in case of datasets affected by systematic errors. Although the method was developed and tested to investigate landslides, it should be also useful for the analysis of other ground deformation processes such as subsidence, swelling/shrinkage of soils, or uplifts due to deep injections in reservoirs.},
	language = {English},
	number = {8},
	urldate = {2024-01-22},
	journal = {Natural Hazards and Earth System Sciences},
	author = {Berti, M. and Corsini, A. and Franceschini, S. and Iannacone, J. P.},
	month = aug,
	year = {2013},
	note = {Publisher: Copernicus GmbH},
	pages = {1945--1958},
}

@book{noauthor_testing_2005,
	address = {New York, NY},
	series = {Springer {Texts} in {Statistics}},
	title = {Testing {Statistical} {Hypotheses}},
	isbn = {978-0-387-98864-1 978-0-387-27605-2},
	url = {http://link.springer.com/10.1007/0-387-27605-X},
	language = {en},
	urldate = {2024-01-19},
	publisher = {Springer},
	year = {2005},
	doi = {10.1007/0-387-27605-X},
	note = {ISSN: 1431-875X},
	keywords = {Excel, Resampling, Statistical Hypotheses, Statistical Theory, best fit},
}

@article{weyant_contributions_2017,
	title = {Some {Contributions} of {Integrated} {Assessment} {Models} of {Global}                         {Climate} {Change}},
	volume = {11},
	issn = {1750-6816},
	url = {https://www.journals.uchicago.edu/doi/full/10.1093/reep/rew018},
	doi = {10.1093/reep/rew018},
	abstract = {This article reviews the use of integrated assessment models (IAMs) in climate policy and research at the global scale. Two different types of IAMs are discussed. First, there are models that focus on climate change mitigation options and climate change impacts in some detail without necessarily valuing or aggregating all possible impacts into a single measure of projected climate damages. Here they are called detailed process (DP) IAMs. A second class of IAMs are much more highly aggregated and focus on calculating carbon emissions trajectories and carbon prices that maximize global welfare. Here these models are referred to as aggregate benefit–cost analysis (BCA) IAMs.

Early IAMs of both types were introduced about 30 years ago and by now have been applied to many important policy and research design issues. Continual advancements in physical and economic system understanding, modeling techniques, and computational power should continue to open up many additional opportunities for using these models to provide relevant information to decision makers. While the models can be improved in many areas, much of the uncertainty that exists reflects a lack of complete scientific understanding of the systems involved rather than limitations of one or another approach to model construction and use.},
	number = {1},
	urldate = {2024-01-15},
	journal = {Review of Environmental Economics and Policy},
	author = {Weyant, John},
	month = jan,
	year = {2017},
	note = {Publisher: The University of Chicago Press},
	pages = {115--137},
}

@article{vallone_ground_2008,
	title = {Ground motion phenomena in {Caltanissetta} ({Italy}) investigated by {InSAR} and geological data integration},
	volume = {98},
	issn = {0013-7952},
	url = {https://www.sciencedirect.com/science/article/pii/S0013795208000422},
	doi = {10.1016/j.enggeo.2008.02.004},
	abstract = {Urban areas are frequently affected by ground instabilities of various origins. The location of urban zones affected by ground instability phenomena is crucially important for hazard mitigation policies. Satellite-based Interferometric Synthetic Aperture Radar (InSAR) has demonstrated its remarkable capability to detect and quantify ground and building motion in urban areas, especially since the development of Advanced Differential Interferometric SAR techniques (A-DInSAR). In fact, the high density of reflectors like buildings and infrastructures in urban areas improves the quality of the InSAR signal, allowing sub-centimetric displacements to be reliably detected. The A-DInSAR techniques allow urban zones affected by ground deformation to be located and mapped, but clearly they are not able to point out the causes of the instability phenomena. These can only be highlighted by an integrated analysis of multidisciplinary data, like geological, geotechnical, SAR interferometric and historical data. The overlay of these data, which is possible within a Geographic Information System (GIS), is a useful tool to identify ground motion phenomena affecting urban zones. In this study we apply this kind of approach to Caltanissetta, a provincial capital in Sicily (Italy), where local damage has been detected. The reconstruction of the local near-surface geology shows the presence of zones affected by local natural hazard factors, essentially due to the local presence of soils with poor mechanical properties or swelling soils, high topographic gradients and steep slopes on loose soils. Processing 17 ASAR-ENVISAT SAR images covering the time interval October 2002–December 2005 by means of an A-DInSAR procedure, the Caltanissetta deformation map has been realized. It shows that most of the city is stable, with the exception of three zones, situated in the northwestern, northeastern and southern parts of the city, respectively. Two of them, characterized by high topographic gradients and steep slopes on sandy soils, are affected by subsidence ground motion. An uplift motion is recognized in the other zone, characterized by the local presence of expansible clays. Geotechnical swelling tests carried out on them have shown a swelling behavior. On site surveys have highlighted the presence of damage in the zones affected by ground motion.},
	number = {3},
	urldate = {2024-01-11},
	journal = {Engineering Geology},
	author = {Vallone, P. and Giammarinaro, M. S. and Crosetto, M. and Agudo, M. and Biescas, E.},
	month = may,
	year = {2008},
	keywords = {DInSAR, GIS, Ground instabilities, Urban geology},
	pages = {144--155},
}

@article{berardino_new_2002,
	title = {A new algorithm for surface deformation monitoring based on small baseline differential {SAR} interferograms},
	volume = {40},
	issn = {1558-0644},
	url = {https://ieeexplore.ieee.org/document/1166596},
	doi = {10.1109/TGRS.2002.803792},
	abstract = {We present a new differential synthetic aperture radar (SAR) interferometry algorithm for monitoring the temporal evolution of surface deformations. The presented technique is based on an appropriate combination of differential interferograms produced by data pairs characterized by a small orbital separation (baseline) in order to limit the spatial decorrelation phenomena. The application of the singular value decomposition method allows us to easily "link" independent SAR acquisition datasets, separated by large baselines, thus increasing the observation temporal sampling rate. The availability of both spatial and temporal information in the processed data is used to identify and filter out atmospheric phase artifacts. We present results obtained on the data acquired from 1992 to 2000 by the European Remote Sensing satellites and relative to the Campi Flegrei caldera and to the city of Naples, Italy, that demonstrate the capability of the proposed approach to follow the dynamics of the detected deformations.},
	number = {11},
	urldate = {2024-01-11},
	journal = {IEEE Transactions on Geoscience and Remote Sensing},
	author = {Berardino, P. and Fornaro, G. and Lanari, R. and Sansosti, E.},
	month = nov,
	year = {2002},
	note = {Conference Name: IEEE Transactions on Geoscience and Remote Sensing},
	pages = {2375--2383},
}

@article{casu_sbas-dinsar_2014,
	title = {{SBAS}-{DInSAR} {Parallel} {Processing} for {Deformation} {Time}-{Series} {Computation}},
	volume = {7},
	issn = {2151-1535},
	url = {https://ieeexplore.ieee.org/document/6823083},
	doi = {10.1109/JSTARS.2014.2322671},
	abstract = {The aim of this paper is to design a novel parallel computing solution for the processing chain implementing the Small BAseline Subset (SBAS) Differential SAR Interferometry (DInSAR) technique. The proposed parallel solution (P-SBAS) is based on a dual-level parallelization approach and encompasses combined parallelization strategies, which are fully discussed in this paper. Moreover, the main methodological aspects of the proposed approach and their implications are also addressed. Finally, an experimental analysis, aimed at quantitatively evaluating the computational efficiency of the implemented parallel prototype, with respect to appropriate metrics, has been carried out on real data; this analysis confirms the effectiveness of the proposed parallel computing solution. In the current scenario, characterized by huge SAR archives relevant to the present and future SAR missions, the P-SBAS processing chain can play a key role to effectively exploit these big data volumes for the comprehension of the surface deformation dynamics of large areas of Earth.},
	number = {8},
	urldate = {2024-01-11},
	journal = {IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	author = {Casu, Francesco and Elefante, Stefano and Imperatore, Pasquale and Zinno, Ivana and Manunta, Michele and De Luca, Claudio and Lanari, Riccardo},
	month = aug,
	year = {2014},
	note = {Conference Name: IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing},
	pages = {3285--3296},
}

@article{bernardi_use_2021,
	title = {On the {Use} of {Interferometric} {Synthetic} {Aperture} {Radar} {Data} for {Monitoring} and {Forecasting} {Natural} {Hazards}},
	volume = {53},
	issn = {1874-8953},
	url = {https://doi.org/10.1007/s11004-021-09948-8},
	doi = {10.1007/s11004-021-09948-8},
	abstract = {Recent advances in satellite technologies, statistical and mathematical models, and computational resources have paved the way for operational use of satellite data in monitoring and forecasting natural hazards. We present a review of the use of satellite data for Earth observation in the context of geohazards preventive monitoring and disaster evaluation and assessment. We describe the techniques exploited to extract ground displacement information from satellite radar sensor images and the applicability of such data to the study of natural hazards such as landslides, earthquakes, volcanic activity, and ground subsidence. In this context, statistical techniques, ranging from time series analysis to spatial statistics, as well as continuum or discrete physics-based models, adopting deterministic or stochastic approaches, are irreplaceable tools for modeling and simulating natural hazards scenarios from a mathematical perspective. In addition to this, the huge amount of data collected nowadays and the complexity of the models and methods needed for an effective analysis set new computational challenges. The synergy among statistical methods, mathematical models, and optimized software, enriched with the assimilation of satellite data, is essential for building predictive and timely monitoring models for risk analysis.},
	language = {en},
	number = {8},
	urldate = {2024-01-11},
	journal = {Mathematical Geosciences},
	author = {Bernardi, Mara S. and Africa, Pasquale C. and de Falco, Carlo and Formaggia, Luca and Menafoglio, Alessandra and Vantini, Simone},
	month = nov,
	year = {2021},
	keywords = {Earth observation, Ground displacement, InSAR data, Mathematical models, Natural hazards, Statistical models},
	pages = {1781--1812},
}

@article{rosen_synthetic_2000,
	title = {Synthetic aperture radar interferometry},
	volume = {88},
	issn = {0018-9219, 1558-2256},
	url = {http://ieeexplore.ieee.org/document/838084/},
	doi = {10.1109/5.838084},
	number = {3},
	urldate = {2024-01-11},
	journal = {Proceedings of the IEEE},
	author = {Rosen, P.A. and Hensley, S. and Joughin, I.R. and Li, F.K. and Madsen, S.N. and Rodriguez, E. and Goldstein, R.M.},
	month = mar,
	year = {2000},
	pages = {333--382},
}

@article{freedman_nonstochastic_1983,
	title = {A {Nonstochastic} {Interpretation} of {Reported} {Significance} {Levels}},
	volume = {1},
	issn = {0735-0015},
	url = {https://www.jstor.org/stable/1391660},
	doi = {10.2307/1391660},
	abstract = {Tests of significance are often made in situations where the standard assumptions underlying the probability calculations do not hold. As a result, the reported significance levels become difficult to interpret. This article sketches an alternative interpretation of a reported significance level, valid in considerable generality. This level locates the given data set within the spectrum of other data sets derived from the given one by an appropriate class of transformations. If the null hypothesis being tested holds, the derived data sets should be equivalent to the original one. Thus, a small reported significance level indicates an unusual data set. This development parallels that of randomization tests, but there is a crucial technical difference: our approach involves permuting observed residuals; the classical randomization approach involves permuting unobservable, or perhaps nonexistent, stochastic disturbance terms.},
	number = {4},
	urldate = {2024-01-04},
	journal = {Journal of Business \& Economic Statistics},
	author = {Freedman, David and Lane, David},
	year = {1983},
	note = {Publisher: [American Statistical Association, Taylor \& Francis, Ltd.]},
	pages = {292--298},
}

@article{gamboa_sensitivity_2014,
	title = {Sensitivity analysis for multidimensional and functional outputs},
	volume = {8},
	issn = {1935-7524, 1935-7524},
	url = {https://projecteuclid.org/journals/electronic-journal-of-statistics/volume-8/issue-1/Sensitivity-analysis-for-multidimensional-and-functional-outputs/10.1214/14-EJS895.full},
	doi = {10.1214/14-EJS895},
	abstract = {Let \$X:=(X\_\{1\},{\textbackslash}ldots,X\_\{p\})\$ be random objects (the inputs), defined on some probability space \$({\textbackslash}Omega,{\textbackslash}mathcal\{F\},{\textbackslash}mathbb\{P\})\$ and valued in some measurable space \$E=E\_\{1\}{\textbackslash}times{\textbackslash}ldots{\textbackslash}times E\_\{p\}\$. Further, let \$Y:=Y=f(X\_\{1\},{\textbackslash}ldots,X\_\{p\})\$ be the output. Here, \$f\$ is a measurable function from \$E\$ to some Hilbert space \${\textbackslash}mathbb\{H\}\$ (\${\textbackslash}mathbb\{H\}\$ could be either of finite or infinite dimension). In this work, we give a natural generalization of the Sobol indices (that are classically defined when \$Y{\textbackslash}in{\textbackslash}mathbb\{R\}\$), when the output belongs to \${\textbackslash}mathbb\{H\}\$. These indices have very nice properties. First, they are invariant under isometry and scaling. Further they can be, as in dimension \$1\$, easily estimated by using the so-called Pick and Freeze method. We investigate the asymptotic behaviour of such an estimation scheme.},
	number = {1},
	urldate = {2023-12-21},
	journal = {Electronic Journal of Statistics},
	author = {Gamboa, Fabrice and Janon, Alexandre and Klein, Thierry and Lagnoux, Agnès},
	month = jan,
	year = {2014},
	note = {Publisher: Institute of Mathematical Statistics and Bernoulli Society},
	keywords = {62G05, 62G20, Concentration inequalities, Semi-parametric efficient estimation, Sobol indices, quadratic functionals, sensitivity analysis, temporal output, vector output},
	pages = {575--603},
}

@article{francom_sensitivity_2017,
	title = {Sensitivity {Analysis} and {Emulation} for {Functional} {Data} using {Bayesian} {Adaptive} {Splines}},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J28N2/j28n211/j28n211.html},
	doi = {10.5705/ss.202016.0130},
	abstract = {When a computer code is used to simulate a complex system, one of the fundamental tasks is to assess the sensitivity of the simulator to the diﬀerent input parameters. In the case of computationally expensive simulators, this is often accomplished via a surrogate statistical model, a statistical output emulator. An eﬀective emulator is one that provides good approximations to the computer code output for wide ranges of input values. In addition, an emulator should be able to handle large dimensional simulation output for a relevant number of inputs; it should ﬂexibly capture heterogeneities in the variability of the response surface; it should be fast to evaluate for arbitrary combinations of input parameters, and it should provide an accurate quantiﬁcation of the emulation uncertainty. In this paper we discuss the Bayesian approach to multivariate adaptive regression splines (BMARS) as an emulator for a computer model that outputs curves. We introduce modiﬁcations to traditional BMARS approaches that allow for ﬁtting large amounts of data and allow for more eﬃcient MCMC sampling. We emphasize the ease with which sensitivity analysis can be performed in this situation. We present a sensitivity analysis of a computer model of the deformation of a protective plate used in pressure-driven experiments. Our example serves as an illustration of the ability of BMARS emulators to fulﬁll all the necessities of computability, ﬂexibility and reliable calculation on relevant measures of sensitivity.},
	language = {en},
	urldate = {2023-12-21},
	journal = {Statistica Sinica},
	author = {Francom,, Devin and Sanso, Bruno and Kupresanin, Ana and Johannesson, Gardar},
	year = {2017},
}

@book{hsing_theoretical_2015,
	address = {Chichester, West Sussex, UK},
	series = {Wiley series in probability and statistics},
	title = {Theoretical foundations of functional data analysis, with an introduction to linear operators},
	isbn = {978-0-470-01691-6},
	publisher = {John Wiley and Sons, Inc},
	author = {Hsing, Tailen and Eubank, Randall L.},
	year = {2015},
	keywords = {Linear operators, Multivariate analysis, Statistical functionals},
}

@article{eilers_splines_2010,
	title = {Splines, knots, and penalties},
	volume = {2},
	copyright = {Copyright © 2010 John Wiley \& Sons, Inc.},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.125},
	doi = {10.1002/wics.125},
	abstract = {Penalized splines have gained much popularity as a flexible tool for smoothing and semi-parametric models. Two approaches have been advocated: (1) use a B-spline basis, equally spaced knots, and difference penalties [Eilers PHC, Marx BD. Flexible smoothing using B-splines and penalized likelihood (with Comments and Rejoinder). Stat Sci 1996, 11:89–121.] and (2) use truncated power functions, knots based on quantiles of the independent variable and a ridge penalty [Ruppert D, Wand MP, Carroll RJ. Semiparametric Regression. New York: Cambridge University Press; 2003]. We compare the two approaches on many aspects: numerical stability, quality of the fit, interpolation/extrapolation, derivative estimation, visual presentation and extension to multidimensional smoothing. We discuss mixed model and Bayesian parallels to penalized regression. We conclude that B-splines with difference penalties are clearly to be preferred. WIREs Comp Stat 2010 2 637–653 DOI: 10.1002/wics.125 This article is categorized under: Statistical and Graphical Methods of Data Analysis {\textgreater} Density Estimation},
	language = {en},
	number = {6},
	urldate = {2023-12-19},
	journal = {WIREs Computational Statistics},
	author = {Eilers, Paul H. C. and Marx, Brian D.},
	year = {2010},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wics.125},
	keywords = {P-splines, difference penalty, interpolation, smoothing, truncated power functions},
	pages = {637--653},
}

@article{pinson_verification_2012,
	title = {Verification of the {ECMWF} ensemble forecasts of wind speed against analyses and observations},
	volume = {19},
	copyright = {Copyright © 2011 Royal Meteorological Society},
	issn = {1469-8080},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/met.283},
	doi = {10.1002/met.283},
	abstract = {A framework for the verification of ensemble forecasts of near-surface wind speed is described. It is based on existing scores and diagnostic tools, though considering observations from synoptic stations as reference instead of the analysis. This approach is motivated by the idea of having a user-oriented view of verification, for instance with the wind power applications in mind. The verification framework is specifically applied to the case of ECMWF ensemble forecasts and over Europe. Dynamic climatologies are derived at the various stations, serving as a benchmark. The impact of observational uncertainty on scores and diagnostic tools is also considered. The interest of this framework is demonstrated from its application to the routine evaluation of ensemble forecasts and to the assessment of the quality improvements brought in by the recent change in horizontal resolution of the ECMWF ensemble prediction system. The most important conclusions cover (1) the generally high skill of these ensemble forecasts of near-surface wind speed when evaluated at synoptic stations, (2) the noteworthy improvement of scores brought by the change of horizontal resolution, and, (3) the scope for further improvements of reliability and skill of wind speed ensemble forecasts by appropriate post-processing. Copyright © 2011 Royal Meteorological Society},
	language = {en},
	number = {4},
	urldate = {2023-12-15},
	journal = {Meteorological Applications},
	author = {Pinson, Pierre and Hagedorn, Renate},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/met.283},
	keywords = {benchmark, calibration, energy application, observational uncertainty, probabilistic forecasting, scores, skill},
	pages = {484--500},
}

@article{brunner_challenges_2021,
	title = {Challenges in modeling and predicting floods and droughts: {A} review},
	volume = {8},
	copyright = {© 2021 The Authors. WIREs Water published by Wiley Periodicals LLC.},
	issn = {2049-1948},
	shorttitle = {Challenges in modeling and predicting floods and droughts},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wat2.1520},
	doi = {10.1002/wat2.1520},
	abstract = {Predictions of floods, droughts, and fast drought-flood transitions are required at different time scales to develop management strategies targeted at minimizing negative societal and economic impacts. Forecasts at daily and seasonal scale are vital for early warning, estimation of event frequency for hydraulic design, and long-term projections for developing adaptation strategies to future conditions. All three types of predictions—forecasts, frequency estimates, and projections—typically treat droughts and floods independently, even though both types of extremes can be studied using related approaches and have similar challenges. In this review, we (a) identify challenges common to drought and flood prediction and their joint assessment and (b) discuss tractable approaches to tackle these challenges. We group challenges related to flood and drought prediction into four interrelated categories: data, process understanding, modeling and prediction, and human–water interactions. Data-related challenges include data availability and event definition. Process-related challenges include the multivariate and spatial characteristics of extremes, non-stationarities, and future changes in extremes. Modeling challenges arise in frequency analysis, stochastic, hydrological, earth system, and hydraulic modeling. Challenges with respect to human–water interactions lie in establishing links to impacts, representing human–water interactions, and science communication. We discuss potential ways of tackling these challenges including exploiting new data sources, studying droughts and floods in a joint framework, studying societal influences and compounding drivers, developing continuous stochastic models or non-stationary models, and obtaining stakeholder feedback. Tackling one or several of these challenges will improve flood and drought predictions and help to minimize the negative impacts of extreme events. This article is categorized under: Science of Water {\textgreater} Science of Water},
	language = {en},
	number = {3},
	urldate = {2023-12-15},
	journal = {WIREs Water},
	author = {Brunner, Manuela I. and Slater, Louise and Tallaksen, Lena M. and Clark, Martyn},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/wat2.1520},
	keywords = {droughts, floods, forecasting, hydrologic extremes, prediction},
	pages = {e1520},
}

@article{vitousek_model_2023,
	title = {A {Model} {Integrating} {Satellite}-{Derived} {Shoreline} {Observations} for {Predicting} {Fine}-{Scale} {Shoreline} {Response} to {Waves} and {Sea}-{Level} {Rise} {Across} {Large} {Coastal} {Regions}},
	volume = {128},
	copyright = {© 2023 The Authors. This article has been contributed to by U.S. Government employees and their work is in the public domain in the USA.},
	issn = {2169-9011},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1029/2022JF006936},
	doi = {10.1029/2022JF006936},
	abstract = {Satellite-derived shoreline observations combined with dynamic shoreline models enable fine-scale predictions of coastal change across large spatiotemporal scales. Here, we present a satellite-data-assimilated, “littoral-cell”-based, ensemble Kalman-filter shoreline model to predict coastal change and uncertainty due to waves, sea-level rise (SLR), and other natural and anthropogenic processes. We apply the developed ensemble model to the entire California coastline (approximately 1,760 km), much of which is sparsely monitored with traditional survey methods (e.g., Lidar/GPS). Water-level-corrected, satellite-derived shoreline observations (obtained from the CoastSat toolbox) offer a nearly unbiased representation of in situ surveyed shorelines (e.g., mean sea-level elevation contours) at Ocean Beach, San Francisco. We demonstrate that model calibration with satellite observations during a 20-year hindcast period (1995–2015) provides nearly equivalent model forecast accuracy during a validation period (2015–2020) compared to model calibration with monthly in situ observations at Ocean Beach. When comparing model-predicted shoreline positions to satellite-derived observations, the model achieves an accuracy of {\textless}10 m RMSE for nearly half of the entire California coastline for the validation period. The calibrated/validated model is then applied for multi-decadal simulations of shoreline change due to projected wave and sea-level conditions, while holding the model parameters fixed. By 2100, the model estimates that 24\%–75\% of California's beaches may become completely eroded due to SLR scenarios of 1.0–3.0 m, respectively. The satellite-data-assimilated modeling system presented here is generally applicable to a variety of coastal settings around the world owing to the global coverage of satellite imagery.},
	language = {en},
	number = {7},
	urldate = {2023-12-15},
	journal = {Journal of Geophysical Research: Earth Surface},
	author = {Vitousek, Sean and Vos, Kilian and Splinter, Kristen D. and Erikson, Li and Barnard, Patrick L.},
	year = {2023},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1029/2022JF006936},
	keywords = {coastal change, data assimilation, ensemble Kalman filter, satellite-derived shorelines, sea-level rise, shoreline model},
	pages = {e2022JF006936},
}

@article{thunis_sensitivity_2021,
	title = {Sensitivity of air quality modelling to different emission inventories: {A} case study over {Europe}},
	volume = {10},
	issn = {2590-1621},
	shorttitle = {Sensitivity of air quality modelling to different emission inventories},
	url = {https://www.sciencedirect.com/science/article/pii/S2590162121000113},
	doi = {10.1016/j.aeaoa.2021.100111},
	abstract = {In this paper we run the EMEP air quality model with 3 state-of-the-art emission inventories for Europe, that is to say with CAMS-REG-AP, EMEP emissions, and EDGAR. The main aim of the work is to evaluate if the EDGAR inventory (despite its global coverage and independent approach to emission evaluation) is accurate enough, to be used for air quality modelling in Europe. To do so, the EMEP model has been run for the entire year 2015 using the 3 aforementioned emission inventories, and the resulting concentrations have been compared with ‘background’ monitoring stations. Results show that the air quality model, run with the 3 emission inventories, provide similar results despite important differences in emissions. EDGAR emissions provide slightly better validation results for PM2.5, while the EMEP emissions lead to slightly better results for yearly average NO2. The main differences among the model applications arise in the Eastern part of Europe, where the deviations between the officially estimated emissions and those independently evaluated by EDGAR are more important. Results suggest that EDGAR, despite being a methodology aimed at global coverage, with independent sources for activity level, technologies and emission factors and generic gridding practices, can be effectively used for air quality modelling in Europe. The EDGAR dataset (v5.0) used in this paper is available at: https://doi.pangaea.de/10.1594/PANGAEA.921922.},
	urldate = {2023-12-15},
	journal = {Atmospheric Environment: X},
	author = {Thunis, Philippe and Crippa, Monica and Cuvelier, Cornelis and Guizzardi, Diego and de Meij, Alexander and Oreggioni, Gabriel and Pisoni, Enrico},
	month = apr,
	year = {2021},
	keywords = {Air pollution concentrations, Air quality modelling, Emission inventories},
	pages = {100111},
}

@article{hallin_measure_2022,
	title = {Measure {Transportation} and {Statistical} {Decision} {Theory}},
	volume = {9},
	url = {https://doi.org/10.1146/annurev-statistics-040220-105948},
	doi = {10.1146/annurev-statistics-040220-105948},
	abstract = {Unlike the real line, the real space, in dimension d ≥ 2, is not canonically ordered. As a consequence, extending to a multivariate context fundamental univariate statistical tools such as quantiles, signs, and ranks is anything but obvious. Tentative definitions have been proposed in the literature but do not enjoy the basic properties (e.g., distribution-freeness of ranks, their independence with respect to the order statistic, their independence with respect to signs) they are expected to satisfy. Based on measure transportation ideas, new concepts of distribution and quantile functions, ranks, and signs have been proposed recently that, unlike previous attempts, do satisfy these properties. These ranks, signs, and quantiles have been used, quite successfully, in several inference problems and have triggered, in a short span of time, a number of applications: fully distribution-free testing for multiple-output regression, MANOVA, and VAR models; R-estimation for VARMA parameters; distribution-free testing for vector independence; multiple-output quantile regression; nonlinear independent component analysis; and so on.},
	number = {1},
	urldate = {2023-11-28},
	journal = {Annual Review of Statistics and Its Application},
	author = {Hallin, Marc},
	year = {2022},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-040220-105948},
	keywords = {R-estimation, ancillarity, distribution-freeness, local asymptotic normality, multivariate distribution function, multivariate quantiles, multivariate ranks, multivariate signs, optimal transport, parametric and semiparametric efficiency, rank tests},
	pages = {401--424},
}

@misc{mlss_africa_marco_2019,
	title = {Marco {Cuturi} {A} {Primer} on {Optimal} {Transport} {Part} 2},
	url = {https://www.youtube.com/watch?v=R49Xb9eAUBA},
	urldate = {2023-12-04},
	author = {{MLSS Africa}},
	year = {2019},
}

@misc{mlss_africa_marco_2019-1,
	title = {Marco {Cuturi} - {A} {Primer} on {Optimal} {Transport} {Part} 1},
	url = {https://www.youtube.com/watch?v=6iR1E6t1MMQ},
	abstract = {https://mlssafrica.com/},
	urldate = {2023-12-04},
	author = {{MLSS Africa}},
	year = {2019},
}

@misc{meng_scores_2023,
	title = {Scores for {Multivariate} {Distributions} and {Level} {Sets}},
	url = {http://arxiv.org/abs/2002.09578},
	doi = {10.48550/arXiv.2002.09578},
	abstract = {Forecasts of multivariate probability distributions are required for a variety of applications. Scoring rules enable the evaluation of forecast accuracy, and comparison between forecasting methods. We propose a theoretical framework for scoring rules for multivariate distributions, which encompasses the existing quadratic score and multivariate continuous ranked probability score. We demonstrate how this framework can be used to generate new scoring rules. In some multivariate contexts, it is a forecast of a level set that is needed, such as a density level set for anomaly detection or the level set of the cumulative distribution as a measure of risk. This motivates consideration of scoring functions for such level sets. For univariate distributions, it is well-established that the continuous ranked probability score can be expressed as the integral over a quantile score. We show that, in a similar way, scoring rules for multivariate distributions can be decomposed to obtain scoring functions for level sets. Using this, we present scoring functions for different types of level set, including density level sets and level sets for cumulative distributions. To compute the scores, we propose a simple numerical algorithm. We perform a simulation study to support our proposals, and we use real data to illustrate usefulness for forecast combining and CoVaR estimation.},
	urldate = {2023-11-29},
	publisher = {arXiv},
	author = {Meng, Xiaochun and Taylor, James W. and Taieb, Souhaib Ben and Li, Siran},
	month = jun,
	year = {2023},
	note = {arXiv:2002.09578 [math, q-fin, stat]},
	keywords = {Mathematics - Statistics Theory, Quantitative Finance - Statistical Finance, Statistics - Methodology},
}

@misc{virtual_mlss2020_optimal_2020,
	title = {Optimal {Transport}, part 1 - {Marco} {Cuturi} - {MLSS} 2020, {Tübingen}},
	url = {https://www.youtube.com/watch?v=jgrkhZ8ovVc},
	abstract = {Table of Contents (powered by https://videoken.com)
0:00:00 Optimal Transport
0:02:52 Why Optimal Transport?
0:07:11 Optimal Transport in Data Science
0:11:17 Short Course Outline
0:12:42 Introduction to OT
0:13:39 Origins: Monger Problem (1781)
0:22:09 Kantorovich Problem
0:27:38 Why Optimal Transport?
0:34:46 Mathematical Formalism
0:35:12 OT: Nature's way to move particles
0:39:48 Monger Problem
0:48:03 Example 2: Monge-Ampere Equation
0:50:42 Kantorovich Relaxation
0:52:50 Kantorovich Problem
0:59:15 Deriving Kantorovich Duality
1:02:12 Deriving Kantorovich Duality Let 4, 1 : 0 - R, and P E II( M,
1:08:44 Wasserstein Distances
1:10:46 Kantorovich Problem
1:12:35 Kantorovich Duality
1:13:18 transforms
1:20:21 transforms, WI
1:21:52 Links between Monger \& Kantorovic
1:24:18 Optimal Transport Geometry
1:26:22 Variational OT Problems in ML
1:27:40 2. Computing OT exactly},
	urldate = {2023-12-04},
	author = {{virtual mlss2020}},
	year = {2020},
}

@misc{feldman_improving_2021,
	title = {Improving {Conditional} {Coverage} via {Orthogonal} {Quantile} {Regression}},
	url = {http://arxiv.org/abs/2106.00394},
	doi = {10.48550/arXiv.2106.00394},
	abstract = {We develop a method to generate prediction intervals that have a user-specified coverage level across all regions of feature-space, a property called conditional coverage. A typical approach to this task is to estimate the conditional quantiles with quantile regression -- it is well-known that this leads to correct coverage in the large-sample limit, although it may not be accurate in finite samples. We find in experiments that traditional quantile regression can have poor conditional coverage. To remedy this, we modify the loss function to promote independence between the size of the intervals and the indicator of a miscoverage event. For the true conditional quantiles, these two quantities are independent (orthogonal), so the modified loss function continues to be valid. Moreover, we empirically show that the modified loss function leads to improved conditional coverage, as evaluated by several metrics. We also introduce two new metrics that check conditional coverage by looking at the strength of the dependence between the interval size and the indicator of miscoverage.},
	urldate = {2023-11-28},
	publisher = {arXiv},
	author = {Feldman, Shai and Bates, Stephen and Romano, Yaniv},
	month = oct,
	year = {2021},
	note = {arXiv:2106.00394 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@misc{genevay_gan_2017,
	title = {{GAN} and {VAE} from an {Optimal} {Transport} {Point} of {View}},
	url = {http://arxiv.org/abs/1706.01807},
	abstract = {This short article revisits some of the ideas introduced in arXiv:1701.07875 and arXiv:1705.07642 in a simple setup. This sheds some lights on the connexions between Variational Autoencoders (VAE), Generative Adversarial Networks (GAN) and Minimum Kantorovitch Estimators (MKE).},
	urldate = {2023-11-30},
	publisher = {arXiv},
	author = {Genevay, Aude and Peyré, Gabriel and Cuturi, Marco},
	month = jun,
	year = {2017},
	note = {arXiv:1706.01807 [stat]},
	keywords = {Statistics - Machine Learning},
}

@article{hallin_distribution_2021,
	title = {Distribution and quantile functions, ranks and signs in dimension d: {A} measure transportation approach},
	volume = {49},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Distribution and quantile functions, ranks and signs in dimension d},
	url = {https://projecteuclid.org/journals/annals-of-statistics/volume-49/issue-2/Distribution-and-quantile-functions-ranks-and-signs-in-dimension-d/10.1214/20-AOS1996.full},
	doi = {10.1214/20-AOS1996},
	abstract = {Unlike the real line, the real space Rd, for d≥2, is not canonically ordered. As a consequence, such fundamental univariate concepts as quantile and distribution functions and their empirical counterparts, involving ranks and signs, do not canonically extend to the multivariate context. Palliating that lack of a canonical ordering has been an open problem for more than half a century, generating an abundant literature and motivating, among others, the development of statistical depth and copula-based methods. We show that, unlike the many definitions proposed in the literature, the measure transportation-based ranks and signs introduced in Chernozhukov, Galichon, Hallin and Henry (Ann. Statist. 45 (2017) 223–256) enjoy all the properties that make univariate ranks a successful tool for semiparametric inference. Related with those ranks, we propose a new center-outward definition of multivariate distribution and quantile functions, along with their empirical counterparts, for which we establish a Glivenko–Cantelli result. Our approach is based on McCann (Duke Math. J. 80 (1995) 309–323) and our results do not require any moment assumptions. The resulting ranks and signs are shown to be strictly distribution-free and essentially maximal ancillary in the sense of Basu (Sankhyā 21 (1959) 247–256) which, in semiparametric models involving noise with unspecified density, can be interpreted as a finite-sample form of semiparametric efficiency. Although constituting a sufficient summary of the sample, empirical center-outward distribution functions are defined at observed values only. A continuous extension to the entire d-dimensional space, yielding smooth empirical quantile contours and sign curves while preserving the essential monotonicity and Glivenko–Cantelli features of the concept, is provided. A numerical study of the resulting empirical quantile contours is conducted.},
	number = {2},
	urldate = {2023-11-28},
	journal = {The Annals of Statistics},
	author = {Hallin, Marc and Barrio, Eustasio del and Cuesta-Albertos, Juan and Matrán, Carlos},
	month = apr,
	year = {2021},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {62B05, 62G30, Basu theorem, Glivenko–Cantelli theorem, Multivariate quantiles, ancillarity, cyclical monotonicity, distribution-freeness, multivariate distribution function, multivariate ranks, multivariate signs},
	pages = {1139--1165},
}

@book{panaretos_invitation_2020,
	address = {Cham},
	series = {{SpringerBriefs} in {Probability} and {Mathematical} {Statistics}},
	title = {An {Invitation} to {Statistics} in {Wasserstein} {Space}},
	isbn = {978-3-030-38437-1 978-3-030-38438-8},
	url = {http://link.springer.com/10.1007/978-3-030-38438-8},
	language = {en},
	urldate = {2023-12-11},
	publisher = {Springer International Publishing},
	author = {Panaretos, Victor M. and Zemel, Yoav},
	year = {2020},
	doi = {10.1007/978-3-030-38438-8},
	keywords = {Barycenter, Fréchet mean, Functional Data Analysis, Geometrical statistics, Gradient descent, Manifold Statistics, Monge-Kantorovich Problem, Multimarginal Transport, Open Access, Optimal Transportation, Phase variation, Point Processes, Procrustes analysis, Random Measures, Wasserstein metric},
}

@misc{noauthor_mapie_nodate,
	title = {{MAPIE} - {Model} {Agnostic} {Prediction} {Interval} {Estimator} — {MAPIE} 0.7.0 documentation},
	url = {https://mapie.readthedocs.io/en/latest/},
	urldate = {2023-12-12},
}

@article{stock_identification_2018,
	title = {Identification and {Estimation} of {Dynamic} {Causal} {Effects} in {Macroeconomics} {Using} {External} {Instruments}},
	volume = {128},
	issn = {0013-0133, 1468-0297},
	url = {https://academic.oup.com/ej/article/128/610/917-948/5069563},
	doi = {10.1111/ecoj.12593},
	language = {en},
	number = {610},
	urldate = {2023-12-01},
	journal = {The Economic Journal},
	author = {Stock, James H. and Watson, Mark W.},
	month = may,
	year = {2018},
	pages = {917--948},
}

@article{plagborg-moller_local_2021,
	title = {Local {Projections} and {VARs} {Estimate} the {Same} {Impulse} {Responses}},
	volume = {89},
	copyright = {© 2021 The Econometric Society},
	issn = {1468-0262},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.3982/ECTA17813},
	doi = {10.3982/ECTA17813},
	abstract = {We prove that local projections (LPs) and Vector Autoregressions (VARs) estimate the same impulse responses. This nonparametric result only requires unrestricted lag structures. We discuss several implications: (i) LP and VAR estimators are not conceptually separate procedures; instead, they are simply two dimension reduction techniques with common estimand but different finite-sample properties. (ii) VAR-based structural identification—including short-run, long-run, or sign restrictions—can equivalently be performed using LPs, and vice versa. (iii) Structural estimation with an instrument (proxy) can be carried out by ordering the instrument first in a recursive VAR, even under noninvertibility. (iv) Linear VARs are as robust to nonlinearities as linear LPs.},
	language = {en},
	number = {2},
	urldate = {2023-12-01},
	journal = {Econometrica},
	author = {Plagborg-Møller, Mikkel and Wolf, Christian K.},
	year = {2021},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.3982/ECTA17813},
	keywords = {External instrument, impulse response function, local projection, proxy variable, structural vector autoregression},
	pages = {955--980},
}

@article{hall_permutation_2002,
	title = {Permutation {Tests} for {Equality} of {Distributions} in {High}-{Dimensional} {Settings}},
	volume = {89},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/4140582},
	abstract = {Motivated by applications in high-dimensional settings, we suggest a test of the hypothesis H0 that two sampled distributions are identical. It is assumed that two independent datasets are drawn from the respective populations, which may be very general. In particular, the distributions may be multivariate or infinite-dimensional, in the latter case representing, for example, the distributions of random functions from one Euclidean space to another. Our test uses a measure of distance between data. This measure should be symmetric but need not satisfy the triangle inequality, so it is not essential that it be a metric. The test is based on ranking the pooled dataset, with respect to the distance and relative to any fixed data value, and repeating this operation for each fixed datum. A permutation argument enables a critical point to be chosen such that the test has concisely known significance level, conditional on the set of all pairwise distances.},
	number = {2},
	urldate = {2023-11-24},
	journal = {Biometrika},
	author = {Hall, Peter and Tajvidi, Nader},
	year = {2002},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {359--374},
}

@article{izbicki_cd-split_2022,
	title = {{CD}-split and {HPD}-split: {Efﬁcient} {Conformal} {Regions} in {High} {Dimensions}},
	abstract = {Conformal methods create prediction bands that control average coverage assuming solely i.i.d. data. Although the literature has mostly focused on prediction intervals, more general regions can often better represent uncertainty. For instance, a bimodal target is better represented by the union of two intervals. Such prediction regions are obtained by CD-split, which combines the split method and a data-driven partition of the feature space which scales to high dimensions. CD-split however contains many tuning parameters, and their role is not clear. In this paper, we provide new insights on CD-split by exploring its theoretical properties. In particular, we show that CD-split converges asymptotically to the oracle highest predictive density set and satisﬁes local and asymptotic conditional validity. We also present simulations that show how to tune CD-split. Finally, we introduce HPD-split, a variation of CD-split that requires less tuning, and show that it shares the same theoretical guarantees as CD-split. In a wide variety of our simulations, CD-split and HPD-split have better conditional coverage and yield smaller prediction regions than other methods.},
	language = {en},
	author = {Izbicki, Rafael and Shimizu, Gilson and Stern, Rafael B},
	year = {2022},
}

@misc{feldman_calibrated_2022,
	title = {Calibrated {Multiple}-{Output} {Quantile} {Regression} with {Representation} {Learning}},
	url = {http://arxiv.org/abs/2110.00816},
	doi = {10.48550/arXiv.2110.00816},
	abstract = {We develop a method to generate predictive regions that cover a multivariate response variable with a user-specified probability. Our work is composed of two components. First, we use a deep generative model to learn a representation of the response that has a unimodal distribution. Existing multiple-output quantile regression approaches are effective in such cases, so we apply them on the learned representation, and then transform the solution to the original space of the response. This process results in a flexible and informative region that can have an arbitrary shape, a property that existing methods lack. Second, we propose an extension of conformal prediction to the multivariate response setting that modifies any method to return sets with a pre-specified coverage level. The desired coverage is theoretically guaranteed in the finite-sample case for any distribution. Experiments conducted on both real and synthetic data show that our method constructs regions that are significantly smaller compared to existing techniques.},
	urldate = {2023-11-23},
	publisher = {arXiv},
	author = {Feldman, Shai and Bates, Stephen and Romano, Yaniv},
	year = {2022},
	note = {arXiv:2110.00816 [cs]},
	keywords = {Computer Science - Machine Learning},
}

@article{sun_copula_2022,
	title = {Copula {Conformal} {Prediction} for {Multi}-step {Time} {Series} {Forecasting}},
	url = {https://openreview.net/forum?id=jCdoLxMZxf},
	abstract = {Accurate uncertainty measurement is a key step to building robust and reliable machine learning systems. Conformal prediction is a distribution-free uncertainty quantification algorithm popular for its ease of implementation, statistical coverage guarantees, and versatility for underlying forecasters. However, existing conformal prediction algorithms for time series are limited to single-step prediction without considering the temporal dependency. In this paper we propose a Copula-based Conformal Prediction algorithm for multivariate, multi-step Time Series forecasting, CopulaCPTS. On several synthetic and real-world multivariate time series datasets, we show that CopulaCPTS produces more calibrated and sharp confidence intervals for multi-step prediction tasks than existing techniques.},
	language = {en},
	urldate = {2023-11-23},
	author = {Sun, Sophia Huiwen and Yu, Rose},
	month = sep,
	year = {2022},
}

@article{lazer_computational_2020,
	title = {Computational social science: {Obstacles} and opportunities},
	volume = {369},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Computational social science},
	url = {https://www.science.org/doi/10.1126/science.aaz8170},
	doi = {10.1126/science.aaz8170},
	abstract = {Data sharing, research ethics, and incentives must improve
          , 
            The field of computational social science (CSS) has exploded in prominence over the past decade, with thousands of papers published using observational data, experimental designs, and large-scale simulations that were once unfeasible or unavailable to researchers. These studies have greatly improved our understanding of important phenomena, ranging from social inequality to the spread of infectious diseases. The institutions supporting CSS in the academy have also grown substantially, as evidenced by the proliferation of conferences, workshops, and summer schools across the globe, across disciplines, and across sources of data. But the field has also fallen short in important ways. Many institutional structures around the field—including research ethics, pedagogy, and data infrastructure—are still nascent. We suggest opportunities to address these issues, especially in improving the alignment between the organization of the 20th-century university and the intellectual requirements of the field.},
	language = {en},
	number = {6507},
	urldate = {2022-01-28},
	journal = {Science},
	author = {Lazer, David and Pentland, Alex and Watts, Duncan J. and Aral, Sinan and Athey, Susan and Contractor, Noshir and Freelon, Deen and Gonzalez-Bailon, Sandra and King, Gary and Margetts, Helen and Nelson, Alondra and Salganik, Matthew J. and Strohmaier, Markus and Vespignani, Alessandro and Wagner, Claudia},
	month = aug,
	year = {2020},
	pages = {1060--1062},
}

@misc{noauthor_posta_nodate,
	title = {Posta in arrivo (25) - mtt.fnt@gmail.com - {Gmail}},
	url = {https://mail.google.com/mail/u/0/#inbox},
	urldate = {2023-10-27},
}

@book{johnson_applied_2002,
	address = {Upper Saddle River, NJ},
	edition = {5. ed},
	title = {Applied multivariate statistical analysis},
	isbn = {978-0-13-092553-4 978-0-13-092554-1},
	language = {eng},
	publisher = {Prentice Hall},
	author = {Johnson, Richard Arnold and Wichern, Dean W.},
	year = {2002},
}

@article{groves-kirkby_large-scale_2023,
	title = {Large-scale calibration and simulation of {COVID}-19 epidemiologic scenarios to support healthcare planning},
	volume = {42},
	issn = {1755-4365},
	url = {https://www.sciencedirect.com/science/article/pii/S1755436522001025},
	doi = {10.1016/j.epidem.2022.100662},
	abstract = {The COVID-19 pandemic has provided stiff challenges for planning and resourcing in health services in the UK and worldwide. Epidemiological models can provide simulations of how infectious disease might progress in a population given certain parameters. We adapted an agent-based model of COVID-19 to inform planning and decision-making within a healthcare setting, and created a software framework that automates processes for calibrating the model parameters to health data and allows the model to be run at national population scale on National Health Service (NHS) infrastructure. We developed a method for calibrating the model to three daily data streams (hospital admissions, intensive care occupancy, and deaths), and demonstrate that on cross-validation the model fits acceptably to unseen data streams including official estimates of COVID-19 incidence. Once calibrated, we use the model to simulate future scenarios of the spread of COVID-19 in England and show that the simulations provide useful projections of future COVID-19 clinical demand. These simulations were used to support operational planning in the NHS in England, and we present the example of the use of these simulations in projecting future clinical demand during the rollout of the national COVID-19 vaccination programme. Being able to investigate uncertainty and test sensitivities was particularly important to the operational planning team. This epidemiological model operates within an ecosystem of data technologies, drawing on a range of NHS, government and academic data sources, and provides results to strategists, planners and downstream data systems. We discuss the data resources that enabled this work and the data challenges that were faced.},
	urldate = {2023-10-27},
	journal = {Epidemics},
	author = {Groves-Kirkby, Nick and Wakeman, Ewan and Patel, Seema and Hinch, Robert and Poot, Tineke and Pearson, Jonathan and Tang, Lily and Kendall, Edward and Tang, Ming and Moore, Kim and Stevenson, Scott and Mathias, Bryn and Feige, Ilya and Nakach, Simon and Stevenson, Laura and O'Dwyer, Paul and Probert, William and Panovska-Griffiths, Jasmina and Fraser, Christophe},
	month = mar,
	year = {2023},
	keywords = {Agent-based models, Data, Epidemiology, Healthcare, Model calibration, Modelling},
	pages = {100662},
}

@article{sung_efficient_2023,
	title = {Efficient calibration for imperfect epidemic models with applications to the analysis of {COVID}-19},
	issn = {0035-9254},
	url = {https://doi.org/10.1093/jrsssc/qlad083},
	doi = {10.1093/jrsssc/qlad083},
	abstract = {The estimation of unknown parameters in simulations, also known as calibration, is crucial for practical management of epidemics and prediction of pandemic risk. A simple yet widely used approach is to estimate the parameters by minimising the sum of the squared distances between actual observations and simulation outputs. It is shown in this paper that this method is inefficient, particularly when the epidemic models are developed based on certain simplifications of reality, also known as imperfect models which are commonly used in practice. To address this issue, a new estimator is introduced that is asymptotically consistent, has a smaller estimation variance than the least-squares estimator, and achieves the semiparametric efficiency. Numerical studies are performed to examine the finite sample performance. The proposed method is applied to the analysis of the COVID-19 pandemic for 20 countries based on the susceptible-exposed-infectious-recovered model with both deterministic and stochastic simulations. The estimation of the parameters, including the basic reproduction number and the average incubation period, reveal the risk of disease outbreaks in each country and provide insights to the design of public health interventions.},
	urldate = {2023-10-27},
	journal = {Journal of the Royal Statistical Society Series C: Applied Statistics},
	author = {Sung, Chih-Li and Hung, Ying},
	month = sep,
	year = {2023},
	pages = {qlad083},
}

@article{ajroldi_conformal_2023,
	title = {Conformal prediction bands for two-dimensional functional time series},
	volume = {187},
	copyright = {All rights reserved},
	issn = {0167-9473},
	url = {https://www.sciencedirect.com/science/article/pii/S0167947323001329},
	doi = {10.1016/j.csda.2023.107821},
	abstract = {Time evolving surfaces can be modeled as two-dimensional Functional time series, exploiting the tools of Functional data analysis. Leveraging this approach, a forecasting framework for such complex data is developed. The main focus revolves around Conformal Prediction, a versatile nonparametric paradigm used to quantify uncertainty in prediction problems. Building upon recent variations of Conformal Prediction for Functional time series, a probabilistic forecasting scheme for two-dimensional functional time series is presented, while providing an extension of Functional Autoregressive Processes of order one to this setting. Estimation techniques for the latter process are introduced, and their performance are compared in terms of the resulting prediction regions. Finally, the proposed forecasting procedure and the uncertainty quantification technique are applied to a real dataset, collecting daily observations of Sea Level Anomalies of the Black Sea.},
	urldate = {2023-10-25},
	journal = {Computational Statistics \& Data Analysis},
	author = {Ajroldi, Niccolò and Diquigiovanni, Jacopo and Fontana, Matteo and Vantini, Simone},
	month = nov,
	year = {2023},
	keywords = {Conformal prediction, Forecasting, Functional autoregressive process, Functional time series, Two-dimensional functional data, Uncertainty quantification},
	pages = {107821},
}

@misc{balakrishnan_conservative_2023,
	title = {Conservative {Inference} for {Counterfactuals}},
	url = {http://arxiv.org/abs/2310.12757},
	abstract = {In causal inference, the joint law of a set of counterfactual random variables is generally not identified. We show that a conservative version of the joint law - corresponding to the smallest treatment effect - is identified. Finding this law uses recent results from optimal transport theory. Under this conservative law we can bound causal effects and we may construct inferences for each individual's counterfactual dose-response curve. Intuitively, this is the flattest counterfactual curve for each subject that is consistent with the distribution of the observables. If the outcome is univariate then, under mild conditions, this curve is simply the quantile function of the counterfactual distribution that passes through the observed point. This curve corresponds to a nonparametric rank preserving structural model.},
	urldate = {2023-10-25},
	publisher = {arXiv},
	author = {Balakrishnan, Sivaraman and Kennedy, Edward and Wasserman, Larry},
	month = oct,
	year = {2023},
	note = {arXiv:2310.12757 [math, stat]},
	keywords = {Mathematics - Statistics Theory, Statistics - Methodology},
}

@article{li_multivariate_2022,
	series = {50th {Anniversary} {Jubilee} {Edition}},
	title = {From multivariate to functional data analysis: {Fundamentals}, recent developments, and emerging areas},
	volume = {188},
	issn = {0047-259X},
	shorttitle = {From multivariate to functional data analysis},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X21000841},
	doi = {10.1016/j.jmva.2021.104806},
	abstract = {Functional data analysis (FDA), which is a branch of statistics on modeling infinite dimensional random vectors resided in functional spaces, has become a major research area for Journal of Multivariate Analysis. We review some fundamental concepts of FDA, their origins and connections from multivariate analysis, and some of its recent developments, including multi-level functional data analysis, high-dimensional functional regression, and dependent functional data analysis. We also discuss the impact of these new methodology developments on genetics, plant science, wearable device data analysis, image data analysis, and business analytics. Two real data examples are provided to motivate our discussions.},
	urldate = {2023-10-17},
	journal = {Journal of Multivariate Analysis},
	author = {Li, Yehua and Qiu, Yumou and Xu, Yuhang},
	month = mar,
	year = {2022},
	keywords = {Functional data analysis, High-dimensional statistics, Multi-level modeling, Spatial dependence},
	pages = {104806},
}

@article{morris_functional_2015,
	title = {Functional {Regression}},
	volume = {2},
	url = {https://doi.org/10.1146/annurev-statistics-010814-020413},
	doi = {10.1146/annurev-statistics-010814-020413},
	abstract = {Functional data analysis (FDA) involves the analysis of data whose ideal units of observation are functions defined on some continuous domain, and the observed data consist of a sample of functions taken from some population, sampled on a discrete grid. Ramsay \& Silverman's (1997) textbook sparked the development of this field, which has accelerated in the past 10 years to become one of the fastest growing areas of statistics, fueled by the growing number of applications yielding this type of data. One unique characteristic of FDA is the need to combine information both across and within functions, which Ramsay and Silverman called replication and regularization, respectively. This article focuses on functional regression, the area of FDA that has received the most attention in applications and methodological development. First, there is an introduction to basis functions, key building blocks for regularization in functional regression methods, followed by an overview of functional regression methods, split into three types: (a) functional predictor regression (scalar-on-function), (b) functional response regression (function-on-scalar), and (c) function-on-function regression. For each, the role of replication and regularization is discussed and the methodological development described in a roughly chronological manner, at times deviating from the historical timeline to group together similar methods. The primary focus is on modeling and methodology, highlighting the modeling structures that have been developed and the various regularization approaches employed. The review concludes with a brief discussion describing potential areas of future development in this field.},
	number = {1},
	urldate = {2023-10-17},
	journal = {Annual Review of Statistics and Its Application},
	author = {Morris, Jeffrey S.},
	year = {2015},
	note = {\_eprint: https://doi.org/10.1146/annurev-statistics-010814-020413},
	keywords = {functional data analysis, functional mixed effect models, generalized additive models, principal component analysis, splines, wavelets},
	pages = {321--359},
}

@article{dottori_sir_2014,
	title = {{SIR} model on a dynamical network and the endemic state of an infectious disease},
	copyright = {arXiv.org perpetual, non-exclusive license},
	url = {https://arxiv.org/abs/1410.1383},
	doi = {10.48550/ARXIV.1410.1383},
	abstract = {In this work we performed a numerical study of an epidemic model that mimics the endemic state of whooping cough in the pre-vaccine era. We considered a stochastic SIR model on dynamical networks that involve local and global contacts among individuals and analyzed the influence of the network properties on the characterization of the quasi-stationary state. We computed probability density functions (PDF) for infected fraction of individuals and found that they are well fitted by gamma functions, excepted the tails of the distributions that are q-exponentials. We also computed the fluctuation power spectra of infective time series for different networks. We found that network effects can be partially absorbed by rescaling the rate of infective contacts of the model. An explicit relation between the effective transmission rate of the disease and the correlation of susceptible individuals with their infective nearest neighbours was obtained. This relation quantifies the known screening of infective individuals observed in these networks. We finally discuss the goodness and limitations of the SIR model with homogeneous mixing and parameters taken from epidemiological data to describe the dynamic behaviour observed in the networks studied.},
	urldate = {2023-09-28},
	author = {Dottori, Martin and Fabricius, Gabriel},
	year = {2014},
	keywords = {FOS: Biological sciences, FOS: Physical sciences, Physics and Society (physics.soc-ph), Populations and Evolution (q-bio.PE), Statistical Mechanics (cond-mat.stat-mech)},
}

@inproceedings{lekeufack_conformal_2023,
	title = {Conformal {Decision} {Theory}: {Safe} {Autonomous} {Decisions} from {Imperfect} {Predictions}},
	shorttitle = {Conformal {Decision} {Theory}},
	url = {https://www.semanticscholar.org/paper/5ae9ee919c4dfa033929a26e04ac874eecbe08e2},
	abstract = {We introduce Conformal Decision Theory, a framework for producing safe autonomous decisions despite imperfect machine learning predictions. Examples of such decisions are ubiquitous, from robot planning algorithms that rely on pedestrian predictions, to calibrating autonomous manufacturing to exhibit high throughput and low error, to the choice of trusting a nominal policy versus switching to a safe backup policy at run-time. The decisions produced by our algorithms are safe in the sense that they come with provable statistical guarantees of having low risk without any assumptions on the world model whatsoever; the observations need not be I.I.D. and can even be adversarial. The theory extends results from conformal prediction to calibrate decisions directly, without requiring the construction of prediction sets. Experiments demonstrate the utility of our approach in robot motion planning around humans, automated stock trading, and robot manufacturing.},
	urldate = {2023-10-16},
	author = {Lekeufack, Jordan and Angelopoulos, Anastasios Nikolas and Bajcsy, Andrea V. and Jordan, Michael I. and Malik, Jitendra},
	month = oct,
	year = {2023},
}

@article{kohler_robust_2021,
	title = {Robust and optimal predictive control of the {COVID}-19 outbreak},
	volume = {51},
	issn = {1367-5788},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7757387/},
	doi = {10.1016/j.arcontrol.2020.11.002},
	abstract = {We investigate adaptive strategies to robustly and optimally control the COVID-19 pandemic via social distancing measures based on the example of Germany. Our goal is to minimize the number of fatalities over the course of two years without inducing excessive social costs. We consider a tailored model of the German COVID-19 outbreak with different parameter sets to design and validate our approach. Our analysis reveals that an open-loop optimal control policy can significantly decrease the number of fatalities when compared to simpler policies under the assumption of exact model knowledge. In a more realistic scenario with uncertain data and model mismatch, a feedback strategy that updates the policy weekly using model predictive control (MPC) leads to a reliable performance, even when applied to a validation model with deviant parameters. On top of that, we propose a robust MPC-based feedback policy using interval arithmetic that adapts the social distancing measures cautiously and safely, thus leading to a minimum number of fatalities even if measurements are inaccurate and the infection rates cannot be precisely specified by social distancing. Our theoretical findings support various recent studies by showing that (1) adaptive feedback strategies are required to reliably contain the COVID-19 outbreak, (2) well-designed policies can significantly reduce the number of fatalities compared to simpler ones while keeping the amount of social distancing measures on the same level, and (3) imposing stronger social distancing measures early on is more effective and cheaper in the long run than opening up too soon and restoring stricter measures at a later time.},
	urldate = {2023-10-16},
	journal = {Annual Reviews in Control},
	author = {Köhler, Johannes and Schwenkel, Lukas and Koch, Anne and Berberich, Julian and Pauli, Patricia and Allgöwer, Frank},
	year = {2021},
	pmid = {33362428},
	pmcid = {PMC7757387},
	pages = {525--539},
}

@article{dutta_using_2021,
	title = {Using mobility data in the design of optimal lockdown strategies for the {COVID}-19 pandemic},
	volume = {17},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1009236},
	doi = {10.1371/journal.pcbi.1009236},
	abstract = {A mathematical model for the COVID-19 pandemic spread, which integrates age-structured Susceptible-Exposed-Infected-Recovered-Deceased dynamics with real mobile phone data accounting for the population mobility, is presented. The dynamical model adjustment is performed via Approximate Bayesian Computation. Optimal lockdown and exit strategies are determined based on nonlinear model predictive control, constrained to public-health and socio-economic factors. Through an extensive computational validation of the methodology, it is shown that it is possible to compute robust exit strategies with realistic reduced mobility values to inform public policy making, and we exemplify the applicability of the methodology using datasets from England and France.},
	language = {en},
	number = {8},
	urldate = {2023-10-16},
	journal = {PLOS Computational Biology},
	author = {Dutta, Ritabrata and Gomes, Susana N. and Kalise, Dante and Pacchiardi, Lorenzo},
	month = aug,
	year = {2021},
	note = {Publisher: Public Library of Science},
	keywords = {Age groups, COVID 19, Economics, England, France, Health economics, Optimization, Pandemics},
	pages = {e1009236},
}

@article{acemoglu_optimal_2021,
	title = {Optimal {Targeted} {Lockdowns} in a {Multigroup} {SIR} {Model}},
	volume = {3},
	issn = {2640-205X, 2640-2068},
	url = {https://pubs.aeaweb.org/doi/10.1257/aeri.20200590},
	doi = {10.1257/aeri.20200590},
	abstract = {We study targeted lockdowns in a multigroup SIR model where infection, hospitalization, and fatality rates vary between groups—in particular between the “young,” the “middle-aged,” and the “old.” Our model enables a tractable quantitative analysis of optimal policy. For baseline parameter values for the COVID-19 pandemic applied to the US, we find that optimal policies differentially targeting risk/age groups significantly outperform optimal uniform policies and most of the gains can be realized by having stricter protective measures such as lockdowns on the more vulnerable, old group. Intuitively, a strict and long lockdown for the old both reduces infections and enables less strict lockdowns for the lower-risk groups. (JEL H51, I12, I18, J13, J14)},
	language = {en},
	number = {4},
	urldate = {2023-10-16},
	journal = {American Economic Review: Insights},
	author = {Acemoglu, Daron and Chernozhukov, Victor and Werning, Iván and Whinston, Michael D.},
	month = dec,
	year = {2021},
	pages = {487--502},
}

@article{bassett_strong_1986,
	title = {Strong {Consistency} of {Regression} {Quantiles} and {Related} {Empirical} {Processes}},
	volume = {2},
	issn = {0266-4666},
	url = {https://www.jstor.org/stable/3532422},
	abstract = {The strong consistency of regression quantile statistics (Koenker and Bassett [4]) in linear models with iid errors is established. Mild regularity conditions on the regression design sequence and the error distribution are required. Strong consistency of the associated empirical quantile process (introduced in Bassett and Koenker [1]) is also established under analogous conditions. However, for the proposed estimate of the conditional distribution function of Y, no regularity conditions on the error distribution are required for uniform strong convergence, thus establishing a Glivenko-Cantelli-type theorem for this estimator.},
	number = {2},
	urldate = {2023-10-12},
	journal = {Econometric Theory},
	author = {Bassett, Gilbert W. and Koenker, Roger W.},
	year = {1986},
	note = {Publisher: Cambridge University Press},
	pages = {191--201},
}

@article{chernozhukov_distributional_2021,
	title = {Distributional conformal prediction},
	volume = {118},
	url = {https://www.pnas.org/doi/10.1073/pnas.2107794118},
	doi = {10.1073/pnas.2107794118},
	abstract = {We propose a robust method for constructing conditionally valid prediction intervals based on models for conditional distributions such as quantile and distribution regression. Our approach can be applied to important prediction problems, including cross-sectional prediction, k–step-ahead forecasts, synthetic controls and counterfactual prediction, and individual treatment effects prediction. Our method exploits the probability integral transform and relies on permuting estimated ranks. Unlike regression residuals, ranks are independent of the predictors, allowing us to construct conditionally valid prediction intervals under heteroskedasticity. We establish approximate conditional validity under consistent estimation and provide approximate unconditional validity under model misspecification, under overfitting, and with time series data. We also propose a simple “shape” adjustment of our baseline method that yields optimal prediction intervals.},
	number = {48},
	urldate = {2023-10-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Chernozhukov, Victor and Wüthrich, Kaspar and Zhu, Yinchu},
	month = nov,
	year = {2021},
	note = {Publisher: Proceedings of the National Academy of Sciences},
	pages = {e2107794118},
}

@article{adrian_term_2022,
	title = {The {Term} {Structure} of {Growth}-at-{Risk}},
	volume = {14},
	issn = {1945-7707},
	url = {https://www.aeaweb.org/articles?id=10.1257/mac.20180428},
	doi = {10.1257/mac.20180428},
	abstract = {We show that the conditional distribution of forecasted GDP growth depends on financial conditions in a panel of 11 advanced economies. Financial conditions have a larger effect on the lower fifth percentile of conditional growth—which we call growth-at-risk (GaR)—than the median. In addition, the term structure of GaR reflects that when initial financial conditions are loose, downside risks are lower in the near term but increase in later quarters. This intertemporal trade-off for loose financial conditions is amplified when credit-to-GDP growth is rapid. Using granular instrumental variables, we also provide evidence that the relationship from loose financial conditions to future downside risks is causal. Our results suggest that models of macrofinancial linkages should incorporate the endogeneity of higher-order moments to systematically account for downside risks to growth in the medium run.},
	language = {en},
	number = {3},
	urldate = {2023-10-04},
	journal = {American Economic Journal: Macroeconomics},
	author = {Adrian, Tobias and Grinberg, Federico and Liang, Nellie and Malik, Sheheryar and Yu, Jie},
	month = jul,
	year = {2022},
	keywords = {Cycles, Financial Markets and the Macroeconomy, Macroeconomics: Production, Macroeconomics: Consumption, Saving, Production, Employment, and Investment: Forecasting and Simulation: Models and Applications, Business Fluctuations},
	pages = {283--323},
}

@article{adrian_vulnerable_2019,
	title = {Vulnerable {Growth}},
	volume = {109},
	issn = {0002-8282},
	url = {https://www.aeaweb.org/articles?id=10.1257/aer.20161923},
	doi = {10.1257/aer.20161923},
	abstract = {We study the conditional distribution of GDP growth as a function of economic and financial conditions. Deteriorating financial conditions are associated with an increase in the conditional volatility and a decline in the conditional mean of GDP growth, leading the lower quantiles of GDP growth to vary with financial conditions and the upper quantiles to be stable over time. Upside risks to GDP growth are low in most periods while downside risks increase as financial conditions become tighter. We argue that amplification mechanisms in the financial sector generate the observed growth vulnerability dynamics.},
	language = {en},
	number = {4},
	urldate = {2023-10-04},
	journal = {American Economic Review},
	author = {Adrian, Tobias and Boyarchenko, Nina and Giannone, Domenico},
	month = apr,
	year = {2019},
	keywords = {Cycles, Financial Markets and the Macroeconomy, Forecasting Models, Simulation Methods, Macroeconomics: Production, Macroeconomics: Consumption, Saving, Production, Employment, and Investment: Forecasting and Simulation: Models and Applications, Business Fluctuations},
	pages = {1263--1289},
}

@article{plagborg-moller_when_2020,
	title = {When {Is} {Growth} at {Risk}?},
	volume = {2020},
	issn = {1533-4465},
	url = {https://muse.jhu.edu/article/787103},
	doi = {10.1353/eca.2020.0002},
	abstract = {This paper empirically evaluates the potentially nonlinear nexus between ﬁnancial indicators and the distribution of future GDP growth, using a rich set of macroeconomic and ﬁnancial variables covering thirteen advanced economies. We evaluate the out-of-sample forecast performance of ﬁnancial variables for GDP growth, including a fully real-time exercise based on a ﬂexible nonparametric model. We also use a parametric model to estimate the moments of the time-varying distribution of GDP and evaluate their in-sample estimation uncertainty. Our overall conclusion is pessimistic: moments other than the conditional mean are poorly estimated, and no predictors we consider provide robust and precise advance warnings of tail risks or indeed about any features of the GDP growth distribution other than the mean. In particular, ﬁnancial variables contribute little to such distributional forecasts, beyond the information contained in real indicators.},
	language = {en},
	number = {1},
	urldate = {2023-10-04},
	journal = {Brookings Papers on Economic Activity},
	author = {Plagborg-Møller, Mikkel and Reichlin, Lucrezia and Ricco, Giovanni and Hasenzagl, Thomas},
	year = {2020},
	pages = {167--229},
}

@incollection{lee_quantile_2015,
	address = {New York, NY},
	title = {Quantile {Regression} and {Value} at {Risk}},
	isbn = {978-1-4614-7749-5 978-1-4614-7750-1},
	url = {http://link.springer.com/10.1007/978-1-4614-7750-1_41},
	language = {en},
	urldate = {2023-10-03},
	booktitle = {Handbook of {Financial} {Econometrics} and {Statistics}},
	publisher = {Springer New York},
	author = {Xiao, Zhijie and Guo, Hongtao and Lam, Miranda S.},
	editor = {Lee, Cheng-Few and Lee, John C.},
	year = {2015},
	doi = {10.1007/978-1-4614-7750-1_41},
	pages = {1143--1167},
}

@article{meeks_heterogeneous_2023,
	title = {Heterogeneous beliefs and the {Phillips} curve},
	issn = {0304-3932},
	url = {https://www.sciencedirect.com/science/article/pii/S0304393223000703},
	doi = {10.1016/j.jmoneco.2023.06.003},
	abstract = {Heterogeneous beliefs modify the New Keynesian Phillips curve by introducing a term in the cross-section distribution of expectations. To take that model to the data, we develop a novel functional data approach to estimation and inference that accounts for variation in distributions of expectations. We find that this variation may be summarized using a handful of functional factors, and demonstrate their statistical and economic relevance for inflation dynamics. Our results are among the first to highlight the potential benefits to be gained in empirical work from a rigorous treatment of diverse beliefs in the study of macroeconomic outcomes.},
	urldate = {2023-10-03},
	journal = {Journal of Monetary Economics},
	author = {Meeks, Roland and Monti, Francesca},
	month = jun,
	year = {2023},
	keywords = {Functional principal components, Functional regression, Inflation dynamics, New Keynesian Phillips curve, Survey expectations},
}

@book{vovk_algorithmic_2023,
	address = {New York, NY},
	edition = {Second Edition},
	title = {Algorithmic learning in a random world},
	isbn = {978-3-031-06649-8},
	language = {en},
	publisher = {Springer},
	author = {Vovk, Vladimir and Gammerman, Alexander and Shafer, Glenn},
	year = {2023},
	note = {OCLC: 634599165},
}

@book{brumback_fundamentals_2022,
	address = {Boca Raton},
	edition = {First edition},
	series = {Texts in statistical science},
	title = {Fundamentals of causal inference: with {R}},
	isbn = {978-0-367-70505-3},
	shorttitle = {Fundamentals of causal inference},
	abstract = {"One of the primary motivations for clinical trials and observational studies of humans is to infer cause and effect. Disentangling causation from confounding is of utmost importance. Fundamentals of Causal Inference explains and relates different methods of confounding adjustment in terms of potential outcomes and graphical models, including standardization, difference-in-differences estimation, the front-door method, instrumental variables estimation, and propensity score methods. It also covers effect-measure modification, precision variables, mediation analyses, and time-dependent confounding. Several real data examples, simulation studies, and analyses using R motivate the methods throughout. The book assumes familiarity with basic statistics and probability, regression, and R and is suitable for seniors or graduate students in statistics, biostatistics, and data science as well as PhD students in a wide variety of other disciplines, including epidemiology, pharmacy, the health sciences, education, and the social, economic, and behavioral sciences. Beginning with a brief history and a review of essential elements of probability and statistics, a unique feature of the book is its focus on real and simulated datasets with all binary variables to reduce complex methods down to their fundamentals. Calculus is not required, but a willingness to tackle mathematical notation, difficult concepts, and intricate logical arguments is essential. While many real data examples are included, the book also features the Double What-If Study, based on simulated data with known causal mechanisms, in the belief that the methods are best understood in circumstances where they are known to either succeed or fail. Datasets, R code, and solutions to odd-numbered exercises are available at www.routledge.com. Babette A. Brumback is Professor and Associate Chair for Education in the Department of Biostatistics at the University of Florida; she won the department's Outstanding Teacher Award for 2020-2021. A Fellow of the American Statistical Association, she has researched and applied methods for causal inference since 1998, specializing in methods for time-dependent confounding, complex survey samples and clustered data"--},
	publisher = {CRC Press},
	author = {Brumback, Babette A.},
	year = {2022},
	keywords = {Acyclic models, Causation, Conditional expectations (Mathematics), Effect sizes (Statistics), Estimation theory, Inference, Mathematical models, R (Computer program language)},
}

@book{imbens_causal_2015,
	edition = {1},
	title = {Causal {Inference} for {Statistics}, {Social}, and {Biomedical} {Sciences}: {An} {Introduction}},
	isbn = {978-0-521-88588-1 978-1-139-02575-1},
	shorttitle = {Causal {Inference} for {Statistics}, {Social}, and {Biomedical} {Sciences}},
	url = {https://www.cambridge.org/core/product/identifier/9781139025751/type/book},
	urldate = {2022-11-17},
	publisher = {Cambridge University Press},
	author = {Imbens, Guido W. and Rubin, Donald B.},
	month = apr,
	year = {2015},
	doi = {10.1017/CBO9781139025751},
}

@article{lei_conformal_2021,
	title = {Conformal inference of counterfactuals and individual treatment effects},
	volume = {83},
	issn = {1369-7412, 1467-9868},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/rssb.12445},
	doi = {10.1111/rssb.12445},
	language = {en},
	number = {5},
	urldate = {2022-11-17},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Lei, Lihua and Candès, Emmanuel J.},
	month = nov,
	year = {2021},
	pages = {911--938},
}

@article{yin_conformal_2022,
	title = {Conformal {Sensitivity} {Analysis} for {Individual} {Treatment} {Effects}},
	issn = {0162-1459, 1537-274X},
	url = {https://www.tandfonline.com/doi/full/10.1080/01621459.2022.2102503},
	doi = {10.1080/01621459.2022.2102503},
	language = {en},
	urldate = {2022-11-17},
	journal = {Journal of the American Statistical Association},
	author = {Yin, Mingzhang and Shi, Claudia and Wang, Yixin and Blei, David M.},
	month = sep,
	year = {2022},
	pages = {1--14},
}

@book{wood_generalized_2017,
	edition = {2},
	title = {Generalized {Additive} {Models}: {An} {Introduction} with {R}},
	isbn = {978-1-315-37027-9},
	shorttitle = {Generalized {Additive} {Models}},
	url = {https://www.taylorfrancis.com/books/9781498728348},
	language = {en},
	urldate = {2022-11-07},
	publisher = {Chapman and Hall/CRC},
	author = {Wood, Simon N.},
	month = may,
	year = {2017},
	doi = {10.1201/9781315370279},
}

@book{davison_bootstrap_1997,
	edition = {1},
	title = {Bootstrap {Methods} and their {Application}},
	isbn = {978-0-521-57391-7 978-0-521-57471-6 978-0-511-80284-3},
	url = {https://www.cambridge.org/core/product/identifier/9780511802843/type/book},
	urldate = {2022-11-07},
	publisher = {Cambridge University Press},
	author = {Davison, A. C. and Hinkley, D. V.},
	month = oct,
	year = {1997},
	doi = {10.1017/CBO9780511802843},
}

@book{james_introduction_2013,
	address = {New York},
	series = {Springer texts in statistics},
	title = {An introduction to statistical learning: with applications in {R}},
	isbn = {978-1-4614-7137-0},
	shorttitle = {An introduction to statistical learning},
	number = {103},
	publisher = {Springer},
	editor = {James, Gareth and Witten, Daniela and Hastie, Trevor and Tibshirani, Robert},
	year = {2013},
	note = {OCLC: ocn828488009},
	keywords = {Mathematical models, Mathematical statistics, Problems, exercises, etc, R (Computer program language), Statistics},
}

@misc{vergottini_conformalinferencemulti_2022,
	title = {{conformalInference}.multi: {Conformal} {Inference} {Tools} for {Regression} with {Multivariate} {Response}},
	copyright = {All rights reserved},
	url = {https://cran.r-project.org/package=conformalInference.multi},
	author = {Vergottini, Paolo and Fontana, Matteo and Diquigiovanni, Jacopo and Solari, Aldo and Tibshirani, Ryan J.},
	year = {2022},
}

@misc{tibshirani_conformalinference_2022,
	title = {{conformalInference}: {R} {Software} tools for conformal prediction},
	copyright = {All rights reserved},
	url = {https://github.com/ryantibs/conformal/tree/master/conformalInference},
	author = {Tibshirani, Ryan J. and Vergottini, Paolo and Fontana, Matteo and Diquigiovanni, Jacopo and Solari, Aldo},
	year = {2022},
}

@misc{vergottini_conformalinferencefd_2022,
	title = {{conformalInference}.fd: {Tools} for {Conformal} {Inference} for {Regression} in {Multivariate} {Functional} {Setting}},
	copyright = {All rights reserved},
	url = {https://cran.r-project.org/package=conformalInference.fd},
	author = {Vergottini, Paolo and Fontana, Matteo and Diquigiovanni, Jacopo and Solari, Aldo and Tibshirani, Ryan J.},
	year = {2022},
}

@book{vespe_mobility_2021,
	address = {LU},
	title = {Mobility and economic impact of {COVID}-19 restrictions in {Italy} using mobile network operator data.},
	copyright = {All rights reserved},
	url = {https://data.europa.eu/doi/10.2760/241286},
	language = {eng},
	urldate = {2022-08-01},
	publisher = {Publications Office of the European Union},
	author = {Vespe, Michele and Minora, Umberto and Spyratos, Spyridon and Sermi, Francesco and Fontana, Matteo and Ciuffo, Biagio and Christidis, Panayotis},
	year = {2021},
}

@book{bertoni_mapping_2022,
	address = {LU},
	title = {Mapping the demand side of computational social science for policy: harnessing digital trace data and computational methods to address societal challenges.},
	copyright = {All rights reserved},
	shorttitle = {Mapping the demand side of computational social science for policy},
	url = {https://data.europa.eu/doi/10.2760/901622},
	language = {eng},
	urldate = {2022-03-30},
	publisher = {Publications Office of the European Union},
	editor = {Bertoni, Eleonora and Fontana, Matteo and Gabrielli, Lorenzo and Signorelli, Serena and Vespe, Michele},
	year = {2022},
}

@article{vergottini_conformalinferencemulti_2022-1,
	title = {{conformalInference}.multi and {conformalInference}.fd: {Twin} {Packages} for {Conformal} {Prediction}},
	copyright = {arXiv.org perpetual, non-exclusive license},
	shorttitle = {{conformalInference}.multi and {conformalInference}.fd},
	url = {https://arxiv.org/abs/2206.14663},
	doi = {10.48550/ARXIV.2206.14663},
	abstract = {Building on top of a regression model, Conformal Prediction methods produce distribution free prediction sets, requiring only i.i.d. data. While R packages implementing such methods for the univariate response framework have been developed, this is not the case with multivariate and functional responses. conformalInference.multi and conformalInference.fd address this void, by extending classical and more advanced conformal prediction methods like full conformal, split conformal, jackknife+ and multi split conformal to deal with the multivariate and functional case. The extreme flexibility of conformal prediction, fully embraced by the structure of the package, which does not require any specific regression model, enables users to pass in any regression function as input while using basic regression models as reference. Finally, the issue of visualisation is addressed by providing embedded plotting functions to visualize prediction regions.},
	urldate = {2022-08-01},
	author = {Vergottini, Paolo and Fontana, Matteo and Diquigiovanni, Jacopo and Solari, Aldo and Vantini, Simone},
	year = {2022},
	note = {Publisher: arXiv
Version Number: 1},
	keywords = {Computation (stat.CO), FOS: Computer and information sciences, Methodology (stat.ME)},
}

@incollection{aneiros_conformal_2020,
	address = {Cham},
	title = {A {Conformal} {Approach} for {Distribution}-free {Prediction} of {Functional} {Data}},
	copyright = {All rights reserved},
	isbn = {978-3-030-47755-4 978-3-030-47756-1},
	url = {http://link.springer.com/10.1007/978-3-030-47756-1_12},
	language = {en},
	urldate = {2022-08-01},
	booktitle = {Functional and {High}-{Dimensional} {Statistics} and {Related} {Fields}},
	publisher = {Springer International Publishing},
	author = {Fontana, Matteo and Vantini, Simone and Tavoni, Massimo and Gammerman, Alexander},
	editor = {Aneiros, Germán and Horová, Ivana and Hušková, Marie and Vieu, Philippe},
	year = {2020},
	doi = {10.1007/978-3-030-47756-1_12},
	note = {Series Title: Contributions to Statistics},
	pages = {83--90},
}

@article{centofanti_smooth_2022,
	title = {Smooth {LASSO} estimator for the {Function}-on-{Function} linear regression model},
	volume = {176},
	copyright = {All rights reserved},
	issn = {01679473},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167947322001360},
	doi = {10.1016/j.csda.2022.107556},
	language = {en},
	urldate = {2022-08-01},
	journal = {Computational Statistics \& Data Analysis},
	author = {Centofanti, Fabio and Fontana, Matteo and Lepore, Antonio and Vantini, Simone},
	month = dec,
	year = {2022},
	pages = {107556},
}

@incollection{medeiros_data_2022,
	title = {Data and modelling for the {Territorial} {Impact} {Assessment} ({TIA}) of policies},
	booktitle = {Handbook of {Computational} {Social} {Science} for {Policy}},
	publisher = {Springer},
	author = {Medeiros, Eduardo},
	year = {2022},
}

@article{lazer_computational_2009,
	title = {Computational {Social} {Science}},
	volume = {323},
	issn = {0036-8075, 1095-9203},
	url = {https://www.science.org/doi/10.1126/science.1167742},
	doi = {10.1126/science.1167742},
	language = {en},
	number = {5915},
	urldate = {2022-01-31},
	journal = {Science},
	author = {Lazer, David and Pentland, Alex and Adamic, Lada and Aral, Sinan and Barabási, Albert-László and Brewer, Devon and Christakis, Nicholas and Contractor, Noshir and Fowler, James and Gutmann, Myron and Jebara, Tony and King, Gary and Macy, Michael and Roy, Deb and Van Alstyne, Marshall},
	month = feb,
	year = {2009},
	pages = {721--723},
}

@article{lazer_data_2017,
	title = {Data ex {Machina}: {Introduction} to {Big} {Data}},
	volume = {43},
	issn = {0360-0572, 1545-2115},
	shorttitle = {Data ex {Machina}},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-soc-060116-053457},
	doi = {10.1146/annurev-soc-060116-053457},
	abstract = {Social life increasingly occurs in digital environments and continues to be mediated by digital systems. Big data represents the data being generated by the digitization of social life, which we break down into three domains: digital life, digital traces, and digitalized life. We argue that there is enormous potential in using big data to study a variety of phenomena that remain difficult to observe. However, there are some recurring vulnerabilities that should be addressed. We also outline the role institutions must play in clarifying the ethical rules of the road. Finally, we conclude by pointing to a number of nascent but important trends in the use of big data.},
	language = {en},
	number = {1},
	urldate = {2022-07-07},
	journal = {Annual Review of Sociology},
	author = {Lazer, David and Radford, Jason},
	month = jul,
	year = {2017},
	pages = {19--39},
}

@article{calissano_conformal_2021,
	title = {Conformal {Prediction} {Sets} for {Populations} of {Graphs}},
	volume = {42/2021},
	copyright = {All rights reserved},
	journal = {MOX-Report},
	author = {Calissano, Anna and Fontana, Matteo and Zeni, Gianluca and Vantini, Simone},
	year = {2021},
}

@article{torti_modelling_2021,
	title = {Modelling time-varying mobility flows using function-on-function regression: {Analysis} of a bike sharing system in the city of {Milan}},
	volume = {70},
	copyright = {© 2020 Royal Statistical Society},
	issn = {1467-9876},
	shorttitle = {Modelling time-varying mobility flows using function-on-function regression},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12456},
	doi = {10.1111/rssc.12456},
	abstract = {In today's world, bike sharing systems are becoming increasingly common in all main cities around the world. To understand the spatiotemporal patterns of how people move by bike through the city of Milan, we apply functional data analysis to study the flows of a bike sharing mobility network. We introduce a complete pipeline to properly analyse and model functional data through a concurrent functional-on-functional model taking into account the effects of weather conditions and calendar on the bike flows. In the end, we develop an interactive interface to explore the results of the analyses.},
	language = {en},
	number = {1},
	urldate = {2021-06-27},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Torti, Agostino and Pini, Alessia and Vantini, Simone},
	year = {2021},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssc.12456},
	keywords = {bike sharing, functional data, functional regression, mobility, network, time evolving graph},
	pages = {226--247},
}

@article{shah_forecasting_2020,
	title = {Forecasting of electricity price through a functional prediction of sale and purchase curves},
	volume = {39},
	copyright = {© 2019 John Wiley \& Sons, Ltd.},
	issn = {1099-131X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/for.2624},
	doi = {10.1002/for.2624},
	abstract = {This work proposes a new approach for the prediction of the electricity price based on forecasting aggregated purchase and sale curves. The basic idea is to model the hourly purchase and the sale curves, to predict them and to find the intersection of the predicted curves in order to obtain the predicted equilibrium market price and volume. Modeling and forecasting of purchase and sale curves is performed by means of functional data analysis methods. More specifically, parametric (FAR) and nonparametric (NPFAR) functional autoregressive models are considered and compared to some benchmarks. An appealing feature of the functional approach is that, unlike other methods, it provides insights into the sale and purchase mechanism connected with the price and demand formation process and can therefore be used for the optimization of bidding strategies. An application to the Italian electricity market (IPEX) is also provided, showing that NPFAR models lead to a statistically significant improvement in the forecasting accuracy.},
	language = {en},
	number = {2},
	urldate = {2021-06-26},
	journal = {Journal of Forecasting},
	author = {Shah, Ismail and Lisi, Francesco},
	year = {2020},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/for.2624},
	keywords = {Italian electricity market, electricity prices, forecasting, functional data, sale and purchase curves},
	pages = {242--259},
}

@article{rossini_quantifying_2019,
	series = {Special {Issue} on {Functional} {Data} {Analysis} and {Related} {Topics}},
	title = {Quantifying prediction uncertainty for functional-and-scalar to functional autoregressive models under shape constraints},
	volume = {170},
	issn = {0047-259X},
	url = {https://www.sciencedirect.com/science/article/pii/S0047259X17307327},
	doi = {10.1016/j.jmva.2018.10.007},
	abstract = {Motivated by demand and supply curve forecasting in energy markets, we discuss an autoregressive functional modeling framework that preserves curve constraints, includes exogenous scalar information, and provides prediction uncertainty quantification. The model is a functional autoregressive model that relies on a non-concurrent functional autoregressive model in a non-standard pre-Hilbert space in order to satisfy the curve constraints. Prediction uncertainty is quantified by means of a novel bootstrap approach for dependent functional data where the predictive bootstrap trajectories are represented alongside the prediction to show how forecasting confidence varies in the domain. Computational and numerical details are discussed in order to replicate the model estimation process an adequate number of times during the bootstrap phase. The method is applied to Italian natural gas market data.},
	language = {en},
	urldate = {2021-06-26},
	journal = {Journal of Multivariate Analysis},
	author = {Rossini, Jacopo and Canale, Antonio},
	month = mar,
	year = {2019},
	keywords = {Demand and offer model, Functional bootstrap, Functional ridge regression},
	pages = {221--231},
}

@article{canale_constrained_2016,
	title = {Constrained functional time series: {Applications} to the {Italian} gas market},
	volume = {32},
	issn = {0169-2070},
	shorttitle = {Constrained functional time series},
	url = {https://www.sciencedirect.com/science/article/pii/S0169207016300620},
	doi = {10.1016/j.ijforecast.2016.05.002},
	abstract = {Motivated by market dynamic modelling in the Italian Natural Gas Balancing Platform, we propose a model for analyzing time series of functions, subject to equality and inequality constraints at the two edges of the domain, respectively, such as daily demand and offer curves. Specifically, we provide the constrained functions with suitable pre-Hilbert structures, and introduce a useful isometric bijective map that associates each possible bounded and monotonic function to an unconstrained one. We introduce a functional-to-functional autoregressive model that is used to forecast future demand/offer functions, and estimate the model via the minimization of a penalized mean squared error of prediction, with a penalty term based on the Hilbert–Schmidt squared norm of autoregressive lagged operators. The approach is of general interest and could be generalized to any situation in which one has to deal with functions that are subject to the above constraints which evolve over time.},
	language = {en},
	number = {4},
	urldate = {2021-06-26},
	journal = {International Journal of Forecasting},
	author = {Canale, Antonio and Vantini, Simone},
	month = oct,
	year = {2016},
	keywords = {Autoregressive model, Demand and offer model, Energy forecasting, Functional data analysis, Functional ridge regression},
	pages = {1340--1351},
}

@article{gao_multivariate_2017,
	title = {Multivariate {Functional} {Time} {Series} {Forecasting}: {Application} to {Age}-{Specific} {Mortality} {Rates}},
	volume = {5},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	shorttitle = {Multivariate {Functional} {Time} {Series} {Forecasting}},
	url = {https://www.mdpi.com/2227-9091/5/2/21},
	doi = {10.3390/risks5020021},
	abstract = {This study considers the forecasting of mortality rates in multiple populations. We propose a model that combines mortality forecasting and functional data analysis (FDA). Under the FDA framework, the mortality curve of each year is assumed to be a smooth function of age. As with most of the functional time series forecasting models, we rely on functional principal component analysis (FPCA) for dimension reduction and further choose a vector error correction model (VECM) to jointly forecast mortality rates in multiple populations. This model incorporates the merits of existing models in that it excludes some of the inherent randomness with the nonparametric smoothing from FDA, and also utilizes the correlation structures between the populations with the use of VECM in mortality models. A nonparametric bootstrap method is also introduced to construct interval forecasts. The usefulness of this model is demonstrated through a series of simulation studies and applications to the age-and sex-specific mortality rates in Switzerland and the Czech Republic. The point forecast errors of several forecasting methods are compared and interval scores are used to evaluate and compare the interval forecasts. Our model provides improved forecast accuracy in most cases.},
	language = {en},
	number = {2},
	urldate = {2021-06-26},
	journal = {Risks},
	author = {Gao, Yuan and Shang, Han Lin},
	month = jun,
	year = {2017},
	note = {Number: 2
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {age-and sex-specific mortality rate, bootstrapping prediction interval, interval score, vector autoregressive model, vector error correction model},
	pages = {21},
}

@article{hyndman_forecasting_2009,
	title = {Forecasting functional time series},
	volume = {38},
	issn = {1226-3192},
	url = {https://www.sciencedirect.com/science/article/pii/S1226319209000398},
	doi = {10.1016/j.jkss.2009.06.002},
	abstract = {We propose forecasting functional time series using weighted functional principal component regression and weighted functional partial least squares regression. These approaches allow for smooth functions, assign higher weights to more recent data, and provide a modeling scheme that is easily adapted to allow for constraints and other information. We illustrate our approaches using age-specific French female mortality rates from 1816 to 2006 and age-specific Australian fertility rates from 1921 to 2006, and show that these weighted methods improve forecast accuracy in comparison to their unweighted counterparts. We also propose two new bootstrap methods to construct prediction intervals, and evaluate and compare their empirical coverage probabilities.},
	language = {en},
	number = {3},
	urldate = {2021-06-26},
	journal = {Journal of the Korean Statistical Society},
	author = {Hyndman, Rob J. and Shang, Han Lin},
	month = sep,
	year = {2009},
	keywords = {Demographic forecasting, Functional data, Functional partial least squares, Functional principal components, Functional time series},
	pages = {199--211},
}

@article{ferraty_nonparametric_2004,
	title = {Nonparametric models for functional data, with application in regression, time series prediction and curve discrimination},
	volume = {16},
	issn = {1048-5252},
	url = {https://doi.org/10.1080/10485250310001622686},
	doi = {10.1080/10485250310001622686},
	abstract = {The aim of this article is to investigate a new approach for estimating a regression model with scalar response and in which the explanatory variable is valued in some abstract semi-metric functional space. Nonparametric estimates are introduced, and their behaviors are investigated in the situation of dependent data. Our study contains asymptotic results with rates. The curse of dimensionality, which is of great importance in this infinite dimensional setting, is highlighted by our asymptotic results. Some ideas, based on fractal dimension modelizations, are given to reduce dimensionality of the problem. Generalization of the model leads to possible applications in several fields of applied statistics, and we present three applications among these namely: regression estimation, time-series prediction, and curve discrimination. As a by-product of our approach in the finite-dimensional context, we give a new proof for the rates of convergence of some Nadaraya–Watson kernel-type smoother without needing any smoothness assumption on the density function of the explanatory variables.},
	number = {1-2},
	urldate = {2021-06-26},
	journal = {Journal of Nonparametric Statistics},
	author = {Ferraty, F. and Vieu, P.},
	month = feb,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10485250310001622686},
	keywords = {Curves discrimination, Fractal dimension, Functional data, Nonparametric regression, Semi-metric space, Time-series prediction},
	pages = {111--125},
}

@article{ferraty_functional_2002,
	title = {Functional nonparametric model for time series: a fractal approach for dimension reduction},
	volume = {11},
	issn = {1863-8260},
	shorttitle = {Functional nonparametric model for time series},
	url = {https://doi.org/10.1007/BF02595710},
	doi = {10.1007/BF02595710},
	abstract = {In this paper we propose a functional nonparametric model for time series prediction. The originality of this model consists in using as predictor a continuous set of past values. This time series problem is presented in the general framework of regression estimation from dependent samples with regressor valued in some infinite dimensional semi-normed vectorial space. The curse of dimensionality induced by our approach is overridden by means of fractal dimension considerations. We give asymptotics for a kernel type nonparametric predictor linking the rates of convergence with the fractal dimension of the functional process. Finally, our method has been implemented and applied to some electricity consumption data.},
	language = {en},
	number = {2},
	urldate = {2021-06-26},
	journal = {Test},
	author = {Ferraty, Frédéric and Goia, Aldo and Vieu, Philippe},
	month = dec,
	year = {2002},
	pages = {317--344},
}

@misc{noauthor_full_nodate,
	title = {Full article: {Functional} time series approach for forecasting very short-term electricity demand},
	url = {https://www.tandfonline.com/doi/full/10.1080/02664763.2012.740619},
	urldate = {2021-06-26},
}

@book{ferraty_nonparametric_2006,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Nonparametric {Functional} {Data} {Analysis}: {Theory} and {Practice}},
	isbn = {978-0-387-30369-7},
	shorttitle = {Nonparametric {Functional} {Data} {Analysis}},
	url = {https://www.springer.com/gp/book/9780387303697},
	abstract = {Modern apparatuses allow us to collect samples of functional data, mainly curves but also images. On the other hand, nonparametric statistics produces useful tools for standard data exploration. This book links these two fields of modern statistics by explaining how functional data can be studied through parameter-free statistical ideas. This book starts from theoretical foundations including functional nonparametric modeling, description of the mathematical framework, construction of the statistical methods, and statements of their asymptotic behaviors. It proceeds to computational issues including R and S-PLUS routines. Several functional datasets in chemometrics, econometrics, and pattern recognition are used to emphasize the wide scope of nonparametric functional data analysis in applied sciences. The companion Web site includes R and S-PLUS routines, command lines for reproducing examples presented in the book, and the functional datasets. Rather than set application against theory, this book is really an interface of these two features of statistics. A special effort has been made in writing this book to accommodate several levels of reading. The computational aspects are oriented toward practitioners whereas open problems emerging from this new field of statistics will attract Ph.D. students and academic researchers. Finally, this book is also accessible to graduate students starting in the area of functional statistics. Frédéric Ferraty and Philippe Vieu are both researchers in statistics at Toulouse University (France). They are co-founders and co-organizers of the working group STAPH which acquired an international reputation for functional and operatorial statistics. They are authors of many international publications in nonparametric inference as well as functional data analysis. Their scientific works are based on extensive collaborations both with academic statisticians and with scientists from other areas. They have been invited to organize special sessions on functional data in recent international conferences and to teach Ph.D. courses in various countries.},
	language = {en},
	urldate = {2021-06-26},
	publisher = {Springer-Verlag},
	author = {Ferraty, Frédéric and Vieu, Philippe},
	year = {2006},
	doi = {10.1007/0-387-36620-2},
}

@article{chen_review_2021,
	title = {A review study of functional autoregressive models with application to energy forecasting},
	volume = {13},
	copyright = {© 2020 Wiley Periodicals LLC.},
	issn = {1939-0068},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/wics.1525},
	doi = {10.1002/wics.1525},
	abstract = {In this data-rich era, it is essential to develop advanced techniques to analyze and understand large amounts of data and extract the underlying information in a flexible way. We provide a review study on the state-of-the-art statistical time series models for univariate and multivariate functional data with serial dependence. In particular, we review functional autoregressive (FAR) models and their variations under different scenarios. The models include the classic FAR model under stationarity; the FARX and pFAR model dealing with multiple exogenous functional variables and large-scale mixed-type exogenous variables; the vector FAR model and common functional principal component technique to handle multiple dimensional functional time series; and the warping FAR, varying coefficient-FAR and adaptive FAR models to handle seasonal variations, slow varying effects and the more challenging cases of structural changes or breaks respectively. We present the models’ setup and detail the estimation procedure. We discuss the models’ applicability and illustrate the numerical performance using real-world data of high-resolution natural gas flows in the high-pressure gas pipeline network of Germany. We conduct 1-day and 14-days-ahead out-of-sample forecasts of the daily gas flow curves. We observe that the functional time series models generally produce stable out-of-sample forecast accuracy. This article is categorized under: Statistical Models {\textgreater} Semiparametric Models Data: Types and Structure {\textgreater} Time Series, Stochastic Processes, and Functional Data},
	language = {en},
	number = {3},
	urldate = {2021-06-26},
	journal = {WIREs Computational Statistics},
	author = {Chen, Ying and Koch, Thorsten and Lim, Kian Guan and Xu, Xiaofei and Zakiyeva, Nazgul},
	year = {2021},
	note = {\_eprint: https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/wics.1525},
	keywords = {energy forecast, functional autoregressive modeling, functional time series, sieve estimation},
	pages = {e1525},
}

@article{hernandez_simultaneous_2021,
	title = {Simultaneous predictive bands for functional time series using minimum entropy sets},
	url = {http://arxiv.org/abs/2105.13627},
	abstract = {Functional Time Series are sequences of dependent random elements taking values on some functional space. Most of the research on this domain is focused on producing a predictor able to forecast the value of the next function having observed a part of the sequence. For this, the Autoregresive Hilbertian process is a suitable framework. We address here the problem of constructing simultaneous predictive conﬁdence bands for a stationary functional time series. The method is based on an entropy measure for stochastic processes, in particular functional time series. To construct predictive bands we use a functional bootstrap procedure that allow us to estimate the prediction law through the use of pseudo-predictions. Each pseudo-realisation is then projected into a space of ﬁnite dimension, associated to a functional basis. We use Reproducing Kernel Hilbert Spaces (RKHS) to represent the functions, considering then the basis associated to the reproducing kernel. Using a simple decision rule, we classify the points on the projected space among those belonging to the minimum entropy set and those that do not. We push back the minimum entropy set to the functional space and construct a band using the regularity property of the RKHS. The proposed methodology is illustrated through artiﬁcial and real-world data sets.},
	language = {en},
	urldate = {2021-06-21},
	journal = {arXiv:2105.13627 [math, stat]},
	author = {Hernández, Nicolás and Cugliari, Jairo and Jacques, Julien},
	month = jun,
	year = {2021},
	note = {arXiv: 2105.13627},
	keywords = {Mathematics - Functional Analysis, Statistics - Methodology},
}

@article{rahimi_review_2021,
	title = {A review on {COVID}-19 forecasting models},
	issn = {1433-3058},
	url = {https://doi.org/10.1007/s00521-020-05626-8},
	doi = {10.1007/s00521-020-05626-8},
	abstract = {The novel coronavirus (COVID-19) has spread to more than 200 countries worldwide, leading to more than 36 million confirmed cases as of October 10, 2020. As such, several machine learning models that can forecast the outbreak globally have been released. This work presents a review and brief analysis of the most important machine learning forecasting models against COVID-19. The work presented in this study possesses two parts. In the first section, a detailed scientometric analysis presents an influential tool for bibliometric analyses, which were performed on COVID-19 data from the Scopus and Web of Science databases. For the above-mentioned analysis, keywords and subject areas are addressed, while the classification of machine learning forecasting models, criteria evaluation, and comparison of solution approaches are discussed in the second section of the work. The conclusion and discussion are provided as the final sections of this study.},
	language = {en},
	urldate = {2021-04-02},
	journal = {Neural Computing and Applications},
	author = {Rahimi, Iman and Chen, Fang and Gandomi, Amir H.},
	month = feb,
	year = {2021},
}

@article{miller_jackknife--review_1974,
	title = {The {Jackknife}--{A} {Review}},
	volume = {61},
	issn = {0006-3444},
	url = {https://www.jstor.org/stable/2334280},
	doi = {10.2307/2334280},
	abstract = {Research on the jackknife technique since its introduction by Quenouille and Tukey is reviewed. Both its role in bias reduction and in robust interval estimation are treated. Some speculations and suggestions about future research are made. The bibliography attempts to include all published work on jackknife methodology.},
	number = {1},
	urldate = {2020-11-10},
	journal = {Biometrika},
	author = {Miller, Rupert G.},
	year = {1974},
	note = {Publisher: [Oxford University Press, Biometrika Trust]},
	pages = {1--15},
}

@article{jain_structure_2009,
	title = {Structure {Spaces}},
	volume = {10},
	issn = {1533-7928},
	url = {http://jmlr.org/papers/v10/jain09a.html},
	number = {93},
	urldate = {2020-10-15},
	journal = {Journal of Machine Learning Research},
	author = {Jain, Brijnesh J. and Obermayer, Klaus},
	year = {2009},
	pages = {2667--2714},
}

@article{jain_geometry_2016,
	title = {On the geometry of graph spaces},
	volume = {214},
	issn = {0166-218X},
	url = {http://www.sciencedirect.com/science/article/pii/S0166218X16303067},
	doi = {10.1016/j.dam.2016.06.027},
	abstract = {Optimal alignment kernels are graph similarity functions defined as pointwise maximizers of a set of positive-definite kernels. Due to the max-operation, optimal alignment kernels are indefinite graph kernels. This contribution studies how the max-operation transforms the geometry of the associated feature space and how standard pattern recognition methods such as linear classifiers can be extended to those transformed spaces. The main result is the Graph Representation Theorem stating that a graph is a point in some geometric space, called orbit space. This result shows that the max-operation transforms the feature space to a quotient by a group action. Orbit spaces are well investigated and easier to explore than the original graph space. We derive a number of geometric results, translate them to graph spaces, and show how the proposed results can be applied to statistical pattern recognition.},
	language = {en},
	urldate = {2020-10-15},
	journal = {Discrete Applied Mathematics},
	author = {Jain, Brijnesh J.},
	month = dec,
	year = {2016},
	keywords = {Graph metric, Graphs, Orbit space, Pattern recognition},
	pages = {126--144},
}

@inproceedings{cabassi_three_2018,
	address = {Cham},
	series = {Springer {Proceedings} in {Mathematics} \& {Statistics}},
	title = {Three {Testing} {Perspectives} on {Connectome} {Data}},
	copyright = {All rights reserved},
	isbn = {978-3-030-00039-4},
	doi = {10.1007/978-3-030-00039-4_3},
	abstract = {Guided by an application in the analysis of Magnetic Resonance Imaging (MRI) scans from the neuroimaging realm, we provide some perspectives on statistical techniques that are able to address issues commonly encountered when dealing with Magnetic Resonance images. The first section of the chapter is devoted to a boostrap-based inferential tool to test for correlation between anatomy and functional activity. The second provides a Bayesian framework to improve estimation of fiber counts from Diffusion Tensor Imaging (DTI) scans. The third one introduces an object-oriented framework to explore and perform testing over network-valued data.},
	language = {en},
	booktitle = {Studies in {Neural} {Data} {Science}},
	publisher = {Springer International Publishing},
	author = {Cabassi, Alessandra and Casa, Alessandro and Fontana, Matteo and Russo, Massimiliano and Farcomeni, Alessio},
	editor = {Canale, Antonio and Durante, Daniele and Paci, Lucia and Scarpa, Bruno},
	year = {2018},
	keywords = {Bayesian statistics, Bootstrap inference, Graphical lasso, Hypothesis testing, Object oriented data analysis, Permutation tests},
	pages = {37--55},
}

@article{wang_object_2007,
	title = {Object oriented data analysis: {Sets} of trees},
	volume = {35},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Object oriented data analysis},
	url = {https://projecteuclid.org/euclid.aos/1194461714},
	doi = {10.1214/009053607000000217},
	abstract = {Object oriented data analysis is the statistical analysis of populations of complex objects. In the special case of functional data analysis, these data objects are curves, where standard Euclidean approaches, such as principal component analysis, have been very successful. Recent developments in medical image analysis motivate the statistical analysis of populations of more complex data objects which are elements of mildly non-Euclidean spaces, such as Lie groups and symmetric spaces, or of strongly non-Euclidean spaces, such as spaces of tree-structured data objects. These new contexts for object oriented data analysis create several potentially large new interfaces between mathematics and statistics. This point is illustrated through the careful development of a novel mathematical framework for statistical analysis of populations of tree-structured objects.},
	language = {EN},
	number = {5},
	urldate = {2020-10-14},
	journal = {Annals of Statistics},
	author = {Wang, Haonan and Marron, J. S.},
	month = oct,
	year = {2007},
	mrnumber = {MR2363955},
	zmnumber = {1126.62002},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {Functional data analysis, nonlinear data space, object oriented data analysis, population of tree-structured objects, principal component analysis},
	pages = {1849--1873},
}

@article{marron_overview_2014,
	title = {Overview of object oriented data analysis},
	volume = {56},
	copyright = {© 2014 WILEY‐VCH Verlag GmbH \& Co. KGaA, Weinheim},
	issn = {1521-4036},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bimj.201300072},
	doi = {10.1002/bimj.201300072},
	abstract = {Object oriented data analysis is the statistical analysis of populations of complex objects. In the special case of functional data analysis, these data objects are curves, where a variety of Euclidean approaches, such as principal components analysis, have been very successful. Challenges in modern medical image analysis motivate the statistical analysis of populations of more complex data objects that are elements of mildly non-Euclidean spaces, such as lie groups and symmetric spaces, or of strongly non-Euclidean spaces, such as spaces of tree-structured data objects. These new contexts for object oriented data analysis create several potentially large new interfaces between mathematics and statistics. The notion of object oriented data analysis also impacts data analysis, through providing a framework for discussion of the many choices needed in many modern complex data analyses, especially in interdisciplinary contexts.},
	language = {en},
	number = {5},
	urldate = {2020-10-14},
	journal = {Biometrical Journal},
	author = {Marron, J. Steve and Alonso, Andrés M.},
	year = {2014},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/bimj.201300072},
	keywords = {Data objects, Functional data analysis, Non-Euclidean, Principal components},
	pages = {732--753},
}

@book{ramsay_functional_2005,
	series = {Springer series in statistics},
	title = {Functional data analysis},
	isbn = {978-0-387-40080-8},
	language = {en},
	publisher = {Springer},
	author = {Ramsay, James O. and Silverman, Bernard W.},
	year = {2005},
	note = {OCLC: 249216329},
}

@book{schwab_fourth_2016,
	address = {New York},
	title = {The {Fourth} {Industrial} {Revolution}},
	isbn = {978-1-5247-5886-8},
	publisher = {Crown Business},
	author = {Schwab, Klaus},
	year = {2016},
	keywords = {Economic aspects, Social aspects, Technological innovations, Technology and civilization},
}

@phdthesis{laxhammar_conformal_2014,
	type = {{PhD} {Thesis}},
	title = {Conformal {Anomaly} {Detection}},
	school = {University of Skövde},
	author = {Laxhammar, Rikard},
	year = {2014},
}

@article{centofanti_functional_2020,
	title = {Functional {Regression} {Control} {Chart}},
	volume = {0},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2020.1753581},
	doi = {10.1080/00401706.2020.1753581},
	abstract = {The modern development of data acquisition technologies in many industrial processes is facilitating the collection of quality characteristics that are apt to be modeled as functions, which are usually referred to as profiles. At the same time, measurements of concurrent variables, which are related to the quality characteristic profiles, are often available in a functional form as well, and usually referred to as covariates. To adjust the monitoring of the quality characteristic profiles by the effect of this additional information, a new functional control chart is elaborated on the residuals obtained from a function-on-function linear regression of the quality characteristic profile on the functional covariates. By means of a Monte Carlo simulation study, the proposed control chart is compared with other control charts already appeared in the literature and some remarks are given on its use in presence of covariate mean shifts. Furthermore, a real-case study in the shipping industry is presented with the purpose of monitoring ship fuel consumption and thus, CO2 emissions from a Ro-Pax ship, with particular regard to detecting their reduction after a specific energy efficiency initiative.},
	number = {0},
	urldate = {2020-10-06},
	journal = {Technometrics},
	author = {Centofanti, Fabio and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
	month = apr,
	year = {2020},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2020.1753581},
	keywords = {Functional data analysis, Multivariate functional linear regression, Profile monitoring, Statistical process control},
	pages = {1--14},
}

@article{lopez-pintado_concept_2009,
	title = {On the {Concept} of {Depth} for {Functional} {Data}},
	volume = {104},
	issn = {0162-1459},
	url = {https://www.jstor.org/stable/40592217},
	abstract = {[The statistical analysis of functional data is a growing need in many research areas. In particular, a robust methodology is important to study curves, which are the output of many experiments in applied statistics. As a starting point for this robust analysis, we propose, analyze, and apply a new definition of depth for functional observations based on the graphic representation of the curves. Given a collection of functions, it establishes the "centrality" of an observation and provides a natural center-outward ordering of the sample curves. Robust statistics, such as the median function or a trimmed mean function, can be defined from this depth definition. Its finite-dimensional version provides a new depth for multivariate data that is computationally feasible and useful for studying high-dimensional observations. Thus, this new depth is also suitable for complex observations such as microarray data, images, and those arising in some recent marketing and financial studies. Natural properties of these new concepts are established and the uniform consistency of the sample depth is proved. Simulation results show that the corresponding depth based trimmed mean presents better performance than other possible location estimators proposed in the literature for some contaminated models. Data depth can be also used to screen for outliers. The ability of the new notions of depth to detect "shape" outliers is presented. Several real datasets are considered to illustrate this new concept of depth, including applications to microarray observations, weather data, and growth curves. Finally, through this depth, we generalize to functions the Wilcoxon rank sum test. It allows testing whether two groups of curves come from the same population. This functional rank test when applied to children growth curves shows different growth patterns for boys and girls.]},
	number = {486},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Lopez-Pintado, Sara and Romo, Juan},
	year = {2009},
	pages = {718--734},
}

@incollection{boeing_commercial_airplanes_group_location_1998,
	title = {Location {Charts}},
	booktitle = {Advanced quality system tools, {AQS} {D1}-9000-1},
	author = {{Boeing Commercial Airplanes Group}},
	year = {1998},
	pages = {87--96},
}

@article{gertheiss_variable_2013,
	title = {Variable selection in generalized functional linear models},
	volume = {2},
	copyright = {Copyright © 2013 John Wiley \& Sons Ltd},
	issn = {2049-1573},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.20},
	doi = {10.1002/sta4.20},
	abstract = {Modern research data, where a large number of functional predictors is collected on few subjects are becoming increasingly common. In this paper we propose a variable selection technique, when the predictors are functional and the response is scalar. Our approach is based on adopting a generalized functional linear model framework and using a penalized likelihood method that simultaneously controls the sparsity of the model and the smoothness of the corresponding coefficient functions by adequate penalization. The methodology is characterized by high predictive accuracy, and yields interpretable models, while retaining computational efficiency. The proposed method is investigated numerically in finite samples, and applied to a diffusion tensor imaging tractography data set and a chemometric data set. Copyright © 2013 John Wiley \& Sons Ltd},
	language = {en},
	number = {1},
	urldate = {2020-09-30},
	journal = {Stat},
	author = {Gertheiss, Jan and Maity, Arnab and Staicu, Ana-Maria},
	year = {2013},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.20},
	keywords = {group lasso, multiple functional predictors, penalized estimation, variable selection},
	pages = {86--101},
}

@article{bro_exploratory_1999,
	title = {Exploratory study of sugar production using fluorescence spectroscopy and multi-way analysis},
	volume = {46},
	issn = {0169-7439},
	url = {http://www.sciencedirect.com/science/article/pii/S0169743998001816},
	doi = {10.1016/S0169-7439(98)00181-6},
	abstract = {This paper is concerned with the possibility of obtaining chemically meaningful models of complicated processes by the use of fluorescence spectroscopy screening and the unique parallel factor analysis (PARAFAC) model. The second-order nature of fluorescence excitation emission data and the fact that the PARAFAC model has no rotational indeterminacy mean that in certain cases, it is possible to decompose complex mixture signals into contributions from individual chemical components. Relating the thus obtained information to, e.g., important quality parameters, it is possible to analyze, understand, predict and monitor the quality based on a chemical foundation. The proposed approach thus gives a direct link between process analytical chemistry and multivariate statistical process control.},
	language = {en},
	number = {2},
	urldate = {2020-09-30},
	journal = {Chemometrics and Intelligent Laboratory Systems},
	author = {Bro, Rasmus},
	month = mar,
	year = {1999},
	keywords = {MSPC, Multilinear PLS, N-PLS, Non-negativity, PAC, PARAFAC, Unimodality, Uniqueness},
	pages = {133--147},
}

@article{wilson_probable_1927,
	title = {Probable {Inference}, the {Law} of {Succession}, and {Statistical} {Inference}},
	volume = {22},
	issn = {0162-1459},
	url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1927.10502953},
	doi = {10.1080/01621459.1927.10502953},
	number = {158},
	urldate = {2020-09-24},
	journal = {Journal of the American Statistical Association},
	author = {Wilson, Edwin B.},
	month = jun,
	year = {1927},
	note = {Publisher: Taylor \& Francis
\_eprint: https://www.tandfonline.com/doi/pdf/10.1080/01621459.1927.10502953},
	pages = {209--212},
}

@article{menafoglio_profile_2018,
	title = {Profile {Monitoring} of {Probability} {Density} {Functions} via {Simplicial} {Functional} {PCA} {With} {Application} to {Image} {Data}},
	volume = {60},
	issn = {0040-1706},
	url = {https://doi.org/10.1080/00401706.2018.1437473},
	doi = {10.1080/00401706.2018.1437473},
	abstract = {The advance of sensor and information technologies is leading to data-rich industrial environments, where large amounts of data are potentially available. This study focuses on industrial applications where image data are used more and more for quality inspection and statistical process monitoring. In many cases of interest, acquired images consist of several and similar features that are randomly distributed within a given region. Examples are pores in parts obtained via casting or additive manufacturing, voids in metal foams and light-weight components, grains in metallographic analysis, etc. The proposed approach summarizes the random occurrences of the observed features via their (empirical) probability density functions (PDFs). In particular, a novel approach for PDF monitoring is proposed. It is based on simplicial functional principal component analysis (SFPCA), which is performed within the space of density functions, that is, the Bayes space B2. A simulation study shows the enhanced monitoring performances provided by SFPCA-based profile monitoring against other competitors proposed in the literature. Finally, a real case study dealing with the quality control of foamed material production is discussed, to highlight a practical use of the proposed methodology. Supplementary materials for the article are available online.},
	number = {4},
	urldate = {2020-09-22},
	journal = {Technometrics},
	author = {Menafoglio, Alessandra and Grasso, Marco and Secchi, Piercesare and Colosimo, Bianca Maria},
	month = oct,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00401706.2018.1437473},
	keywords = {Bayes space, Constrained curves, Functional data analysis, Image-based process monitoring, Statistical process control},
	pages = {497--510},
}

@article{grasso_using_2016,
	title = {Using {Curve}-{Registration} {Information} for {Profile} {Monitoring}},
	volume = {48},
	issn = {0022-4065},
	url = {https://doi.org/10.1080/00224065.2016.11918154},
	doi = {10.1080/00224065.2016.11918154},
	abstract = {The quality characteristics in manufacturing processes are often represented in terms of spatially- or time-ordered data, called “profiles”, which are characterized by amplitude and phase variability. In this context, curve registration plays a key role, as it allows separating the two kinds of between-profiles variability and reducing any undesired inflation of the natural phase variability. In the mainstream literature, warping functions are not generally considered in the monitoring process, even though this may cause significant information loss. We propose a novel approach for profile monitoring that combines the functional principal component analysis and the use of parametric warping functions. The key idea is to jointly monitor the stability over time of the registered profiles (i.e., the information related to amplitude variability) and the registration coefficients (i.e., the information related to phase variability). This allows improving the capability of detecting unnatural pattern modifications, thanks to a better characterization of the overall natural variability. The benefits of a proper management of functional data registration, together with the advantages over the most common approaches used in the literature, are demonstrated by means of Monte Carlo simulations. The proposed methodology is finally applied to a real industrial case study relying on a dataset acquired in waterjet cutting processes under different health conditions of the machine tool.},
	number = {2},
	urldate = {2020-09-22},
	journal = {Journal of Quality Technology},
	author = {Grasso, Marco and Menafoglio, Alessandra and Colosimo, Bianca M. and Secchi, Piercesare},
	month = apr,
	year = {2016},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00224065.2016.11918154},
	keywords = {Curve Registration, Functional-Data Analysis, Profile Monitoring, Warping Functions},
	pages = {99--127},
}

@misc{noauthor_full_nodate,
	title = {Full article: {Profile} {Monitoring} of {Probability} {Density} {Functions} via {Simplicial} {Functional} {PCA} {With} {Application} to {Image} {Data}},
	url = {https://www.tandfonline.com/doi/full/10.1080/00401706.2018.1437473},
	urldate = {2020-09-22},
}

@article{woodall_using_2004,
	title = {Using {Control} {Charts} to {Monitor} {Process} and {Product} {Quality} {Profiles}},
	volume = {36},
	issn = {0022-4065},
	url = {https://doi.org/10.1080/00224065.2004.11980276},
	doi = {10.1080/00224065.2004.11980276},
	abstract = {In most statistical process control (SPC) applications, it is assumed that the quality of a process or product can be adequately represented by the distribution of a univariate quality characteristic or by the general multivariate distribution of a vector consisting of several correlated quality characteristics. In many practical situations, however, the quality of a process or product is better characterized and summarized by a relationship between a response variable and one or more explanatory variables. Thus, at each sampling stage, one observes a collection of data points that can be represented by a curve (or profile). In some calibration applications, the profile can be represented adequately by a simple straight-line model, while in other applications, more complicated models are needed. In this expository paper, we discuss some of the general issues involved in using control charts to monitor such process- and product-quality profiles and review the SPC literature on the topic. We relate this application to functional data analysis and review applications involving linear profiles, nonlinear profiles, and the use of splines and wavelets. We strongly encourage research in profile monitoring and provide some research ideas.},
	number = {3},
	urldate = {2020-09-22},
	journal = {Journal of Quality Technology},
	author = {Woodall, William H. and Spitzner, Dan J. and Montgomery, Douglas C. and Gupta, Shilpa},
	month = jul,
	year = {2004},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00224065.2004.11980276},
	keywords = {Calibration, Linear Regression, Multivariate Quality Control, Nonlinear Regression, Splines, Statistical Process Control, Wavelets},
	pages = {309--320},
}

@article{colosimo_comparison_2010,
	title = {A comparison study of control charts for statistical monitoring of functional data},
	volume = {48},
	issn = {0020-7543},
	url = {https://doi.org/10.1080/00207540802662888},
	doi = {10.1080/00207540802662888},
	abstract = {The quality of products and processes is more and more often becoming related to functional data, which refer to information summarised in the form of profiles. The recent literature has pointed out that traditional control charting methods cannot be directly applied in these cases and new approaches for profile monitoring are required. While many different profile monitoring approaches have been proposed in the scientific literature, few comparison studies are available. This paper aims at filling this gap by comparing three representative profile monitoring approaches in different production scenarios. The performance comparison will allow us to select a specific approach in a given situation. The competitor approaches are chosen to represent different levels of complexity, as well as different types of modelling approaches. In particular, at a lower level of complexity, the ‘location control chart’ (where the upper and lower control limits are ±K standard deviations from the sample mean at each profile location) is considered to be representative of industrial practice. At a higher complexity level, approaches based on combining a parametric model of functional data with multivariate and univariate control charting are considered. Within this second class, we analyse two different approaches. The first is based on regression and the second focuses on using principal component analysis for modelling functional data. A manufacturing reference case study is used throughout the paper, namely profiles measured on machined items subject to geometrical specification (roundness).},
	number = {6},
	urldate = {2020-09-22},
	journal = {International Journal of Production Research},
	author = {Colosimo, Bianca M. and Pacella, Massimo},
	month = mar,
	year = {2010},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/00207540802662888},
	keywords = {PCA, control chart, functional data, profile monitoring, quality, regression, spatial statistic},
	pages = {1575--1601},
}

@article{colosimo_modeling_2018,
	title = {Modeling and monitoring methods for spatial and image data},
	volume = {30},
	issn = {0898-2112},
	url = {https://doi.org/10.1080/08982112.2017.1366512},
	doi = {10.1080/08982112.2017.1366512},
	abstract = {Intelligent sensing and computerized data analysis are inducing a paradigm shift in industrial statistics applied to discrete part manufacturing. Emerging technologies (e.g., additive manufacturing, micro-manufacturing) combined with new inspection solutions (e.g., non-contact systems, X-ray computer tomography) and fast multi-stream high-speed sensors (e.g., videos and images; acoustic, thermic, power and pressure signals) are paving the way for a new generation of industrial big-data requiring novel modeling and monitoring approaches for zero-defect manufacturing. Starting from real industrial problems, some of the main challenges to be faced in relevant industrial sectors are discussed. Viable solutions and future open issues are specifically outlined.},
	number = {1},
	urldate = {2020-09-22},
	journal = {Quality Engineering},
	author = {Colosimo, Bianca Maria},
	month = jan,
	year = {2018},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/08982112.2017.1366512},
	keywords = {additive manufacturing, functional data, images, profile monitoring, quality monitoring, shapes, signal, statistical process control, surfaces},
	pages = {94--111},
}

@article{balch_satellite_2019,
	title = {Satellite conjunction analysis and the false confidence theorem},
	volume = {475},
	url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2018.0565},
	doi = {10.1098/rspa.2018.0565},
	abstract = {Satellite conjunction analysis is the assessment of collision risk during a close encounter between a satellite and another object in orbit. A counterintuitive phenomenon has emerged in the conjunction analysis literature, namely, probability dilution, in which lower quality data paradoxically appear to reduce the risk of collision. We show that probability dilution is a symptom of a fundamental deficiency in probabilistic representations of statistical inference, in which there are propositions that will consistently be assigned a high degree of belief, regardless of whether or not they are true. We call this deficiency false confidence. In satellite conjunction analysis, it results in a severe and persistent underestimate of collision risk exposure. We introduce the Martin–Liu validity criterion as a benchmark by which to identify statistical methods that are free from false confidence. Such inferences will necessarily be non-probabilistic. In satellite conjunction analysis, we show that uncertainty ellipsoids satisfy the validity criterion. Performing collision avoidance manoeuvres based on ellipsoid overlap will ensure that collision risk is capped at the user-specified level. Furthermore, this investigation into satellite conjunction analysis provides a template for recognizing and resolving false confidence issues as they occur in other problems of statistical inference.},
	number = {2227},
	urldate = {2020-07-22},
	journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Balch, Michael Scott and Martin, Ryan and Ferson, Scott},
	month = jul,
	year = {2019},
	note = {Publisher: Royal Society},
	pages = {20180565},
}

@article{taraldsen_bayesian-fiducial-frequentist_2019,
	title = {Bayesian-{Fiducial}-{Frequentist} {Statistics} {Now}},
	url = {http://rgdoi.net/10.13140/RG.2.2.29428.81282},
	doi = {10.13140/RG.2.2.29428.81282},
	abstract = {This is a project description for a Researcher Project using the funding scheme for independent projects (FRIPRO) deﬁned by The Research Council of Norway. The proposal is based on national and international collaboration of established researchers, and having an additional recruitment component. This FRIPRO application concerns a small contribution covering essentially the recruitment component cost of 2 PhDs and 1 PostDoc, and some communication and engagement activities. Support to this international collaboration can, however, give a good platform for international mobility and later successful funding from grants from the European Research Council (ERC) or other international sources. The track record of the international project team ensures scientiﬁc quality at the forefront of international research.},
	language = {en},
	urldate = {2020-07-22},
	author = {Taraldsen, Gunnar},
	year = {2019},
	note = {Publisher: Unpublished},
}

@incollection{balakrishnan_lenths_2014,
	address = {Chichester, UK},
	title = {Lenth's {Method} for the {Analysis} of {Unreplicated} {Experiments}},
	isbn = {978-1-118-44511-2},
	url = {http://doi.wiley.com/10.1002/9781118445112.stat04086},
	language = {en},
	urldate = {2020-07-21},
	booktitle = {Wiley {StatsRef}: {Statistics} {Reference} {Online}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Lenth, Russell V.},
	editor = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
	month = sep,
	year = {2014},
	doi = {10.1002/9781118445112.stat04086},
	pages = {stat04086},
}

@inproceedings{ouyang_parallel_2018,
	title = {Parallel {Computation} of {Band} {Depth}},
	doi = {10.1109/CSE.2018.00018},
	abstract = {Band depth extends the concept of data depth to functional data. Consider a set of functions F. A pair of functions from F form a band, and a query function gets one count of containment if it is inside this band. When bands are formed with pairs of functions, the depth is called BD2. When bands are formed with triples of functions, the depth is called BD3. Band depth is useful in data analysis, but its computation takes a lot of time. Brute force methods enumerate all pairs or triples of functions to compute BD2 or BD3. In the literature, modified band depth is defined for its ease of computation, a resampling scheme is proposed to approximate band depth, and a faulty algorithm is designed for calculating BD2. We describe two novel algorithms for computing BD2. We also parallelize the calculation of BD2 and BD3 using OpenMP for CPU computation, and using CUDA for GPU computation. Using 32 CPU cores, BD2 of 10,000 functions can be calculated in 70 seconds. Using eight Nvidia V100 GPUs, BD3 of 10,000 functions can be calculated in 80 minutes, which would take 1,500 hours with 32 CPU cores.},
	booktitle = {2018 {IEEE} {International} {Conference} on {Computational} {Science} and {Engineering} ({CSE})},
	author = {Ouyang, Ming and Xu, Hanfei and Zhurkevich, Alexander},
	month = oct,
	year = {2018},
	keywords = {Approximation algorithms, BD, CPU computation, CUDA, Central Processing Unit, DNA, Force, GPU computation, Graphics processing units, Multicore processing, Ocean temperature, OpenMP, band depth, band depth, GPU, OpenMP, parallel computation, R, brute force methods, data analysis, data depth, faulty algorithm, graphics processing units, parallel architectures, parallel computation, query function, resampling scheme},
	pages = {85--90},
}

@article{sun_exact_2012,
	title = {Exact fast computation of band depth for large functional datasets: {How} quickly can one million curves be ranked?},
	volume = {1},
	copyright = {Copyright © 2012 John Wiley \& Sons, Ltd.},
	issn = {2049-1573},
	shorttitle = {Exact fast computation of band depth for large functional datasets},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.8},
	doi = {10.1002/sta4.8},
	abstract = {Band depth is an important nonparametric measure that generalizes order statistics and makes univariate methods based on order statistics possible for functional data. However, the computational burden of band depth limits its applicability when large functional or image datasets are considered. This paper proposes an exact fast method to speed up the band depth computation when bands are defined by two curves. Remarkable computational gains are demonstrated through simulation studies comparing our proposal with the original computation and one existing approximate method. For example, we report an experiment where our method can rank one million curves, evaluated at fifty time points each, in 12.4 seconds with Matlab. Copyright © 2012 John Wiley \& Sons, Ltd.},
	language = {en},
	number = {1},
	urldate = {2020-07-20},
	journal = {Stat},
	author = {Sun, Ying and Genton, Marc G. and Nychka, Douglas W.},
	year = {2012},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/sta4.8},
	keywords = {approximate solution, band depth, exact solution, functional boxplot, functional data, large dataset, modified band depth},
	pages = {68--74},
}

@article{mosler_general_2018,
	title = {General notions of depth for functional data},
	url = {http://arxiv.org/abs/1208.1981},
	abstract = {A data depth measures the centrality of a point with respect to an empirical distribution. Postulates are formulated, which a depth for functional data should satisfy, and a general approach is proposed to construct multivariate data depths in Banach spaces. The new approach, mentioned as Φ-depth, is based on depth inﬁma over a proper set Φ of Rd-valued linear functions. Several desirable properties are established for the Φ-depth and a generalized version of it. The general notions include many new ones as special cases. In particular a location-slope depth and a principal component depth are introduced.},
	language = {en},
	urldate = {2020-07-16},
	journal = {arXiv:1208.1981 [stat]},
	author = {Mosler, Karl and Polyakova, Yulia},
	month = jan,
	year = {2018},
	note = {arXiv: 1208.1981},
	keywords = {Statistics - Methodology},
}

@article{gijbels_consistency_2015,
	title = {Consistency of non-integrated depths for functional data},
	volume = {140},
	issn = {0047259X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0047259X15001396},
	doi = {10.1016/j.jmva.2015.05.012},
	abstract = {In the analysis of functional data, the concept of data depth is of importance. Strong consistency of a sample version of a data depth is among the basic statistical properties that need to hold. In this paper we discuss consistency properties of three popular types of functional depth: the band depth, the half-region depth and the infimal depth. The latter is a special case of the recently introduced general class of Φ-depths. All three considered depth functions are of a non-integrated type. Counterexamples illustrate some problems with consistency results for these data depths. The main contribution of this paper consists of providing sufficient conditions for consistency of these non-integrated data depths to hold. © 2015 Elsevier Inc. All rights reserved.},
	language = {en},
	urldate = {2020-07-16},
	journal = {Journal of Multivariate Analysis},
	author = {Gijbels, Irène and Nagy, Stanislav},
	month = sep,
	year = {2015},
	pages = {259--282},
}

@article{borgonovo_methodology_2010,
	title = {A {Methodology} for {Determining} {Interactions} in {Probabilistic} {Safety} {Assessment} {Models} by {Varying} {One} {Parameter} at a {Time}},
	volume = {30},
	issn = {02724332, 15396924},
	url = {http://doi.wiley.com/10.1111/j.1539-6924.2010.01372.x},
	doi = {10.1111/j.1539-6924.2010.01372.x},
	language = {en},
	number = {3},
	urldate = {2020-07-14},
	journal = {Risk Analysis},
	author = {Borgonovo, Emanuele},
	month = mar,
	year = {2010},
	pages = {385--399},
}

@article{ball_stochastic_2019,
	title = {A stochastic {SIR} network epidemic model with preventive dropping of edges},
	volume = {78},
	issn = {1432-1416},
	url = {https://doi.org/10.1007/s00285-019-01329-4},
	doi = {10.1007/s00285-019-01329-4},
	abstract = {A Markovian Susceptible \$\${\textbackslash}rightarrow \$\$→Infectious \$\${\textbackslash}rightarrow \$\$→Recovered (SIR) model is considered for the spread of an epidemic on a configuration model network, in which susceptible individuals may take preventive measures by dropping edges to infectious neighbours. An effective degree formulation of the model is used in conjunction with the theory of density dependent population processes to obtain a law of large numbers and a functional central limit theorem for the epidemic as the population size \$\$N {\textbackslash}rightarrow {\textbackslash}infty \$\$N→∞, assuming that the degrees of individuals are bounded. A central limit theorem is conjectured for the final size of the epidemic. The results are obtained for both the Molloy–Reed (in which the degrees of individuals are deterministic) and Newman–Strogatz–Watts (in which the degrees of individuals are independent and identically distributed) versions of the configuration model. The two versions yield the same limiting deterministic model but the asymptotic variances in the central limit theorems are greater in the Newman–Strogatz–Watts version. The basic reproduction number \$\$R\_0\$\$R0and the process of susceptible individuals in the limiting deterministic model, for the model with dropping of edges, are the same as for a corresponding SIR model without dropping of edges but an increased recovery rate, though, when \$\$R\_0{\textgreater}1\$\$R0{\textgreater}1, the probability of a major outbreak is greater in the model with dropping of edges. The results are specialised to the model without dropping of edges to yield conjectured central limit theorems for the final size of Markovian SIR epidemics on configuration-model networks, and for the size of the giant components of those networks. The theory is illustrated by numerical studies, which demonstrate that the asymptotic approximations are good, even for moderate N.},
	language = {en},
	number = {6},
	urldate = {2020-06-30},
	journal = {Journal of Mathematical Biology},
	author = {Ball, Frank and Britton, Tom and Leung, Ka Yin and Sirl, David},
	month = may,
	year = {2019},
	pages = {1875--1951},
}

@article{cerina_world_2015,
	title = {World {Input}-{Output} {Network}},
	volume = {10},
	issn = {1932-6203},
	url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0134025},
	doi = {10.1371/journal.pone.0134025},
	abstract = {Production systems, traditionally analyzed as almost independent national systems, are increasingly connected on a global scale. Only recently becoming available, the World Input-Output Database (WIOD) is one of the first efforts to construct the global multi-regional input-output (GMRIO) tables. By viewing the world input-output system as an interdependent network where the nodes are the individual industries in different economies and the edges are the monetary goods flows between industries, we analyze respectively the global, regional, and local network properties of the so-called world input-output network (WION) and document its evolution over time. At global level, we find that the industries are highly but asymmetrically connected, which implies that micro shocks can lead to macro fluctuations. At regional level, we find that the world production is still operated nationally or at most regionally as the communities detected are either individual economies or geographically well defined regions. Finally, at local level, for each industry we compare the network-based measures with the traditional methods of backward linkages. We find that the network-based measures such as PageRank centrality and community coreness measure can give valuable insights into identifying the key industries.},
	language = {en},
	number = {7},
	urldate = {2020-06-30},
	journal = {PLOS ONE},
	author = {Cerina, Federica and Zhu, Zhen and Chessa, Alessandro and Riccaboni, Massimo},
	year = {2015},
	note = {Publisher: Public Library of Science},
	keywords = {Algorithms, Asia, Centrality, Eigenvectors, Europe, Finance, Germany, Inflation rates},
	pages = {e0134025},
}

@article{amador_networks_2017,
	title = {Networks of {Value}-added {Trade}},
	volume = {40},
	copyright = {© 2016 John Wiley \& Sons Ltd},
	issn = {1467-9701},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/twec.12469},
	doi = {10.1111/twec.12469},
	abstract = {Global value chains (GVCs) require new methods for evaluating interconnections among countries, which can no longer be accurately appraised by standard bilateral gross trade flows. This paper uses tools of network analysis to examine the evolution of value-added trade from 1995 to 2011. GVCs are very centralised and asymmetric networks, with a few large economies acting as hubs, which exposes them to the propagation of idiosyncratic shocks. As GVCs expanded, the networks of foreign value added in exports became denser, more complex and intensively connected. The regional dimension of GVCs is still dominant but is progressively giving place to a more global network. Networks of foreign value added in goods exports outpace those of services exports. However, foreign inputs of services are important for exports of both goods and services. There is a striking rise of China as a supplier of value added, while Germany and the United States maintain a central role in GVCs over the whole period.},
	language = {en},
	number = {7},
	urldate = {2020-06-30},
	journal = {The World Economy},
	author = {Amador, João and Cabral, Sónia},
	year = {2017},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/twec.12469},
	pages = {1291--1313},
}

@article{gammerman_abstracts_nodate,
	title = {Abstracts of invited talks and posters},
	language = {en},
	author = {Gammerman, Alex and Vovk, Vladimir and Luo, Zhiyuan and Smirnov, Evgueni},
	pages = {2},
}

@book{saltelli_global_2007,
	address = {Chichester, UK},
	title = {Global {Sensitivity} {Analysis}. {The} {Primer}},
	isbn = {978-0-470-72518-4 978-0-470-05997-5},
	url = {http://doi.wiley.com/10.1002/9780470725184},
	language = {en},
	urldate = {2020-06-25},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Saltelli, Andrea and Ratto, Marco and Andres, Terry and Campolongo, Francesca and Cariboni, Jessica and Gatelli, Debora and Saisana, Michaela and Tarantola, Stefano},
	month = dec,
	year = {2007},
	doi = {10.1002/9780470725184},
}

@incollection{noauthor_introduction_2008,
	title = {Introduction to {Sensitivity} {Analysis}},
	isbn = {978-0-470-72518-4},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9780470725184.ch1},
	abstract = {This chapter contains sections titled: Models and Sensitivity Analysis Methods and Settings for Sensitivity Analysis – an Introduction Nonindependent Input Factors Possible Pitfalls for a Sensitivity Analysis Concluding Remarks Exercises Answers Additional Exercises Solutions to Additional Exercises},
	language = {en},
	urldate = {2020-06-25},
	booktitle = {Global {Sensitivity} {Analysis}. {The} {Primer}},
	publisher = {John Wiley \& Sons, Ltd},
	year = {2008},
	doi = {10.1002/9780470725184.ch1},
	note = {Section: 1
\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9780470725184.ch1},
	keywords = {averaged partial variances, circumscribed chemical kinetics problem, data-driven models and law-driven models, derivative-based approach's fatal limitation, model update and parameter estimation step, modelling and scientific inquiry, multidimensionally averaged measures, scientific controversy or public policy debate, sensitivity analysis measures, variance decomposition framework},
	pages = {1--51},
}

@misc{noauthor_global_nodate,
	title = {Global {Sensitivity} {Analysis}. {The} {Primer} {\textbar} {Wiley} {Online} {Books}},
	url = {https://onlinelibrary.wiley.com/doi/book/10.1002/9780470725184},
	urldate = {2020-06-25},
}

@article{lu_estimation_2018,
	title = {Estimation of {Sobol}’s {Sensitivity} {Indices} under {Generalized} {Linear} {Models}},
	volume = {47},
	issn = {0361-0926},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6141050/},
	doi = {10.1080/03610926.2017.1388397},
	abstract = {We derive explicit formulas for Sobol’s sensitivity indices (SSIs) under the generalized linear models (GLMs) with independent or multivariate normal inputs. We argue that the main-effect SSIs provide a powerful tool for variable selection under GLMs with identity links under polynomial regressions. We also show via examples that the SSI-based variable selection results are similar to the ones obtained by the random forest algorithm but without the computational burden of data permutation. Finally, applying our results to the problem of gene network discovery, we identify though the SSI analysis of a public microarray dataset several novel higher-order gene-gene interactions missed out by the more standard inference methods. The relevant functions for SSI analysis derived here under GLMs with identity, log, and logit links are implemented and made available in the R package SobolSensitivity.},
	number = {21},
	urldate = {2020-06-25},
	journal = {Communications in statistics: theory and methods},
	author = {Lu, Rong and Wang, Danxin and Wang, Min and Rempala, Grzegorz A.},
	year = {2018},
	pmid = {30237653},
	pmcid = {PMC6141050},
	pages = {5163--5195},
}

@article{borgonovo_common_2016,
	title = {A {Common} {Rationale} for {Global} {Sensitivity} {Measures} and {Their} {Estimation}},
	volume = {36},
	copyright = {© 2016 Society for Risk Analysis},
	issn = {1539-6924},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/risa.12555},
	doi = {10.1111/risa.12555},
	abstract = {Measures of sensitivity and uncertainty have become an integral part of risk analysis. Many such measures have a conditional probabilistic structure, for which a straightforward Monte Carlo estimation procedure has a double-loop form. Recently, a more efficient single-loop procedure has been introduced, and consistency of this procedure has been demonstrated separately for particular measures, such as those based on variance, density, and information value. In this work, we give a unified proof of single-loop consistency that applies to any measure satisfying a common rationale. This proof is not only more general but invokes less restrictive assumptions than heretofore in the literature, allowing for the presence of correlations among model inputs and of categorical variables. We examine numerical convergence of such an estimator under a variety of sensitivity measures. We also examine its application to a published medical case study.},
	language = {en},
	number = {10},
	urldate = {2020-06-25},
	journal = {Risk Analysis},
	author = {Borgonovo, Emanuele and Hazen, Gordon B. and Plischke, Elmar},
	year = {2016},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/risa.12555},
	keywords = {Global sensitivity measures, Monte Carlo simulation, probabilistic sensitivity analysis, risk analysis, uncertainty analysis},
	pages = {1871--1895},
}

@article{borgonovo_moment_2008,
	title = {Moment independent and variance-based sensitivity analysis with correlations: {An} application to the stability of a chemical reactor},
	volume = {40},
	copyright = {Copyright © 2008 Wiley Periodicals, Inc.},
	issn = {1097-4601},
	shorttitle = {Moment independent and variance-based sensitivity analysis with correlations},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/kin.20368},
	doi = {10.1002/kin.20368},
	abstract = {Recent works have attracted interest toward sensitivity measures that use the entire model output distribution, without dependence on any of its particular moments (e.g., variance). However, the computation of moment-independent importance measures in the presence of dependencies among model inputs has not been dealt with yet. This work has two purposes. On the one hand, to introduce moment independent techniques in the analysis of chemical reaction models. On the other hand, to allow their computation in the presence of correlations. To do so, a new approach based on Gibbs sampling is presented that allows the joint estimation of variance-based and moment independent sensitivity measures in the presence of correlations. The application to the stability of a chemical reactor is then discussed, allowing full consideration of historical data that included a correlation coefficient of 0.7 between two of the model parameters. © 2008 Wiley Periodicals, Inc. Int J Chem Kinet 40: 687–698, 2008},
	language = {en},
	number = {11},
	urldate = {2020-06-25},
	journal = {International Journal of Chemical Kinetics},
	author = {Borgonovo, E. and Tarantola, S.},
	year = {2008},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/kin.20368},
	pages = {687--698},
}

@article{helton_survey_2006,
	series = {The {Fourth} {International} {Conference} on {Sensitivity} {Analysis} of {Model} {Output} ({SAMO} 2004)},
	title = {Survey of sampling-based methods for uncertainty and sensitivity analysis},
	volume = {91},
	issn = {0951-8320},
	url = {http://www.sciencedirect.com/science/article/pii/S0951832005002292},
	doi = {10.1016/j.ress.2005.11.017},
	abstract = {Sampling-based methods for uncertainty and sensitivity analysis are reviewed. The following topics are considered: (i) definition of probability distributions to characterize epistemic uncertainty in analysis inputs, (ii) generation of samples from uncertain analysis inputs, (iii) propagation of sampled inputs through an analysis, (iv) presentation of uncertainty analysis results, and (v) determination of sensitivity analysis results. Special attention is given to the determination of sensitivity analysis results, with brief descriptions and illustrations given for the following procedures/techniques: examination of scatterplots, correlation analysis, regression analysis, partial correlation analysis, rank transformations, statistical tests for patterns based on gridding, entropy tests for patterns based on gridding, nonparametric regression analysis, squared rank differences/rank correlation coefficient test, two-dimensional Kolmogorov–Smirnov test, tests for patterns based on distance measures, top down coefficient of concordance, and variance decomposition.},
	language = {en},
	number = {10},
	urldate = {2020-06-25},
	journal = {Reliability Engineering \& System Safety},
	author = {Helton, J. C. and Johnson, J. D. and Sallaberry, C. J. and Storlie, C. B.},
	month = oct,
	year = {2006},
	keywords = {Aleatory uncertainty, Epistemic uncertainty, Latin hypercube sampling, Monte Carlo, Sensitivity analysis, Uncertainty analysis},
	pages = {1175--1209},
}

@article{glasserman_sensitivity_2010,
	title = {Sensitivity {Estimates} from {Characteristic} {Functions}},
	volume = {58},
	issn = {0030-364X, 1526-5463},
	url = {http://pubsonline.informs.org/doi/abs/10.1287/opre.1100.0837},
	doi = {10.1287/opre.1100.0837},
	language = {en},
	number = {6},
	urldate = {2020-06-25},
	journal = {Operations Research},
	author = {Glasserman, Paul and Liu, Zongjian},
	month = dec,
	year = {2010},
	pages = {1611--1623},
}

@article{anderson_uncertainty_2014,
	title = {Uncertainty in climate change modeling: can global sensitivity analysis be of help?},
	volume = {34},
	issn = {1539-6924},
	shorttitle = {Uncertainty in climate change modeling},
	doi = {10.1111/risa.12117},
	abstract = {Integrated assessment models offer a crucial support to decisionmakers in climate policy making. For a full understanding and corroboration of model results, analysts ought to identify the exogenous variables that influence the model results the most (key drivers), appraise the relevance of interactions, and the direction of change associated with the simultaneous variation of uncertain variables. We show that such information can be directly extracted from the data set produced by Monte Carlo simulations. Our discussion is guided by the application to the well-known DICE model of William Nordhaus. The proposed methodology allows analysts to draw robust insights into the dependence of future atmospheric temperature, global emissions, and carbon costs and taxes on the model's exogenous variables.},
	language = {eng},
	number = {2},
	journal = {Risk Analysis: An Official Publication of the Society for Risk Analysis},
	author = {Anderson, Barry and Borgonovo, Emanuele and Galeotti, Marzio and Roson, Roberto},
	month = feb,
	year = {2014},
	pmid = {24111855},
	keywords = {Climate Change, Climate change, Computer Simulation, Models, Theoretical, Monte Carlo Method, Risk Assessment, Uncertainty, global sensitivity analysis, integrated assessment modeling, risk analysis},
	pages = {271--293},
}

@article{antoniano-villalobos_which_2018,
	title = {Which {Parameters} {Are} {Important}? {Differential} {Importance} {Under} {Uncertainty}},
	volume = {38},
	issn = {1539-6924},
	shorttitle = {Which {Parameters} {Are} {Important}?},
	doi = {10.1111/risa.13125},
	abstract = {In probabilistic risk assessment, attention is often focused on the expected value of a risk metric. The sensitivity of this expectation to changes in the parameters of the distribution characterizing uncertainty in the inputs becomes of interest. Approaches based on differentiation encounter limitations when (i) distributional parameters are expressed in different units or (ii) the analyst wishes to transfer sensitivity insights from individual parameters to parameter groups, when alternating between different levels of a probabilistic safety assessment model. Moreover, the analyst may also wish to examine the effect of assuming independence among inputs. This work proposes an approach based on the differential importance measure, which solves these issues. Estimation aspects are discussed in detail, in particular the problem of obtaining all sensitivity measures from a single Monte Carlo sample, thus avoiding potentially costly model runs. The approach is illustrated through an analytical example, highlighting how it can be used to assess the impact of removing the independence assumption. An application to the probabilistic risk assessment model of the Advanced Test Reactor large loss of coolant accident sequence concludes the work.},
	language = {eng},
	number = {11},
	journal = {Risk Analysis: An Official Publication of the Society for Risk Analysis},
	author = {Antoniano-Villalobos, Isadora and Borgonovo, Emanuele and Siriwardena, Sumeda},
	year = {2018},
	pmid = {29924879},
	keywords = {Importance measures, risk analysis, sensitivity analysis, uncertainty analysis},
	pages = {2459--2477},
}

@article{menafoglio_random_2018,
	title = {Random domain decompositions for object-oriented {Kriging} over complex domains},
	volume = {32},
	issn = {1436-3259},
	url = {https://doi.org/10.1007/s00477-018-1596-z},
	doi = {10.1007/s00477-018-1596-z},
	abstract = {We propose a new methodology for the analysis of spatial fields of object data distributed over complex domains. Our approach enables to jointly handle both data and domain complexities, through a divide et impera approach. As a key element of innovation, we propose to use a random domain decomposition, whose realizations define sets of homogeneous sub-regions where to perform simple, independent, weak local analyses (divide), eventually aggregated into a final strong one (impera). In this broad framework, the complexity of the domain (e.g., strong concavities, holes or barriers) can be accounted for by defining its partitions on the basis of a suitable metric, which allows to properly represent the adjacency relationships among the complex data (such as scalar, functional or constrained data) over the domain. As an illustration of the potential of the methodology, we consider the analysis and spatial prediction (Kriging) of the probability density function of dissolved oxygen in the Chesapeake Bay.},
	language = {en},
	number = {12},
	urldate = {2020-06-22},
	journal = {Stochastic Environmental Research and Risk Assessment},
	author = {Menafoglio, Alessandra and Gaetani, Giorgia and Secchi, Piercesare},
	month = dec,
	year = {2018},
	pages = {3421--3437},
}

@article{pigoli_kriging_2016,
	title = {Kriging prediction for manifold-valued random fields},
	doi = {10.1016/j.jmva.2015.12.006},
	abstract = {The statistical analysis of data belonging to Riemannian manifolds is becoming increasingly important in many applications, such as shape analysis, diffusion tensor imaging and the analysis of covariance matrices. In many cases, data are spatially distributed but it is not trivial to take into account spatial dependence in the analysis because of the non linear geometry of the manifold. This work proposes a solution to the problem of spatial prediction for manifold valued data, with a particular focus on the case of positive definite symmetric matrices. Under the hypothesis that the dispersion of the observations on the manifold is not too large, data can be projected on a suitably chosen tangent space, where an additive model can be used to describe the relationship between response variable and covariates. Thus, we generalize classical kriging prediction, dealing with the spatial dependence in this tangent space, where well established Euclidean methods can be used. The proposed kriging prediction is applied to the matrix field of covariances between temperature and precipitation in Quebec, Canada.},
	journal = {J. Multivar. Anal.},
	author = {Pigoli, Davide and Menafoglio, Alessandra and Secchi, Piercesare},
	year = {2016},
}

@article{menafoglio_universal_2013,
	title = {A {Universal} {Kriging} predictor for spatially dependent functional data of a {Hilbert} {Space}},
	volume = {7},
	issn = {1935-7524},
	url = {https://projecteuclid.org/euclid.ejs/1379596770},
	doi = {10.1214/13-EJS843},
	abstract = {We address the problem of predicting spatially dependent functional data belonging to a Hilbert space, with a Functional Data Analysis approach. Having defined new global measures of spatial variability for functional random processes, we derive a Universal Kriging predictor for functional data. Consistently with the new established theoretical results, we develop a two-step procedure for predicting georeferenced functional data: first model selection and estimation of the spatial mean (drift), then Universal Kriging prediction on the basis of the identified model. The proposed methodology is applied to daily mean temperatures curves recorded in the Maritimes Provinces of Canada.},
	language = {EN},
	urldate = {2020-06-22},
	journal = {Electronic Journal of Statistics},
	author = {Menafoglio, Alessandra and Secchi, Piercesare and Rosa, Matilde Dalla},
	year = {2013},
	mrnumber = {MR3108813},
	zmnumber = {1293.62120},
	note = {Publisher: The Institute of Mathematical Statistics and the Bernoulli Society},
	keywords = {Functional data analysis, Sobolev metrics, spatial prediction, variogram},
	pages = {2209--2240},
}

@article{willis_confidence_2019,
	title = {Confidence {Sets} for {Phylogenetic} {Trees}},
	volume = {114},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/full/10.1080/01621459.2017.1395342},
	doi = {10.1080/01621459.2017.1395342},
	abstract = {Inferring evolutionary histories (phylogenetic trees) has important applications in biology, criminology, and public health. However, phylogenetic trees are complex mathematical objects that reside in a non-Euclidean space, which complicates their analysis. While our mathematical, algorithmic, and probabilistic understanding of phylogenies in their metric space is mature, rigorous inferential infrastructure is as yet undeveloped. In this manuscript, we unify recent computational and probabilistic advances to construct tree–valued confidence sets. The procedure accounts for both center and multiple directions of tree–valued variability. We draw on block replicates to improve testing, identifying the best supported most recent ancestor of the Zika virus, and formally testing the hypothesis that a Floridian dentist with AIDS infected two of his patients with HIV. The method illustrates connections between variability in Euclidean and tree space, opening phylogenetic tree analysis to techniques available in the multivariate Euclidean setting. Supplementary materials for this article are available online.},
	number = {525},
	urldate = {2020-06-16},
	journal = {Journal of the American Statistical Association},
	author = {Willis, Amy},
	month = jan,
	year = {2019},
	pages = {235--244},
}

@article{nouretdinov_multi-level_2020,
	title = {Multi-level conformal clustering: {A} distribution-free technique for clustering and anomaly detection},
	volume = {397},
	copyright = {All rights reserved},
	issn = {0925-2312},
	shorttitle = {Multi-level conformal clustering},
	url = {http://www.sciencedirect.com/science/article/pii/S0925231219316169},
	doi = {10.1016/j.neucom.2019.07.114},
	abstract = {In this work we present a clustering technique called multi-level conformal clustering (MLCC). The technique is hierarchical in nature because it can be performed at multiple significance levels which yields greater insight into the data than performing it at just one level. We describe the theoretical underpinnings of MLCC, compare and contrast it with the hierarchical clustering algorithm, and then apply it to real world datasets to assess its performance. There are several advantages to using MLCC over more classical clustering techniques: Once a significance level has been set, MLCC is able to automatically select the number of clusters. Furthermore, thanks to the conformal prediction framework the resulting clustering model has a clear statistical meaning without any assumptions about the distribution of the data. This statistical robustness also allows us to perform clustering and anomaly detection simultaneously. Moreover, due to the flexibility of the conformal prediction framework, our algorithm can be used on top of many other machine learning algorithms.},
	language = {en},
	urldate = {2020-05-30},
	journal = {Neurocomputing},
	author = {Nouretdinov, Ilia and Gammerman, James and Fontana, Matteo and Rehal, Daljit},
	month = jul,
	year = {2020},
	keywords = {Clustering, Conformal prediction, Dendrograms},
	pages = {279--291},
}

@article{choi_geometric_2018,
	title = {A geometric approach to confidence regions and bands for functional parameters},
	volume = {80},
	copyright = {© 2017 Royal Statistical Society},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12239},
	doi = {10.1111/rssb.12239},
	abstract = {Functional data analysis is now a well-established discipline of statistics, with its core concepts and perspectives in place. Despite this, there are still fundamental statistical questions which have received relatively little attention. One of these is the systematic construction of confidence regions for functional parameters. This work is concerned with developing, understanding and visualizing such regions. We provide a general strategy for constructing confidence regions in a real separable Hilbert space by using hyperellipsoids and hyper-rectangles. We then propose specific implementations which work especially well in practice. They provide powerful hypothesis tests and useful visualization tools without relying on simulation. We also demonstrate the negative result that nearly all regions, including our own, have zero coverage when working with empirical covariances. To overcome this challenge we propose a new paradigm for evaluating confidence regions by showing that the distance between an estimated region and the desired region (with proper coverage) tends to 0 faster than the regions shrink to a point. We call this phenomena ghosting and refer to the empirical regions as ghost regions. We illustrate the proposed methods in a simulation study and an application to fractional anisotropy tract profile data.},
	language = {en},
	number = {1},
	urldate = {2020-05-13},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Choi, Hyunphil and Reimherr, Matthew},
	year = {2018},
	note = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/rssb.12239},
	keywords = {Confidence bands, Confidence regions, Functional data analysis, Hypothesis testing, Principal component analysis},
	pages = {239--260},
}

@article{cao_simultaneous_2012,
	title = {Simultaneous {Inference} {For} {The} {Mean} {Function} {Based} on {Dense} {Functional} {Data}},
	volume = {24},
	issn = {1048-5252},
	doi = {10.1080/10485252.2011.638071},
	abstract = {A polynomial spline estimator is proposed for the mean function of dense functional data together with a simultaneous confidence band which is asymptotically correct. In addition, the spline estimator and its accompanying confidence band enjoy oracle efficiency in the sense that they are asymptotically the same as if all random trajectories are observed entirely and without errors. The confidence band is also extended to the difference of mean functions of two populations of functional data. Simulation experiments provide strong evidence that corroborates the asymptotic theory while computing is efficient. The confidence band procedure is illustrated by analyzing the near infrared spectroscopy data.},
	language = {eng},
	number = {2},
	journal = {Journal of Nonparametric Statistics},
	author = {Cao, Guanqun and Yang, Lijian and Todem, David},
	month = jun,
	year = {2012},
	pmid = {22665964},
	pmcid = {PMC3365609},
	pages = {359--377},
}

@article{degras_simultaneous_2011,
	title = {Simultaneous confidence bands for nonparametric regression with functional data},
	volume = {21},
	issn = {10170405},
	url = {http://www3.stat.sinica.edu.tw/statistica/J21N4/j21n412/j21n412.html},
	doi = {10.5705/ss.2009.207},
	abstract = {We consider nonparametric regression in the context of functional data, that is, when a random sample of functions is observed on a ﬁne grid. We obtain a functional asymptotic normality result that can provide simultaneous conﬁdence bands (SCB) for various estimation and inference tasks. Applications to a SCB procedure for the regression function and to a goodness-of-ﬁt test for curvilinear regression models are proposed. The ﬁrst one has improved accuracy over other available methods, while the second can detect local departures from a parametric shape, as opposed to the usual goodness-of-ﬁt tests which only track global departures. A numerical study of the SCB procedures and an illustration with a speech data set are provided.},
	language = {en},
	number = {4},
	urldate = {2020-05-13},
	journal = {Statistica Sinica},
	author = {Degras, David A.},
	month = oct,
	year = {2011},
}

@article{sangalli_k-mean_2010,
	title = {k-mean alignment for curve clustering},
	volume = {54},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947309004605},
	doi = {10.1016/j.csda.2009.12.008},
	abstract = {The problem of curve clustering when curves are misaligned is considered. A novel algorithm is described, which jointly clusters and aligns curves. The proposed procedure efficiently decouples amplitude and phase variability; in particular, it is able to detect amplitude clusters while simultaneously disclosing clustering structures in the phase, pointing out features that can neither be captured by simple curve clustering nor by simple curve alignment. The procedure is illustrated via simulation studies and applications to real data.},
	language = {en},
	number = {5},
	urldate = {2020-02-27},
	journal = {Computational Statistics \& Data Analysis},
	author = {Sangalli, Laura M. and Secchi, Piercesare and Vantini, Simone and Vitelli, Valeria},
	month = may,
	year = {2010},
	keywords = {-mean algorithm, Curve alignment, Curve clustering, Functional data analysis},
	pages = {1219--1233},
}

@article{reiss_methods_2017,
	title = {Methods for {Scalar}-on-{Function} {Regression}},
	volume = {85},
	copyright = {© 2016 The Authors. International Statistical Review © 2016 International Statistical Institute},
	issn = {1751-5823},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12163},
	doi = {10.1111/insr.12163},
	abstract = {Recent years have seen an explosion of activity in the field of functional data analysis (FDA), in which curves, spectra, images and so on are considered as basic functional data units. A central problem in FDA is how to fit regression models with scalar responses and functional data points as predictors. We review some of the main approaches to this problem, categorising the basic model types as linear, non-linear and non-parametric. We discuss publicly available software packages and illustrate some of the procedures by application to a functional magnetic resonance imaging data set.},
	language = {en},
	number = {2},
	urldate = {2020-02-18},
	journal = {International Statistical Review},
	author = {Reiss, Philip T. and Goldsmith, Jeff and Shang, Han Lin and Ogden, R. Todd},
	year = {2017},
	keywords = {Functional additive model, functional generalised linear model, functional linear model, functional polynomial regression, functional single-index model, non-parametric functional regression},
	pages = {228--249},
}

@article{capezza_control_nodate,
	title = {Control charts for monitoring ship operating conditions and {CO2} emissions based on scalar-on-function regression},
	volume = {n/a},
	copyright = {© 2020 John Wiley \& Sons, Ltd.},
	issn = {1526-4025},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/asmb.2507},
	doi = {10.1002/asmb.2507},
	abstract = {To respond to the compelling air pollution programs, shipping companies are nowadays setting-up on their fleets modern multisensor systems that stream massive amounts of observational data, which can be considered as varying over a continuous domain. Motivated by this context, a novel procedure is proposed, which extends classical multivariate techniques to the monitoring of multivariate functional data and a scalar quality characteristic related to them. The proposed procedure is shown to be also applicable in real time and is illustrated by means of a real-case study in the maritime field on the continuous monitoring of operating conditions (ie, the multivariate functional data) and total CO2 emissions (ie, the scalar quality characteristic) at each voyage of a cruise ship. The real-time monitoring is particularly helpful for promptly supporting managerial decision making by indicating if and when an anomaly occurs during the navigation.},
	language = {en},
	number = {n/a},
	urldate = {2020-02-18},
	journal = {Applied Stochastic Models in Business and Industry},
	author = {Capezza, Christian and Lepore, Antonio and Menafoglio, Alessandra and Palumbo, Biagio and Vantini, Simone},
	keywords = {functional data analysis, multivariate functional principal component analysis, profile monitoring, statistical process monitoring},
}

@article{aydin_principal_2008,
	title = {A {Principal} {Component} {Analysis} for {Trees}},
	url = {http://arxiv.org/abs/0810.0944},
	abstract = {The active field of Functional Data Analysis (about understanding the variation in a set of curves) has been recently extended to Object Oriented Data Analysis, which considers populations of more general objects. A particularly challenging extension of this set of ideas is to populations of tree-structured objects. We develop an analog of Principal Component Analysis for trees, based on the notion of tree-lines, and propose numerically fast (linear time) algorithms to solve the resulting optimization problems. The solutions we obtain are used in the analysis of a data set of 73 individuals, where each data object is a tree of blood vessels in one person's brain.},
	language = {en},
	urldate = {2020-02-10},
	journal = {arXiv:0810.0944 [math, q-bio, stat]},
	author = {Aydin, Burcu and Pataki, Gabor and Wang, Haonan and Bullitt, Elizabeth and Marron, J. S.},
	month = oct,
	year = {2008},
	note = {arXiv: 0810.0944},
	keywords = {62H99, 62G99, 05C05, 9008, Mathematics - Optimization and Control, Mathematics - Statistics Theory, Quantitative Biology - Quantitative Methods, Statistics - Computation},
}

@article{bhattacharya_extrinsic_nodate,
	title = {Extrinsic {Data} {Analysis} on {Sample} {Spaces} with a {Manifold} {Stratiﬁcation}},
	language = {en},
	author = {Bhattacharya, R N and Arizona, U and Buibas, M and Dryden, I L and Ellingson, L A and Groisser, D and Florida, U and Hendriks, H and Huckemann, S and Le, Huiling and Liu, X and Osborne, D E and Pa, V and Marron, J S and Schwartzman, A and Thompson, H W and Wood, A T A},
	pages = {16},
}

@incollection{huckemann_nonparametric_2016,
	address = {Cham},
	series = {Contemporary {Mathematicians}},
	title = {Nonparametric {Statistics} on {Manifolds} and {Beyond}},
	isbn = {978-3-319-30190-7},
	url = {https://doi.org/10.1007/978-3-319-30190-7_18},
	abstract = {We review some aspects of the Bhattacharya-Patrangenaru asymptotic theory for intrinsic and extrinsic means on manifolds, some of the problems involved, many of which are still open, and survey some of its impacts on the community.},
	language = {en},
	urldate = {2020-02-10},
	booktitle = {Rabi {N}. {Bhattacharya}: {Selected} {Papers}},
	publisher = {Springer International Publishing},
	author = {Huckemann, Stephan and Hotz, Thomas},
	editor = {Denker, Manfred and Waymire, Edward C.},
	year = {2016},
	doi = {10.1007/978-3-319-30190-7_18},
	keywords = {Central Limit Theorem, Manifold Stability, Procrustes Distance, Sectional Curvature, Shape Space},
	pages = {599--609},
}

@article{bharath_statistical_2017,
	title = {Statistical {Tests} for {Large} {Tree}-{Structured} {Data}},
	volume = {112},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2016.1240081},
	doi = {10.1080/01621459.2016.1240081},
	abstract = {We develop a general statistical framework for the analysis and inference of large tree-structured data, with a focus on developing asymptotic goodness-of-fit tests. We first propose a consistent statistical model for binary trees, from which we develop a class of invariant tests. Using the model for binary trees, we then construct tests for general trees by using the distributional properties of the continuum random tree, which arises as the invariant limit for a broad class of models for tree-structured data based on conditioned Galton–Watson processes. The test statistics for the goodness-of-fit tests are simple to compute and are asymptotically distributed as χ2 and F random variables. We illustrate our methods on an important application of detecting tumor heterogeneity in brain cancer. We use a novel approach with tree-based representations of magnetic resonance images and employ the developed tests to ascertain tumor heterogeneity between two groups of patients. Supplementary materials for this article are available online.},
	number = {520},
	urldate = {2020-02-10},
	journal = {Journal of the American Statistical Association},
	author = {Bharath, Karthik and Kambadur, Prabhanjan and Dey, Dipak K. and Rao, Arvind and Baladandayuthapani, Veerabhadran},
	month = oct,
	year = {2017},
	keywords = {Conditioned Galton–Watson trees, Consistent statistical models, Dyck path, Goodness-of-fit tests},
	pages = {1733--1743},
}

@article{nye_diffusion_2014,
	title = {Diffusion on {Some} {Simple} {Stratified} {Spaces}},
	volume = {50},
	issn = {1573-7683},
	url = {https://doi.org/10.1007/s10851-013-0457-0},
	doi = {10.1007/s10851-013-0457-0},
	abstract = {A variety of different imaging techniques produce data which naturally lie in stratified spaces. These spaces consist of smooth regions of maximal dimension glued together along lower dimensional boundaries. Diffusion processes are important as they can be used to represent noise in statistical models on spaces for which standard parametric probability distributions do not exist. We consider particles undergoing Brownian motion in some low dimensional stratified spaces, and obtain analytic solutions to the heat equation specifying the distribution of particles. These solutions play the role of prototypical distributions for studying behaviour near singularities. While probabilistic reasoning can be used to solve the heat equation in some straightforward cases, more generally we construct solutions from eigenfunctions of the Laplacian. Specifically, we solve the heat equation on: open books; two-dimensional cones; the Petersen graph with unit edge length; and the cone of this graph which corresponds to a space of evolutionary trees.},
	language = {en},
	number = {1},
	urldate = {2020-02-10},
	journal = {Journal of Mathematical Imaging and Vision},
	author = {Nye, T. M. W. and White, M. C.},
	month = sep,
	year = {2014},
	keywords = {Diffusion, Heat equation, Laplacian, Petersen graph, Phylogeny, Stratified space},
	pages = {115--125},
}

@article{kolaczyk_averages_2019,
	title = {Averages of {Unlabeled} {Networks}: {Geometric} {Characterization} and {Asymptotic} {Behavior}},
	shorttitle = {Averages of {Unlabeled} {Networks}},
	url = {http://arxiv.org/abs/1709.02793},
	abstract = {It is becoming increasingly common to see large collections of network data objects -- that is, data sets in which a network is viewed as a fundamental unit of observation. As a result, there is a pressing need to develop network-based analogues of even many of the most basic tools already standard for scalar and vector data. In this paper, our focus is on averages of unlabeled, undirected networks with edge weights. Specifically, we (i) characterize a certain notion of the space of all such networks, (ii) describe key topological and geometric properties of this space relevant to doing probability and statistics thereupon, and (iii) use these properties to establish the asymptotic behavior of a generalized notion of an empirical mean under sampling from a distribution supported on this space. Our results rely on a combination of tools from geometry, probability theory, and statistical shape analysis. In particular, the lack of vertex labeling necessitates working with a quotient space modding out permutations of labels. This results in a nontrivial geometry for the space of unlabeled networks, which in turn is found to have important implications on the types of probabilistic and statistical results that may be obtained and the techniques needed to obtain them.},
	urldate = {2020-02-10},
	journal = {arXiv:1709.02793 [math, stat]},
	author = {Kolaczyk, Eric and Lin, Lizhen and Rosenberg, Steven and Xu, Jie and Walters, Jackson},
	month = feb,
	year = {2019},
	note = {arXiv: 1709.02793},
	keywords = {62E20 (Primary) 53C20 (Secondary), Mathematics - Differential Geometry, Mathematics - Statistics Theory},
}

@misc{kolaczyk_topics_2017,
	title = {Topics at the {Frontier} of {Statistics} and {Network} {Analysis}: ({Re}){Visiting} the {Foundations}},
	shorttitle = {Topics at the {Frontier} of {Statistics} and {Network} {Analysis}},
	url = {/core/elements/topics-at-the-frontier-of-statistics-and-network-analysis/297CBD5F3961567E16E6A161227CCA83},
	abstract = {{\textless}div class="abstract" data-abstract-type="normal"{\textgreater}This snapshot of the current frontier of statistics and network analysis focuses on the foundational topics of modeling, sampling, and design. Primarily for graduate students and researchers in statistics and closely related fields, emphasis is not only on what has been done, but on what remains to be done.{\textless}/div{\textgreater}},
	language = {en},
	urldate = {2020-02-10},
	journal = {SemStat Elements},
	author = {Kolaczyk, Eric D.},
	month = jun,
	year = {2017},
	doi = {10.1017/9781108290159},
}

@article{alfaro_dimension_2014,
	title = {Dimension reduction in principal component analysis for trees},
	volume = {74},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947313004763},
	doi = {10.1016/j.csda.2013.12.007},
	abstract = {The statistical analysis of tree structured data is a new topic in statistics with wide application areas. Some Principal Component Analysis (PCA) ideas have been previously developed for binary tree spaces. These ideas are extended to the more general space of rooted and ordered trees. Concepts such as tree-line and forward principal component tree-line are redefined for this more general space, and the optimal algorithm that finds them is generalized. An analog of the classical dimension reduction technique in PCA for tree spaces is developed. To do this, backward principal components, the components that carry the least amount of information on tree data set, are defined. An optimal algorithm to find them is presented. Furthermore, the relationship of these to the forward principal components is investigated, and a path-independence property between the forward and backward techniques is proven. These methods are applied to a brain artery data set of 98 subjects. Using these techniques, the effects of aging to the brain artery structure of males and females is investigated. A second data set of the organization structure of a large US company is also analyzed and the structural differences across different types of departments within the company are explored.},
	language = {en},
	urldate = {2020-02-10},
	journal = {Computational Statistics \& Data Analysis},
	author = {Alfaro, Carlos A. and Aydın, Burcu and Valencia, Carlos E. and Bullitt, Elizabeth and Ladha, Alim},
	month = jun,
	year = {2014},
	keywords = {Combinatorial optimization, Dimension reduction, Object oriented data analysis, Principal component analysis, Tree structured objects, Tree-lines},
	pages = {157--179},
}

@article{ginestet_hypothesis_2017,
	title = {Hypothesis testing for network data in functional neuroimaging},
	volume = {11},
	issn = {1932-6157, 1941-7330},
	url = {https://projecteuclid.org/euclid.aoas/1500537721},
	doi = {10.1214/16-AOAS1015},
	abstract = {In recent years, it has become common practice in neuroscience to use networks to summarize relational information in a set of measurements, typically assumed to be reflective of either functional or structural relationships between regions of interest in the brain. One of the most basic tasks of interest in the analysis of such data is the testing of hypotheses, in answer to questions such as “Is there a difference between the networks of these two groups of subjects?” In the classical setting, where the unit of interest is a scalar or a vector, such questions are answered through the use of familiar two-sample testing strategies. Networks, however, are not Euclidean objects, and hence classical methods do not directly apply. We address this challenge by drawing on concepts and techniques from geometry and high-dimensional statistical inference. Our work is based on a precise geometric characterization of the space of graph Laplacian matrices and a nonparametric notion of averaging due to Fréchet. We motivate and illustrate our resulting methodologies for testing in the context of networks derived from functional neuroimaging data on human subjects from the 1000 Functional Connectomes Project. In particular, we show that this global test is more statistically powerful than a mass-univariate approach. In addition, we have also provided a method for visualizing the individual contribution of each edge to the overall test statistic.},
	language = {EN},
	number = {2},
	urldate = {2020-02-10},
	journal = {The Annals of Applied Statistics},
	author = {Ginestet, Cedric E. and Li, Jun and Balachandran, Prakash and Rosenberg, Steven and Kolaczyk, Eric D.},
	month = jun,
	year = {2017},
	mrnumber = {MR3693544},
	zmnumber = {06775890},
	keywords = {Fréchet mean, fMRI, graph Laplacian, hypothesis testing, matrix manifold, network data, object data},
	pages = {725--750},
}

@article{rogelj_scenarios_2018,
	title = {Scenarios towards limiting global mean temperature increase below 1.5 °{C}},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {1758-6798},
	url = {https://www.nature.com/articles/s41558-018-0091-3},
	doi = {10.1038/s41558-018-0091-3},
	abstract = {Scenarios that constrain end-of-century radiative forcing to 1.9 W m–2, and thus global mean temperature increases to below 1.5 °C, are explored. Effective scenarios reduce energy use, deploy CO2 removal measures, and shift to non-emitting energy sources.},
	language = {en},
	number = {4},
	urldate = {2019-10-28},
	journal = {Nature Climate Change},
	author = {Rogelj, Joeri and Popp, Alexander and Calvin, Katherine V. and Luderer, Gunnar and Emmerling, Johannes and Gernaat, David and Fujimori, Shinichiro and Strefler, Jessica and Hasegawa, Tomoko and Marangoni, Giacomo and Krey, Volker and Kriegler, Elmar and Riahi, Keywan and Vuuren, Detlef P. van and Doelman, Jonathan and Drouet, Laurent and Edmonds, Jae and Fricko, Oliver and Harmsen, Mathijs and Havlík, Petr and Humpenöder, Florian and Stehfest, Elke and Tavoni, Massimo},
	month = apr,
	year = {2018},
	pages = {325--332},
}

@article{van_vuuren_representative_2011,
	title = {The representative concentration pathways: an overview},
	volume = {109},
	issn = {1573-1480},
	shorttitle = {The representative concentration pathways},
	url = {https://doi.org/10.1007/s10584-011-0148-z},
	doi = {10.1007/s10584-011-0148-z},
	abstract = {This paper summarizes the development process and main characteristics of the Representative Concentration Pathways (RCPs), a set of four new pathways developed for the climate modeling community as a basis for long-term and near-term modeling experiments. The four RCPs together span the range of year 2100 radiative forcing values found in the open literature, i.e. from 2.6 to 8.5 W/m2. The RCPs are the product of an innovative collaboration between integrated assessment modelers, climate modelers, terrestrial ecosystem modelers and emission inventory experts. The resulting product forms a comprehensive data set with high spatial and sectoral resolutions for the period extending to 2100. Land use and emissions of air pollutants and greenhouse gases are reported mostly at a 0.5 × 0.5 degree spatial resolution, with air pollutants also provided per sector (for well-mixed gases, a coarser resolution is used). The underlying integrated assessment model outputs for land use, atmospheric emissions and concentration data were harmonized across models and scenarios to ensure consistency with historical observations while preserving individual scenario trends. For most variables, the RCPs cover a wide range of the existing literature. The RCPs are supplemented with extensions (Extended Concentration Pathways, ECPs), which allow climate modeling experiments through the year 2300. The RCPs are an important development in climate research and provide a potential foundation for further research and assessment, including emissions mitigation and impact analysis.},
	language = {en},
	number = {1},
	urldate = {2019-10-28},
	journal = {Climatic Change},
	author = {van Vuuren, Detlef P. and Edmonds, Jae and Kainuma, Mikiko and Riahi, Keywan and Thomson, Allison and Hibbard, Kathy and Hurtt, George C. and Kram, Tom and Krey, Volker and Lamarque, Jean-Francois and Masui, Toshihiko and Meinshausen, Malte and Nakicenovic, Nebojsa and Smith, Steven J. and Rose, Steven K.},
	month = aug,
	year = {2011},
	keywords = {Climate Policy, Force Level, Integrate Assessment Model, Mitigation Scenario, Representative Concentration Pathway},
	pages = {5},
}

@article{riahi_shared_2017,
	title = {The {Shared} {Socioeconomic} {Pathways} and their energy, land use, and greenhouse gas emissions implications: {An} overview},
	volume = {42},
	issn = {0959-3780},
	shorttitle = {The {Shared} {Socioeconomic} {Pathways} and their energy, land use, and greenhouse gas emissions implications},
	url = {http://www.sciencedirect.com/science/article/pii/S0959378016300681},
	doi = {10.1016/j.gloenvcha.2016.05.009},
	abstract = {This paper presents the overview of the Shared Socioeconomic Pathways (SSPs) and their energy, land use, and emissions implications. The SSPs are part of a new scenario framework, established by the climate change research community in order to facilitate the integrated analysis of future climate impacts, vulnerabilities, adaptation, and mitigation. The pathways were developed over the last years as a joint community effort and describe plausible major global developments that together would lead in the future to different challenges for mitigation and adaptation to climate change. The SSPs are based on five narratives describing alternative socio-economic developments, including sustainable development, regional rivalry, inequality, fossil-fueled development, and middle-of-the-road development. The long-term demographic and economic projections of the SSPs depict a wide uncertainty range consistent with the scenario literature. A multi-model approach was used for the elaboration of the energy, land-use and the emissions trajectories of SSP-based scenarios. The baseline scenarios lead to global energy consumption of 400–1200 EJ in 2100, and feature vastly different land-use dynamics, ranging from a possible reduction in cropland area up to a massive expansion by more than 700 million hectares by 2100. The associated annual CO2 emissions of the baseline scenarios range from about 25 GtCO2 to more than 120 GtCO2 per year by 2100. With respect to mitigation, we find that associated costs strongly depend on three factors: (1) the policy assumptions, (2) the socio-economic narrative, and (3) the stringency of the target. The carbon price for reaching the target of 2.6W/m2 that is consistent with a temperature change limit of 2°C, differs in our analysis thus by about a factor of three across the SSP marker scenarios. Moreover, many models could not reach this target from the SSPs with high mitigation challenges. While the SSPs were designed to represent different mitigation and adaptation challenges, the resulting narratives and quantifications span a wide range of different futures broadly representative of the current literature. This allows their subsequent use and development in new assessments and research projects. Critical next steps for the community scenario process will, among others, involve regional and sectoral extensions, further elaboration of the adaptation and impacts dimension, as well as employing the SSP scenarios with the new generation of earth system models as part of the 6th climate model intercomparison project (CMIP6).},
	language = {en},
	urldate = {2019-10-28},
	journal = {Global Environmental Change},
	author = {Riahi, Keywan and van Vuuren, Detlef P. and Kriegler, Elmar and Edmonds, Jae and O’Neill, Brian C. and Fujimori, Shinichiro and Bauer, Nico and Calvin, Katherine and Dellink, Rob and Fricko, Oliver and Lutz, Wolfgang and Popp, Alexander and Cuaresma, Jesus Crespo and Kc, Samir and Leimbach, Marian and Jiang, Leiwen and Kram, Tom and Rao, Shilpa and Emmerling, Johannes and Ebi, Kristie and Hasegawa, Tomoko and Havlik, Petr and Humpenöder, Florian and Da Silva, Lara Aleluia and Smith, Steve and Stehfest, Elke and Bosetti, Valentina and Eom, Jiyong and Gernaat, David and Masui, Toshihiko and Rogelj, Joeri and Strefler, Jessica and Drouet, Laurent and Krey, Volker and Luderer, Gunnar and Harmsen, Mathijs and Takahashi, Kiyoshi and Baumstark, Lavinia and Doelman, Jonathan C. and Kainuma, Mikiko and Klimont, Zbigniew and Marangoni, Giacomo and Lotze-Campen, Hermann and Obersteiner, Michael and Tabeau, Andrzej and Tavoni, Massimo},
	month = jan,
	year = {2017},
	keywords = {Adaptation, Climate change, Community scenarios, Mitigation, RCP, SSP, Shared Socioeconomic Pathways},
	pages = {153--168},
}

@article{wasserman_topological_2017,
	title = {Topological {Data} {Analysis}},
	abstract = {Topological data analysis (TDA) can broadly be described as a collection of data analysis methods that ﬁnd structure in data. These methods include clustering, manifold estimation, nonlinear dimension reduction, mode estimation, ridge estimation and persistent homology. This paper reviews some of these methods.},
	language = {en},
	author = {Wasserman, Larry},
	year = {2017},
	pages = {32},
}

@article{salakhutdinov_learning_2015,
	title = {Learning {Deep} {Generative} {Models}},
	volume = {2},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020120},
	doi = {10.1146/annurev-statistics-010814-020120},
	abstract = {Building intelligent systems that are capable of extracting high-level representations from high-dimensional sensory data lies at the core of solving many artiﬁcial intelligence–related tasks, including object recognition, speech perception, and language understanding. Theoretical and biological arguments strongly suggest that building such systems requires models with deep architectures that involve many layers of nonlinear processing. In this article, we review several popular deep learning models, including deep belief networks and deep Boltzmann machines. We show that (a) these deep generative models, which contain many layers of latent variables and millions of parameters, can be learned efﬁciently, and (b) the learned high-level feature representations can be successfully applied in many application domains, including visual object recognition, information retrieval, classiﬁcation, and regression tasks.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Salakhutdinov, Ruslan},
	month = apr,
	year = {2015},
	pages = {361--385},
}

@article{rousseau_frequentist_2016,
	title = {On the {Frequentist} {Properties} of {Bayesian} {Nonparametric} {Methods}},
	volume = {3},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-041715-033523},
	doi = {10.1146/annurev-statistics-041715-033523},
	abstract = {In this paper, I review the main results on the asymptotic properties of the posterior distribution in nonparametric or high-dimensional models. In particular, I explain how posterior concentration rates can be derived and what we learn from such analysis in terms of the impact of the prior distribution on high-dimensional models. These results concern fully Bayes and empirical Bayes procedures. I also describe some of the results that have been obtained recently in semiparametric models, focusing mainly on the Bernstein–von Mises property. Although these results are theoretical in nature, they shed light on some subtle behaviors of the prior models and sharpen our understanding of the family of functionals that can be well estimated for a given prior model.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Rousseau, Judith},
	month = jun,
	year = {2016},
	pages = {211--231},
}

@article{rougier_climate_2014,
	title = {Climate {Simulators} and {Climate} {Projections}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115652},
	doi = {10.1146/annurev-statistics-022513-115652},
	abstract = {We provide a statistical interpretation of current practice in climate modeling. In this review, we deﬁne weather and climate, clarify the relationship between simulator output and simulator climate, distinguish between a climate simulator and a statistical climate model, provide a statistical interpretation of the ubiquitous practice of anomaly correction along with a substantial generalization (the best-parameter approach), and interpret simulator/data comparisons as posterior predictive checking, including a simple adjustment to allow for double counting. We also discuss statistical approaches to simulator tuning, assessing parametric uncertainty, and responding to unrealistic outputs. We ﬁnish with a more general discussion of larger themes.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Rougier, Jonathan and Goldstein, Michael},
	month = jan,
	year = {2014},
	pages = {103--123},
}

@article{nordhausen_robust_2018,
	title = {Robust {Nonparametric} {Inference}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100247},
	doi = {10.1146/annurev-statistics-031017-100247},
	abstract = {In this article, we provide a personal review of the literature on nonparametric and robust tools in the standard univariate and multivariate location and scatter, as well as linear regression problems, with a special focus on sign and rank methods, their equivariance and invariance properties, and their robustness and efﬁciency. Beyond parametric models, the population quantities of interest are often formulated as location, scatter, skewness, kurtosis and other functionals. Some old and recent tools for model checking, dimension reduction, and subspace estimation in wide semiparametric models are discussed. We also discuss recent extensions of procedures in certain nonstandard semiparametric cases including clustered and matrix-valued data. Our personal list of important unsolved and future issues is provided.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Nordhausen, Klaus and Oja, Hannu},
	month = mar,
	year = {2018},
	pages = {473--500},
}

@article{lehoczky_overview_2018,
	title = {Overview and {History} of {Statistics} for {Equity} {Markets}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100518},
	doi = {10.1146/annurev-statistics-031017-100518},
	abstract = {This article surveys the evolution of stock market trading over a 60-year period. It begins before 1960, when there was no database widely available to conduct a statistical analysis of stock price movements. This changed in the 1960s with the introduction of the Center for Research in Security Prices database. A major ﬁnding was the heavy-tailed nature of stock returns. The 1960s also brought major theoretical developments, including the martingale theory of stock price processes and the efﬁcient market hypothesis. This hypothesis prevailed until the 1990s, when the discovery of market anomalies led to statistical arbitrage strategies. We describe the use of modern machine learning methods, such as AdaBoost and random forests, which can combine some of these strategies into an improved trading strategy. The twenty-ﬁrst century was marked by the rapid evolution of electronic markets and the rise of computer-driven high-frequency trading based on computing technology, low latency access, and limit order book modeling.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Lehoczky, John and Schervish, Mark},
	month = mar,
	year = {2018},
	pages = {265--288},
}

@article{held_p_2018,
	title = {On \textit{p} -{Values} and {Bayes} {Factors}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100307},
	doi = {10.1146/annurev-statistics-031017-100307},
	abstract = {The p-value quantiﬁes the discrepancy between the data and a null hypothesis of interest, usually the assumption of no difference or no effect. A Bayesian approach allows the calibration of p-values by transforming them to direct measures of the evidence against the null hypothesis, so-called Bayes factors. We review the available literature in this area and consider two-sided signiﬁcance tests for a point null hypothesis in more detail. We distinguish simple from local alternative hypotheses and contrast traditional Bayes factors based on the data with Bayes factors based on p-values or test statistics. A well-known ﬁnding is that the minimum Bayes factor, the smallest possible Bayes factor within a certain class of alternative hypotheses, provides less evidence against the null hypothesis than the corresponding p-value might suggest. It is less known that the relationship between p-values and minimum Bayes factors also depends on the sample size and on the dimension of the parameter of interest. We illustrate the transformation of p-values to minimum Bayes factors with two examples from clinical research.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Held, Leonhard and Ott, Manuela},
	month = mar,
	year = {2018},
	pages = {393--419},
}

@article{heinze-deml_causal_2018,
	title = {Causal {Structure} {Learning}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100630},
	doi = {10.1146/annurev-statistics-031017-100630},
	abstract = {Graphical models can represent a multivariate distribution in a convenient and accessible form as a graph. Causal models can be viewed as a special class of graphical models that represent not only the distribution of the observed system but also the distributions under external interventions. They hence enable predictions under hypothetical interventions, which is important for decision making. The challenging task of learning causal models from data always relies on some underlying assumptions. We discuss several recently proposed structure learning algorithms and their assumptions, and we compare their empirical performance under various scenarios.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Heinze-Deml, Christina and Maathuis, Marloes H. and Meinshausen, Nicolai},
	month = mar,
	year = {2018},
	pages = {371--391},
}

@article{heard_agent-based_2015,
	title = {Agent-{Based} {Models} and {Microsimulation}},
	volume = {2},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020218},
	doi = {10.1146/annurev-statistics-010814-020218},
	abstract = {Agent-based models (ABMs) are computational models used to simulate the actions and interactions of agents within a system. Usually, each agent has a relatively simple set of rules for how he or she responds to his or her environment and to other agents. These models are used to gain insight into the emergent behavior of complex systems with many agents, in which the emergent behavior depends upon the micro-level behavior of the individuals. ABMs are widely used in many ﬁelds, and this article reviews some of those applications. However, as relatively little work has been done on statistical inference for such models, this article also points out some of those gaps and recent strategies to address them.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Heard, Daniel and Dent, Gelonia and Schifeling, Tracy and Banks, David},
	month = apr,
	year = {2015},
	pages = {259--272},
}

@article{gneiting_probabilistic_2014,
	title = {Probabilistic {Forecasting}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-062713-085831},
	doi = {10.1146/annurev-statistics-062713-085831},
	abstract = {A probabilistic forecast takes the form of a predictive probability distribution over future quantities or events of interest. Probabilistic forecasting aims to maximize the sharpness of the predictive distributions, subject to calibration, on the basis of the available information set. We formalize and study notions of calibration in a prediction space setting. In practice, probabilistic calibration can be checked by examining probability integral transform (PIT) histograms. Proper scoring rules such as the logarithmic score and the continuous ranked probability score serve to assess calibration and sharpness simultaneously. As a special case, consistent scoring functions provide decision-theoretically coherent tools for evaluating point forecasts. We emphasize methodological links to parametric and nonparametric distributional regression techniques, which attempt to model and to estimate conditional distribution functions; we use the context of statistically postprocessed ensemble forecasts in numerical weather prediction as an example. Throughout, we illustrate concepts and methodologies in data examples.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Gneiting, Tilmann and Katzfuss, Matthias},
	month = jan,
	year = {2014},
	pages = {125--151},
}

@article{fraser_p_2017,
	title = {\textit{p} -{Values}: {The} {Insight} to {Modern} {Statistical} {Inference}},
	volume = {4},
	issn = {2326-8298, 2326-831X},
	shorttitle = {\textit{p} -{Values}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-060116-054139},
	doi = {10.1146/annurev-statistics-060116-054139},
	abstract = {I introduce a p-value function that derives from the continuity inherent in a wide range of regular statistical models. This provides conﬁdence bounds and conﬁdence sets, tests, and estimates that all reﬂect model continuity. The development starts with the scalar-variable scalar-parameter exponential model and extends to the vector-parameter model with scalar interest parameter, then to general regular models, and then references for testing vector interest parameters are available. The procedure does not use sufﬁciency but applies directly to general models, although it reproduces sufﬁciency-based results when sufﬁciency is present. The emphasis is on the coherence of the full procedure, and technical details are not emphasized.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Fraser, D.A.S.},
	month = mar,
	year = {2017},
	pages = {1--14},
}

@article{fienberg_what_2014,
	title = {What {Is} {Statistics}?},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115703},
	doi = {10.1146/annurev-statistics-022513-115703},
	abstract = {One might think that there is a simple answer to the question posed in the title of the form “Statistics is. . . . ” Sadly, there is not, although many contemporary statistical authors have attempted to answer the question. This article captures the essence of some of these efforts, setting them in their historical contexts. In the process, we focus on the cross-disciplinary nature of much modern statistical research. This discussion serves as a backdrop to the the aims of the Annual Review of Statistics and its Application, which begins publication with the present volume.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Fienberg, Stephen E.},
	month = jan,
	year = {2014},
	pages = {1--9},
}

@article{embrechts_statistics_2014,
	title = {Statistics and {Quantitative} {Risk} {Management} for {Banking} and {Insurance}},
	volume = {1},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-022513-115631},
	doi = {10.1146/annurev-statistics-022513-115631},
	abstract = {As an emerging ﬁeld of applied research, quantitative risk management (QRM) poses a lot of challenges for probabilistic and statistical modeling. This review provides a discussion on selected past, current, and possible future areas of research at the intersection of statistics and QRM. Topics treated include the use of risk measures in regulation, including their statistical estimation and aggregation properties. An extensive literature provides the statistically interested reader with an entrance to this exciting ﬁeld.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Embrechts, Paul and Hofert, Marius},
	month = jan,
	year = {2014},
	pages = {493--514},
}

@article{dawid_statistical_2015,
	title = {Statistical {Causality} from a {Decision}-{Theoretic} {Perspective}},
	volume = {2},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-010814-020105},
	doi = {10.1146/annurev-statistics-010814-020105},
	abstract = {We present an overview of the decision-theoretic framework of statistical causality, which is well suited for formulating and solving problems of determining the effects of applied causes. The approach is described in detail, and it is related to and contrasted with other current formulations, such as structural equation models and potential responses. Topics and applications covered include confounding, the effect of treatment on the treated, instrumental variables, and dynamic treatment strategies.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Dawid, A. Philip},
	month = apr,
	year = {2015},
	pages = {273--303},
}

@article{cook_principal_2018,
	title = {Principal {Components}, {Sufficient} {Dimension} {Reduction}, and {Envelopes}},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-031017-100257},
	doi = {10.1146/annurev-statistics-031017-100257},
	abstract = {We review probabilistic principal components, principal ﬁtted components, sufﬁcient dimension reduction, and envelopes, arguing that at their core they are all based on variations of the conditional independence argument that Fisher used to develop his fundamental concept of sufﬁciency. We emphasize the foundations of the methods. Methodological details, derivations, and examples are included when they convey the ﬂavor and implications of basic concepts. In addition to the main topics, this review covers extensions of probabilistic principal components, the central subspace and central mean subspace, sliced inverse regression, sliced average variance estimation, dimension reduction for covariance matrices, and response and predictor envelopes.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Cook, R. Dennis},
	month = mar,
	year = {2018},
	pages = {533--559},
}

@article{cook_data_2016,
	title = {Data {Visualization} and {Statistical} {Graphics} in {Big} {Data} {Analysis}},
	volume = {3},
	issn = {2326-8298, 2326-831X},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-statistics-041715-033420},
	doi = {10.1146/annurev-statistics-041715-033420},
	abstract = {This article discusses the role of data visualization in the process of analyzing big data. We describe the historical origins of statistical graphics, from the birth of exploratory data analysis to the impacts of statistical graphics on practice today. We present examples of contemporary data visualizations in the process of exploring airline trafﬁc, global standardized test scores, election monitoring, Wikipedia edits, the housing crisis as observed in San Francisco, and the mining of credit card databases. We provide a review of recent literature. Good data visualization yields better models and predictions and allows for the discovery of the unexpected.},
	language = {en},
	number = {1},
	urldate = {2019-09-23},
	journal = {Annual Review of Statistics and Its Application},
	author = {Cook, Dianne and Lee, Eun-Kyung and Majumder, Mahbubul},
	month = jun,
	year = {2016},
	pages = {133--159},
}

@article{amati_social_2018,
	title = {Social {Network} {Modeling}},
	abstract = {The development of stochastic models for the analysis of social networks is an important growth area in contemporary statistics. The last few decades have witnessed the rapid development of a variety of statistical models capable of representing the global structure of an observed network in terms of underlying generating mechanisms. The distinctive feature of statistical models for social networks is their ability to represent directly the dependence relations that these mechanisms entail. In this review, we focus on models for single network observations, particularly on the family of exponential random graph models. After deﬁning the models, we discuss issues of model speciﬁcation, estimation and assessment. We then review model extensions for the analysis of other types of network data, provide an empirical example, and give a selective overview of empirical studies that have adopted the basic model and its many variants. We conclude with an outline of the current analytical challenges.},
	language = {en},
	author = {Amati, Viviana and Lomi, Alessandro and Mira, Antonietta},
	year = {2018},
	pages = {30},
}

@article{grenander_stochastic_1950,
	title = {Stochastic processes and statistical inference},
	volume = {1},
	issn = {0004-2080, 1871-2487},
	url = {https://projecteuclid.org/euclid.afm/1485803958},
	doi = {10.1007/BF02590638},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {3},
	urldate = {2019-09-21},
	journal = {Arkiv for Matematik},
	author = {Grenander, Ulf},
	month = oct,
	year = {1950},
	mrnumber = {MR0039202},
	zmnumber = {0041.45807},
	pages = {195--277},
}

@article{borgonovo_sensitivity_2010,
	title = {Sensitivity analysis with finite changes: {An} application to modified {EOQ} models},
	volume = {200},
	issn = {0377-2217},
	shorttitle = {Sensitivity analysis with finite changes},
	url = {http://www.sciencedirect.com/science/article/pii/S0377221708010631},
	doi = {10.1016/j.ejor.2008.12.025},
	abstract = {In this work, we introduce a new method for the sensitivity analysis of model output in the presence of finite changes in one or more of the exogenous variables. We define sensitivity measures that do not rest on differentiability. We relate the sensitivity measures to classical differential and comparative statics indicators. We prove a result that allows us to obtain the sensitivity measures at the same cost of one-variable-at-a-time methods, thus making their estimation feasible also for computationally intensive models. We discuss in detail the derivation of managerial insights formulating a procedure based on the concept of “Settings”. The method is applied to the sensitivity analysis of a discrete change in optimal order quantity following a jump in the exogenous variables of a non-linear programming inventory model.},
	number = {1},
	urldate = {2019-09-21},
	journal = {European Journal of Operational Research},
	author = {Borgonovo, E.},
	month = jan,
	year = {2010},
	keywords = {Comparative statics, Finite change sensitivity indices, Inventory, Modified economic order quantity models, Sensitivity analysis},
	pages = {127--138},
}

@article{tebaldi_use_2007,
	title = {The use of the multi-model ensemble in probabilistic climate projections},
	volume = {365},
	url = {https://royalsocietypublishing.org/doi/abs/10.1098/rsta.2007.2076},
	doi = {10.1098/rsta.2007.2076},
	abstract = {Recent coordinated efforts, in which numerous climate models have been run for a common set of experiments, have produced large datasets of projections of future climate for various scenarios. Those multi-model ensembles sample initial condition, parameter as well as structural uncertainties in the model design, and they have prompted a variety of approaches to quantify uncertainty in future climate in a probabilistic way. This paper outlines the motivation for using multi-model ensembles, reviews the methodologies published so far and compares their results for regional temperature projections. The challenges in interpreting multi-model results, caused by the lack of verification of climate projections, the problem of model dependence, bias and tuning as well as the difficulty in making sense of an ‘ensemble of opportunity’, are discussed in detail.},
	number = {1857},
	urldate = {2019-09-21},
	journal = {Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences},
	author = {Tebaldi, Claudia and Knutti, Reto},
	month = aug,
	year = {2007},
	pages = {2053--2075},
}

@article{abramowicz_nonparametric_2018,
	title = {Nonparametric inference for functional-on-scalar linear models applied to knee kinematic hop data after injury of the anterior cruciate ligament},
	volume = {45},
	copyright = {© 2018 The Authors Scandinavian Journal of Statistics published by John Wiley \& Sons Ltd on behalf of The Board of the Foundation of the Scandinavian Journal of Statistics},
	issn = {1467-9469},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/sjos.12333},
	doi = {10.1111/sjos.12333},
	abstract = {Motivated by the analysis of the dependence of knee movement patterns during functional tasks on subject-specific covariates, we introduce a distribution-free procedure for testing a functional-on-scalar linear model with fixed effects. The procedure does not only test the global hypothesis on the entire domain but also selects the intervals where statistically significant effects are detected. We prove that the proposed tests are provided with an asymptotic control of the intervalwise error rate, that is, the probability of falsely rejecting any interval of true null hypotheses. The procedure is applied to one-leg hop data from a study on anterior cruciate ligament injury. We compare knee kinematics of three groups of individuals (two injured groups with different treatments and one group of healthy controls), taking individual-specific covariates into account.},
	language = {en},
	number = {4},
	urldate = {2019-09-21},
	journal = {Scandinavian Journal of Statistics},
	author = {Abramowicz, Konrad and Häger, Charlotte K. and Pini, Alessia and Schelin, Lina and Luna, Sara Sjöstedt de and Vantini, Simone},
	year = {2018},
	keywords = {analysis of covariance, functional data, human movement, intervalwise testing, permutation test},
	pages = {1036--1061},
}

@book{pesarin_permutation_2010,
	address = {Chichester, UK},
	title = {Permutation {Tests} for {Complex} {Data}},
	isbn = {978-0-470-68951-6 978-0-470-51641-6},
	url = {http://doi.wiley.com/10.1002/9780470689516},
	language = {en},
	urldate = {2019-09-21},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Pesarin, Fortunato and Salmaso, Luigi},
	month = mar,
	year = {2010},
	doi = {10.1002/9780470689516},
}

@article{josset_functional_2015,
	title = {Functional error modeling for uncertainty quantification in hydrogeology},
	volume = {51},
	copyright = {© 2015. American Geophysical Union. All Rights Reserved.},
	issn = {1944-7973},
	url = {https://agupubs.onlinelibrary.wiley.com/doi/abs/10.1002/2014WR016028},
	doi = {10.1002/2014WR016028},
	abstract = {Approximate models (proxies) can be employed to reduce the computational costs of estimating uncertainty. The price to pay is that the approximations introduced by the proxy model can lead to a biased estimation. To avoid this problem and ensure a reliable uncertainty quantification, we propose to combine functional data analysis and machine learning to build error models that allow us to obtain an accurate prediction of the exact response without solving the exact model for all realizations. We build the relationship between proxy and exact model on a learning set of geostatistical realizations for which both exact and approximate solvers are run. Functional principal components analysis (FPCA) is used to investigate the variability in the two sets of curves and reduce the dimensionality of the problem while maximizing the retained information. Once obtained, the error model can be used to predict the exact response of any realization on the basis of the sole proxy response. This methodology is purpose-oriented as the error model is constructed directly for the quantity of interest, rather than for the state of the system. Also, the dimensionality reduction performed by FPCA allows a diagnostic of the quality of the error model to assess the informativeness of the learning set and the fidelity of the proxy to the exact model. The possibility of obtaining a prediction of the exact response for any newly generated realization suggests that the methodology can be effectively used beyond the context of uncertainty quantification, in particular for Bayesian inference and optimization.},
	language = {en},
	number = {2},
	urldate = {2019-09-21},
	journal = {Water Resources Research},
	author = {Josset, L. and Ginsbourger, D. and Lunati, I.},
	year = {2015},
	keywords = {machine learning, proxy model},
	pages = {1050--1068},
}

@article{higdon_computer_2008,
	title = {Computer {Model} {Calibration} {Using} {High}-{Dimensional} {Output}},
	volume = {103},
	issn = {0162-1459},
	url = {https://amstat.tandfonline.com/doi/abs/10.1198/016214507000000888},
	doi = {10.1198/016214507000000888},
	abstract = {This work focuses on combining observations from field experiments with detailed computer simulations of a physical process to carry out statistical inference. Of particular interest here is determining uncertainty in resulting predictions. This typically involves calibration of parameters in the computer simulator as well as accounting for inadequate physics in the simulator. The problem is complicated by the fact that simulation code is sufficiently demanding that only a limited number of simulations can be carried out. We consider applications in characterizing material properties for which the field data and the simulator output are highly multivariate. For example, the experimental data and simulation output may be an image or may describe the shape of a physical object. We make use of the basic framework of Kennedy and O'Hagan. However, the size and multivariate nature of the data lead to computational challenges in implementing the framework. To overcome these challenges, we make use of basis representations (e.g., principal components) to reduce the dimensionality of the problem and speed up the computations required for exploring the posterior distribution. This methodology is applied to applications, both ongoing and historical, at Los Alamos National Laboratory.},
	number = {482},
	urldate = {2019-09-21},
	journal = {Journal of the American Statistical Association},
	author = {Higdon, Dave and Gattiker, James and Williams, Brian and Rightley, Maria},
	month = jun,
	year = {2008},
	pages = {570--583},
}

@book{newman_networks_2018,
	address = {Oxford, New York},
	edition = {Second Edition},
	title = {Networks},
	isbn = {978-0-19-880509-0},
	abstract = {The study of networks, including computer networks, social networks, and biological networks, has attracted enormous interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on an unprecedented scale, and the development of new theoretical tools has allowed us to extract knowledge from networks of many different kinds. The study of networks is broadly interdisciplinary and central developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas.Topics covered include the measurement of networks; methods for analyzing network data, including methods developed in physics, statistics, and sociology; fundamentals of graph theory; computer algorithms; mathematical models of networks, including random graph models and generative models; and theories of dynamical processes taking place on networks.},
	publisher = {Oxford University Press},
	author = {Newman, Mark},
	month = jul,
	year = {2018},
}

@book{newman_networks:_2010,
	title = {Networks: {An} {Introduction}},
	isbn = {978-0-19-159417-5},
	shorttitle = {Networks},
	url = {https://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780199206650.001.0001/acprof-9780199206650},
	abstract = {The scientific study of networks, including computer networks, social networks, and biological networks, has received an enormous amount of interest in the last few years. The rise of the Internet and the wide availability of inexpensive computers have made it possible to gather and analyze network data on a large scale, and the development of a variety of new theoretical tools has allowed us to extract new knowledge from many different kinds of networks. The study of networks is broadly interdisciplinary and important developments have occurred in many fields, including mathematics, physics, computer and information sciences, biology, and the social sciences. This book brings together the most important breakthroughs in each of these fields and presents them in a coherent fashion, highlighting the strong interconnections between work in different areas. Subjects covered include the measurement and structure of networks in many branches of science, methods for analyzing network data, including methods developed in physics, statistics, and sociology, the fundamentals of graph theory, computer algorithms, and spectral methods, mathematical models of networks, including random graph models and generative models, and theories of dynamical processes taking place on networks.},
	language = {en\_US},
	urldate = {2019-09-21},
	publisher = {Oxford University Press},
	author = {Newman, Mark},
	month = mar,
	year = {2010},
}

@article{pini_domain-selective_2018,
	title = {Domain-selective functional analysis of variance for supervised statistical profile monitoring of signal data},
	volume = {67},
	copyright = {© 2017 Royal Statistical Society},
	issn = {1467-9876},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssc.12218},
	doi = {10.1111/rssc.12218},
	abstract = {In many applications, process monitoring has to deal with functional responses, which are also known as profile data. In these scenarios, a relevant industrial problem consists of detecting faults by combining supervised learning with functional data analysis and statistical process monitoring. Supervised learning is usually applied to the whole signal domain, with the aim of discovering the features that are affected by the faults of interest. We explore a different perspective, which consists of performing supervised learning to select inferentially the parts of the signal data that are more informative in terms of underlying fault factors. The procedure is based on a non-parametric domain-selective functional analysis of variance and allows us to identify the specific subintervals where the profile is sensitive to process changes. Benefits achieved by coupling the proposed approach with profile monitoring are highlighted by using a simulation study. We show how applying profile monitoring only to the identified subintervals can reduce the time to detect the out-of-control state of the process. To illustrate its potential in industrial applications, the procedure is applied to remote laser welding, where the main aim is monitoring the gap between the welded plates through the observation of the emission spectra of the welded material.},
	language = {en},
	number = {1},
	urldate = {2019-09-21},
	journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
	author = {Pini, Alessia and Vantini, Simone and Colosimo, Bianca Maria and Grasso, Marco},
	year = {2018},
	keywords = {Design of experiments, Functional data analysis, Intervalwise error rate, Statistical process control},
	pages = {55--81},
}

@misc{noauthor_domainselective_nodate,
	title = {Domain‐selective functional analysis of variance for supervised statistical profile monitoring of signal data - {Pini} - 2018 - {Journal} of the {Royal} {Statistical} {Society}: {Series} {C} ({Applied} {Statistics}) - {Wiley} {Online} {Library}},
	url = {https://rss.onlinelibrary.wiley.com/doi/full/10.1111/rssc.12218},
	urldate = {2019-09-21},
}

@misc{noauthor_permutation_nodate,
	title = {Permutation {Tests} for {Complex} {Data}},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/9780470689516},
	language = {en},
	urldate = {2019-09-21},
	journal = {Wiley Online Library},
	doi = {10.1002/9780470689516},
}

@misc{noauthor_analysis_nodate,
	title = {Analysis of {Variance} for {Functional} {Data}},
	url = {https://www.crcpress.com/Analysis-of-Variance-for-Functional-Data/Zhang/p/book/9781439862735},
	abstract = {Despite research interest in functional data analysis in the last three decades, few books are available on the subject. Filling this gap, Analysis of Variance for Functional Data presents up-to-date hypothesis testing methods for functional data analysis. The book covers the reconstruction of funct},
	language = {en},
	urldate = {2019-09-21},
	journal = {CRC Press},
}

@article{chiou_linear_2014,
	title = {Linear manifold modelling of multivariate functional data},
	volume = {76},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12038},
	doi = {10.1111/rssb.12038},
	abstract = {Multivariate functional data are increasingly encountered in data analysis, whereas statistical models for such data are not well developed yet. Motivated by a case‐study where one aims to quantify t...},
	language = {en},
	number = {3},
	urldate = {2019-09-21},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Chiou, Jeng-Min and Müller, Hans-Georg},
	month = jun,
	year = {2014},
	pages = {605--626},
}

@book{horvath_inference_2012,
	address = {New York},
	series = {Springer {Series} in {Statistics}},
	title = {Inference for {Functional} {Data} with {Applications}},
	isbn = {978-1-4614-3654-6},
	url = {https://www.springer.com/gp/book/9781461436546},
	abstract = {This book presents recently developed statistical methods and theory required for the application of the tools of functional data analysis to problems arising in geosciences, finance, economics and biology. It is concerned with inference based on second order statistics, especially those related to the functional principal component analysis. While it covers inference for independent and identically distributed functional data, its distinguishing feature is an in depth coverage of dependent functional data structures, including functional time series and spatially indexed functions. Specific inferential problems studied include two sample inference, change point analysis, tests for dependence in data and model residuals and functional prediction. All procedures are described algorithmically, illustrated on simulated and real data sets, and supported by a complete asymptotic theory. The book can be read at two levels. Readers interested primarily in methodology will find detailed descriptions of the methods and examples of their application. Researchers interested also in mathematical foundations will find carefully developed theory. The organization of the chapters makes it easy for the reader to choose an appropriate focus. The book introduces the requisite, and frequently used, Hilbert space formalism in a systematic manner. This will be useful to graduate or advanced undergraduate students seeking a self-contained introduction to the subject. Advanced researchers will find novel asymptotic arguments.},
	language = {en},
	urldate = {2019-09-21},
	publisher = {Springer-Verlag},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
}

@incollection{horvath_spatially_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Spatially distributed functional data},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_17},
	abstract = {Chapters 13, 14 and 16 focused on functional time series. The present chapter and Chapter 18 deal with curves observed at spatial locations. The data consist of curves {\textbackslash}(X({\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{k\} ; t) {\textbackslash}in [0, 1]{\textbackslash}) observed at spatial locations {\textbackslash}({\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{1\}, {\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{2\}, {\textbackslash}ldots, {\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{N\}{\textbackslash}). We propose methods for the estimation of the mean function and the FPC’s for such data. We also develop a significance test for the correlation of two such functional spatial fields.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_17},
	keywords = {Covariance Tensor, Empirical Variogram, Finite Sample Performance, Functional Data, Spatial Statistic},
	pages = {343--374},
}

@incollection{horvath_tests_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Tests for error correlation in the functional linear model},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_11},
	abstract = {In this chapter, we consider two tests for error correlation in the fully functional linear model, which we call Methods I and II They complement the tools described in Section 8.6 and the graphical goodness of fit checks used in Chapter 9. To construct the test statistics, finite dimensional residuals are computed in two different ways, and then their autocorrelations are suitably defined. From these autocorrelation matrices, two quadratic forms are constructed whose limiting distribution are chi–squared with known numbers of degrees of freedom (different for the two forms). The test statistics can be relatively easily computed using the R package fda.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_11},
	keywords = {Brownian Bridge, Brownian Motion, Error Correlation, Functional Principal Component, Magnetometer Data},
	pages = {191--224},
}

@incollection{horvath_consistency_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Consistency of the simple mean and the empirical functional principal components for spatially distributed curves},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_18},
	abstract = {In this chapter, we continue to study functional data that consist of curves {\textbackslash}(X({\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{k\} ; t), t {\textbackslash}in [0, 1]{\textbackslash}) observed at spatial points {\textbackslash}({\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{1\}, {\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{2\}, {\textbackslash}ldots, {\textbackslash}mathbf\{{\textbackslash}mathrm\{S\}\}\_\{N\}{\textbackslash}). In Chapter 17, we have seen that in this context the simple sample average and the EFPC’s are not the optimal estimators of their population counterparts, and that better estimators can be constructed by using weighted averages.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_18},
	keywords = {Consistent Estimation, Covariance Operator, Functional Principal Component, Functional Time Series, Random Sampling Design},
	pages = {375--403},
}

@incollection{horvath_functional_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Functional linear models},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_8},
	abstract = {In this chapter we review some important ideas related to the functional linear model. Like its multivariate counterpart, this model has been developed in various directions, and has been found to be extremely useful in a broad range of applications. The relevant research is very rich and multifaceted, and we do not aim at a full review of the very extensive literature on this subject. Our objective in this chapter is to explain briefly the general ideas and point to some recent advances. Some additional references are given in Section 8.7. Our choice of topics is partially motivated by the the methodology presented in Chapters 9, 11 and 10. Practically all inferential tool for the functional linear model have been developed under the assumption that the regressor/response pairs, (X i , Y i ), are independent. They must therefore be applied with care to functional data obtained over time or space.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_8},
	keywords = {Approximate Identity, Bibliographical Note, Functional Model, Reproduce Kernel Hilbert Space, Standard Linear Model},
	pages = {127--145},
}

@incollection{horvath_change_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Change point detection in the functional autoregressive process},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_14},
	abstract = {In this chapter, we develop a change point test for the FAR(1) model introduced in Chapter 13. The importance of change point testing was discussed in Chapter 6. Failure to take change points into account leads to spurious inference. This chapter is based on the work of Horváth et al. (2010). Zhang et al. (2011) proposed a self–normalized statistic to solve the problem discussed in this chapter. Self–normalized statistics are discussed in Section 16.6.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_14},
	keywords = {Brownian Bridge, Change Point, Change Point Detection, Ergodic Sequence, Zero Innovation},
	pages = {253--276},
}

@incollection{horvath_canonical_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Canonical correlation analysis},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_4},
	abstract = {Canonical correlation analysis (CCA) is one of the most important tools of multivariate statistical analysis. Its extension to the functional context is not trivial, and in many ways illustrates the differences between multivariate and functional data. One of the most influential contributions has been made by Leurgans et al. (1993) who showed that smoothing is necessary in order to define the functional canonical correlations meaningfully.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_4},
	keywords = {Canonical Correlation, Canonical Correlation Analysis, Covariance Operator, Global Index, Magnetic Storm},
	pages = {45--63},
}

@incollection{horvath_functional_2012-1,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Functional autoregressive model},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_13},
	abstract = {This chapter studies the functional autoregressive (FAR) process which has found many applications. The theory of autoregressive and more general linear processes in Hilbert and Banach spaces is developed in the monograph of Bosq (2000), on which Sections 13.1 and 13.2 are based. We present only a few selected results which provide an introduction to the central ideas, and are needed in the sequel. Section 13.3 is devoted to prediction by means of the FAR process; some theoretical background is given in Section 13.5.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_13},
	keywords = {Brownian Bridge, Estimate Kernel, Partial Isometry, Trace Class, Trace Class Operator},
	pages = {235--252},
}

@incollection{horvath_determining_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Determining the order of the functional autoregressive model},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_15},
	abstract = {This chapter is concerned with determining the order p in the FAR(p) model \$\$Z\_\{i\} = {\textbackslash}sum{\textbackslash}limits\_\{j = 1\}{\textasciicircum}\{p\}{\textbackslash}phi\_j(Z\_\{i - j\}) + {\textbackslash}varepsilon\_i.\$\$ We describe a testing procedure proposed by Kokoszka and Reimherr (2011). At its core is the representation of the FAR(p) process as a fully functional linear model with dependent regressors.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_15},
	keywords = {Empirical Size, Finite Sample Performance, Functional Observation, Functional Principal Component, Random Subspace},
	pages = {277--288},
}

@incollection{horvath_test_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Test for lack of effect in the functional linear model},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_9},
	abstract = {In this chapter, we study the fully functional linear model (8.1) and test the nullity of the operator {\textbackslash}({\textbackslash}Psi{\textbackslash}), i.e. \$\$H\_\{0\} : {\textbackslash}Psi = 0{\textbackslash} \{{\textbackslash}rm versus\}{\textbackslash} H\_\{4\} : {\textbackslash}Psi {\textbackslash}neq 0.\$\$},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_9},
	keywords = {Brownian Bridge, Empirical Size, Geomagnetic Observatory, Magnetometer Data, Straight Line Regression},
	pages = {147--167},
}

@incollection{horvath_two_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Two sample inference for the mean and covariance functions},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_5},
	abstract = {Due to possibly different FPC’s structures, working with two functional samples may be difficult. An important contribution has been made by Benko et al. (2009) who developed bootstrap procedures for testing the equality of mean functions, the FPC’s, and the eigenspaces spanned by them. In this chapter, we present asymptotic procedures for testing the equality of the means and the covariance operators in two independent samples. Section 5.1 focuses on testing the equality of mean functions. It shows that instead of statistics which have chi–square limits, those that converge to weighted sums of squares of independent standard normals can also be used. In other chapters we focus on statistics converging to chi–square distributions, but analogous versions converging to weighted sums of normals can be readily constructed.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_5},
	keywords = {Brownian Bridge, Covariance Function, Covariance Operator, Nominal Size, Schmidt Operator},
	pages = {65--77},
}

@incollection{horvath_test_2012-1,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {A test of significance in functional quadratic regression},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_12},
	abstract = {The functional quadratic model in which a scalar response, Y n , is paired with a functional predictor, Y n (t), is defined as \$\$Y\_n = {\textbackslash}mu + {\textbackslash}int k(t)X{\textasciicircum}\{c\}\_\{n\}(t)dt + {\textbackslash}iint h(s,t)X{\textasciicircum}\{c\}\_\{n\}(S)X{\textasciicircum}\{c\}\_\{n\}(t)dt ds + {\textbackslash}varepsilon\_\{n\},\$\$},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_12},
	pages = {225--232},
}

@incollection{horvath_detection_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Detection of changes in the mean function},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_6},
	abstract = {In this chapter, we present a methodology for the detection of changes in the mean of functional observations. At its core is a significance test for testing the null hypothesis of a constant functional mean against the alternative of a changing mean. We also show how to locate the change points if the null hypothesis is rejected. Our methodology is readily implemented using the R package fda. The null distribution of the test statistic is asymptotically pivotal with a well-known asymptotic distribution going back to the work of Kiefer (1959).},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_6},
	keywords = {Brownian Bridge, Brownian Motion, Change Point, Change Point Detection, Empirical Size},
	pages = {79--104},
}

@incollection{horvath_functional_2012-2,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Functional principal components},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_3},
	abstract = {This chapter introduces one of the most fundamental concepts of FDA, that of the functional principal components (FPC’s). FPC’s allow us to reduce the dimension of infinitely dimensional functional data to a small finite dimension in an optimal way. In Sections 3.1 and 3.2, we introduce the FPC’s from two angles, as coordinates maximizing variability, and as an optimal orthonormal basis. In Section 3.3, we identify the FPC’s with the eigenfunctions of the covariance operator, and show how its eigenvalues decompose the variance of the functional data. We conclude with Section 3.4 which explains how to compute the FPC’s in the R package fda.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_3},
	keywords = {Covariance Operator, Functional Data, Functional Data Analysis, Functional Object, Schmidt Operator},
	pages = {37--43},
}

@incollection{horvath_functional_2012-3,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Functional data structures},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_1},
	abstract = {Statistics is concerned with obtaining information from observations X 1, X 2, …, X N . The X n can be scalars, vectors or other objects. For example, each X n can be a satellite image, in some spectral bandwidth, of a particular region of the Earth taken at time n. Functional Data Analysis (FDA) is concerned with observations which are viewed as functions defined over some set T. A satellite image processed to show surface temperature can be viewed as a function X defined on a subset T of a sphere, X(t) being the temperature at location t. The value X n (t) is then the temperature at location t at time n. Clearly, due to finite resolution, the values of X n are available only at a finite grid of points, but the temperature does exist at every location, so it is natural to view X n as a function defined over the whole set T.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_1},
	keywords = {Fourier Basis, Functional Data, Functional Data Analysis, Functional Object, Universal Time},
	pages = {1--17},
}

@incollection{horvath_portmanteau_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Portmanteau test of independence},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_7},
	abstract = {Most inferential tools of functional data analysis rely on the assumption of iid functional observations. In designed experiments this assumption can be ensured, but for observational data, especially derived from time series, it requires a verification. In this chapter, based on the paper of Gabrys and Kokoszka (2007), we describe a simple portmanteau test of independence for functional observations whose idea is as follows.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_7},
	keywords = {Asymptotic Distribution, Brownian Bridge, Fourth Moment, Functional Data Analysis, Random Element},
	pages = {105--124},
}

@incollection{horvath_two_2012-1,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Two sample inference for regression kernels},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_10},
	abstract = {In Chapter 5, we studied two sample procedures for the mean function and the covariance operator. This chapter is devoted to testing the equality of the regression operators in two functional linear models.We are concerned with the following problem: We observe two samples: sample 1: {\textbackslash}((X\_i, Y\_i), 1 {\textbackslash}leq i {\textbackslash}leq N{\textbackslash}) and sample 2: {\textbackslash}((X\_\{i\}{\textasciicircum}\{{\textbackslash}ast\}, Y\_\{i\}{\textasciicircum}\{{\textbackslash}ast\}), 1 {\textbackslash}leq j {\textbackslash}leq M{\textbackslash}).},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_10},
	keywords = {Covariance Operator, Functional Response, Magnetometer Data, Regression Kernel, Standard Brownian Motion},
	pages = {169--190},
}

@incollection{horvath_hilbert_2012,
	address = {New York, NY},
	series = {Springer {Series} in {Statistics}},
	title = {Hilbert space model for functional data},
	isbn = {978-1-4614-3655-3},
	url = {https://doi.org/10.1007/978-1-4614-3655-3_2},
	abstract = {In this Chapter we introduce some fundamental concepts of the theory of operators in a Hilbert space, and then focus of the properties of random samples in the space L 2 of square integrable functions. The space L 2 is sufficient to handle most procedures considered in this book. We also present a few technical results that fit into the framework considered in this chapter, and are used in subsequent chapters.},
	language = {en},
	urldate = {2019-09-21},
	booktitle = {Inference for {Functional} {Data} with {Applications}},
	publisher = {Springer New York},
	author = {Horváth, Lajos and Kokoszka, Piotr},
	editor = {Horváth, Lajos and Kokoszka, Piotr},
	year = {2012},
	doi = {10.1007/978-1-4614-3655-3_2},
	keywords = {Compact Operator, Covariance Operator, Functional Data, Hilbert Space, Random Function},
	pages = {21--36},
}

@article{bardsley_change_2017,
	title = {Change point tests in functional factor models with application to yield curves},
	volume = {20},
	copyright = {© 2016 Royal Economic Society.},
	issn = {1368-423X},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ectj.12075},
	doi = {10.1111/ectj.12075},
	abstract = {Motivated by the problem of the detection of a change point in the mean structure of yield curves, we introduce several methods to test the null hypothesis that the mean structure of a time series of curves does not change. The mean structure does not refer merely to the level of the curves, but also to their range and other aspects of their shape, most prominently concavity. The performance of the tests depends on whether possible break points in the error structure, which refers to the random variability in the aspects of the curves listed above, are taken into account or not. If they are not taken into account, then an existing change point in the mean structure may fail to be detected with a large probability. The paper contains a complete asymptotic theory, a simulation study and illustrative data examples, as well as details of the numerical implementation of the testing procedures.},
	language = {en},
	number = {1},
	urldate = {2019-09-21},
	journal = {The Econometrics Journal},
	author = {Bardsley, Patrick and Horváth, Lajos and Kokoszka, Piotr and Young, Gabriel},
	year = {2017},
	keywords = {Change point, Functional time series, Yield curve},
	pages = {86--117},
}

@article{tavakoli_detecting_2016,
	title = {Detecting and {Localizing} {Differences} in {Functional} {Time} {Series} {Dynamics}: {A} {Case} {Study} in {Molecular} {Biophysics}},
	volume = {111},
	issn = {0162-1459},
	shorttitle = {Detecting and {Localizing} {Differences} in {Functional} {Time} {Series} {Dynamics}},
	url = {https://doi.org/10.1080/01621459.2016.1147355},
	doi = {10.1080/01621459.2016.1147355},
	abstract = {Motivated by the problem of inferring the molecular dynamics of DNA in solution, and linking them with its base-pair composition, we consider the problem of comparing the dynamics of functional time series (FTS), and of localizing any inferred differences in frequency and along curvelength. The approach we take is one of Fourier analysis, where the complete second-order structure of the FTS is encoded by its spectral density operator, indexed by frequency and curvelength. The comparison is broken down to a hierarchy of stages: at a global level, we compare the spectral density operators of the two FTS, across frequencies and curvelength, based on a Hilbert–Schmidt criterion; then, we localize any differences to specific frequencies; and, finally, we further localize any differences along the length of the random curves, that is, in physical space. A hierarchical multiple testing approach guarantees control of the averaged false discovery rate over the selected frequencies. In this sense, we are able to attribute any differences to distinct dynamic (frequency) and spatial (curvelength) contributions. Our approach is presented and illustrated by means of a case study in molecular biophysics: how can one use molecular dynamics simulations of short strands of DNA to infer their temporal dynamics at the scaling limit, and probe whether these depend on the sequence encoded in these strands? Supplementary materials for this article are available online.},
	number = {515},
	urldate = {2019-09-21},
	journal = {Journal of the American Statistical Association},
	author = {Tavakoli, Shahin and Panaretos, Victor M.},
	month = jul,
	year = {2016},
	keywords = {DNA minicircle, Functional data, Inverse problem, Multiple comparisons},
	pages = {1020--1035},
}

@article{panaretos_fourier_2013,
	title = {Fourier analysis of stationary time series in function space},
	volume = {41},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1366980558},
	doi = {10.1214/13-AOS1086},
	abstract = {We develop the basic building blocks of a frequency domain framework for drawing statistical inferences on the second-order structure of a stationary sequence of functional data. The key element in such a context is the spectral density operator, which generalises the notion of a spectral density matrix to the functional setting, and characterises the second-order dynamics of the process. Our main tool is the functional Discrete Fourier Transform (fDFT). We derive an asymptotic Gaussian representation of the fDFT, thus allowing the transformation of the original collection of dependent random functions into a collection of approximately independent complex-valued Gaussian random functions. Our results are then employed in order to construct estimators of the spectral density operator based on smoothed versions of the periodogram kernel, the functional generalisation of the periodogram matrix. The consistency and asymptotic law of these estimators are studied in detail. As immediate consequences, we obtain central limit theorems for the mean and the long-run covariance operator of a stationary functional time series. Our results do not depend on structural modelling assumptions, but only functional versions of classical cumulant mixing conditions, and are shown to be stable under discrete observation of the individual curves.},
	language = {EN},
	number = {2},
	urldate = {2019-09-21},
	journal = {The Annals of Statistics},
	author = {Panaretos, Victor M. and Tavakoli, Shahin},
	month = apr,
	year = {2013},
	mrnumber = {MR3099114},
	zmnumber = {1267.62094},
	keywords = {Cumulants, discrete Fourier transform, functional data analysis, functional time series, periodogram operator, spectral density operator, weak dependence},
	pages = {568--603},
}

@inproceedings{menafoglio_universal_2016,
	title = {Universal {Kriging} of functional data: {Trace}-variography vs cross-variography? {Application} to gas forecasting in unconventional shales},
	shorttitle = {Universal {Kriging} of functional data},
	doi = {10.1016/j.spasta.2015.12.003},
	abstract = {Abstract In this paper we investigate the practical and methodological use of Universal Kriging of functional data to predict unconventional shale gas production in undrilled locations from known production data. In Universal Kriging of functional data, two approaches are considered: (1) estimation by means of Cokriging of functional components (Universal Cokriging, UCok), requiring cross-variography and (2) estimation by means of trace-variography (Universal Trace-Kriging, UTrK), which avoids cross-variogram modeling. While theoretically, under known variogram structures, such approaches may be quite equivalent, their practical application implies different estimation procedures and modeling efforts. We investigate these differences from the methodological viewpoint and by means of a real field application in the Barnett shale play. An extensive Monte Carlo study inspired from such real field application is employed to support our conclusions.},
	author = {Menafoglio, Alessandra and Grujic, Ognjen and Caers, Jef},
	year = {2016},
}

@book{aneiros_functional_2017,
	series = {Contributions to {Statistics}},
	title = {Functional {Statistics} and {Related} {Fields}},
	isbn = {978-3-319-55845-5},
	url = {https://www.springer.com/it/book/9783319558455},
	abstract = {This volume collects latest methodological and applied contributions on functional, high-dimensional and other complex data, related statistical models and tools as well as on operator-based statistics. It contains selected and refereed contributions presented at the Fourth International Workshop on Functional and Operatorial Statistics (IWFOS 2017) held in A Coruña, Spain, from 15 to 17 June 2017. The series of IWFOS workshops was initiated by the Working Group on Functional and Operatorial Statistics at the University of Toulouse in 2008. Since then, many of the major advances in functional statistics and related fields have been periodically presented and discussed at the IWFOS workshops.},
	language = {en},
	urldate = {2019-09-21},
	publisher = {Springer International Publishing},
	editor = {Aneiros, Germán and Bongiorno, Enea G. and Cao, Ricardo and Vieu, Philippe},
	year = {2017},
}

@article{wang_functional_2016,
	title = {Functional {Data} {Analysis}},
	volume = {3},
	url = {https://doi.org/10.1146/annurev-statistics-041715-033624},
	doi = {10.1146/annurev-statistics-041715-033624},
	abstract = {With the advance of modern technology, more and more data are being recorded continuously during a time interval or intermittently at several discrete time points. These are both examples of functional data, which has become a commonly encountered type of data. Functional data analysis (FDA) encompasses the statistical methodology for such data. Broadly interpreted, FDA deals with the analysis and theory of data that are in the form of functions. This paper provides an overview of FDA, starting with simple statistical notions such as mean and covariance functions, then covering some core techniques, the most popular of which is functional principal component analysis (FPCA). FPCA is an important dimension reduction tool, and in sparse data situations it can be used to impute functional data that are sparsely observed. Other dimension reduction approaches are also discussed. In addition, we review another core technique, functional linear regression, as well as clustering and classification of functional data. Beyond linear and single- or multiple- index methods, we touch upon a few nonlinear approaches that are promising for certain applications. They include additive and other nonlinear functional regression models and models that feature time warping, manifold learning, and empirical differential equations. The paper concludes with a brief discussion of future directions.},
	number = {1},
	urldate = {2019-09-21},
	journal = {Annual Review of Statistics and Its Application},
	author = {Wang, Jane-Ling and Chiou, Jeng-Min and Müller, Hans-Georg},
	year = {2016},
	pages = {257--295},
}

@article{rao_statistical_1958,
	title = {Some {Statistical} {Methods} for {Comparison} of {Growth} {Curves}},
	volume = {14},
	issn = {0006-341X},
	url = {https://www.jstor.org/stable/2527726},
	doi = {10.2307/2527726},
	number = {1},
	urldate = {2019-09-21},
	journal = {Biometrics},
	author = {Rao, C. Radhakrishna},
	year = {1958},
	pages = {1--17},
}

@article{ramsay_tools_1991,
	title = {Some {Tools} for {Functional} {Data} {Analysis}},
	volume = {53},
	issn = {0035-9246},
	url = {https://www.jstor.org/stable/2345586},
	abstract = {Multivariate data analysis permits the study of observations which are finite sets of numbers, but modern data collection situations can involve data, or the processes giving rise to them, which are functions. Functional data analysis involves infinite dimensional processes and/or data. The paper shows how the theory of L-splines can support generalizations of linear modelling and principal components analysis to samples drawn from random functions. Spline smoothing rests on a partition of a function space into two orthogonal subspaces, one of which contains the obvious or structural components of variation among a set of observed functions, and the other of which contains residual components. This partitioning is achieved through the use of a linear differential operator, and we show how the theory of polynomial splines can be applied more generally with an arbitrary operator and associated boundary constraints. These data analysis tools are illustrated by a study of variation in temperature-precipitation patterns among some Canadian weather-stations.},
	number = {3},
	urldate = {2019-09-21},
	journal = {Journal of the Royal Statistical Society. Series B (Methodological)},
	author = {Ramsay, J. O. and Dalzell, C. J.},
	year = {1991},
	pages = {539--572},
}

@article{ramsay_when_1982,
	title = {When the data are functions},
	volume = {47},
	issn = {1860-0980},
	url = {https://doi.org/10.1007/BF02293704},
	doi = {10.1007/BF02293704},
	abstract = {A datum is often a continuous functionx(t) of a variable such as time observed over some interval. One or more such functions are observed for each subject or unit of observation. The extension of classical data analytic techniques designed forp-variate observations to such data is discussed. The essential step is the expression of the classical problem in the language of functional analysis, after which the extension to functions is a straightforward matter. A schematic device called the duality diagram is a very useful tool for describing an analysis and for suggesting new possibilities. Least squares approximation, descriptive statistics, principal components analysis, and canonical correlation analysis are discussed within this broader framework.},
	language = {en},
	number = {4},
	urldate = {2019-09-21},
	journal = {Psychometrika},
	author = {Ramsay, J. O.},
	month = dec,
	year = {1982},
	keywords = {continuous data, duality diagram, functional analysis},
	pages = {379--396},
}

@article{shen_functional_2014,
	title = {Functional {Data} {Analysis} of {Tree} {Data} {Objects}},
	volume = {23},
	issn = {1061-8600},
	doi = {10.1080/10618600.2013.786943},
	abstract = {Data analysis on non-Euclidean spaces, such as tree spaces, can be challenging. The main contribution of this paper is establishment of a connection between tree data spaces and the well developed area of Functional Data Analysis (FDA), where the data objects are curves. This connection comes through two tree representation approaches, the Dyck path representation and the branch length representation. These representations of trees in Euclidean spaces enable us to exploit the power of FDA to explore statistical properties of tree data objects. A major challenge in the analysis is the sparsity of tree branches in a sample of trees. We overcome this issue by using a tree pruning technique that focuses the analysis on important underlying population structures. This method parallels scale-space analysis in the sense that it reveals statistical properties of tree structured data over a range of scales. The effectiveness of these new approaches is demonstrated by some novel results obtained in the analysis of brain artery trees. The scale space analysis reveals a deeper relationship between structure and age. These methods are the first to find a statistically significant gender difference.},
	language = {eng},
	number = {2},
	journal = {Journal of Computational and Graphical Statistics: A Joint Publication of American Statistical Association, Institute of Mathematical Statistics, Interface Foundation of North America},
	author = {Shen, Dan and Shen, Haipeng and Bhamidi, Shankar and Maldonado, Yolanda Muñoz and Kim, Yongdai and Marron, J. S.},
	year = {2014},
	pmid = {25346588},
	pmcid = {PMC4204425},
	keywords = {DiProPerm, Dyck path, Support tree, Tree pruning, ranch length},
	pages = {418--438},
}

@misc{noauthor_water_nodate,
	title = {Water {Resources} {Research} - {Wiley} {Online} {Library}},
	url = {https://agupubs.onlinelibrary.wiley.com/journal/19447973},
	urldate = {2019-09-21},
}

@article{nordhaus_rolling_1993,
	title = {Rolling the ‘{DICE}’: an optimal transition path for controlling greenhouse gases},
	volume = {15},
	issn = {0928-7655},
	shorttitle = {Rolling the ‘{DICE}’},
	url = {http://www.sciencedirect.com/science/article/pii/092876559390017O},
	doi = {10.1016/0928-7655(93)90017-O},
	abstract = {Economic analyses of efficient policies to slow climate change require combining economic and scientific approaches. The present study presents a dynamic integrated climate-economy (‘DICE’) model. This model can be used to investigate alternative approaches to slowing climate change. Evaluation of five policies suggest that a modest carbon tax would be an efficient approach to slow global warming, while rigid emissions-stabilization approaches would impose significant net economic costs.},
	number = {1},
	urldate = {2019-09-20},
	journal = {Resource and Energy Economics},
	author = {Nordhaus, William D.},
	month = mar,
	year = {1993},
	pages = {27--50},
}

@article{messner_users_1995,
	title = {User's {Guide} for {MESSAGE} {I1}},
	language = {en},
	author = {Messner, Sabine and Strubegger, Manfred},
	year = {1995},
	pages = {160},
}

@article{luderer_door_nodate,
	title = {door for achieving climate targets},
	language = {en},
	author = {Luderer, Gunnar and Pietzcker, Robert C and Bertram, Christoph and Kriegler, Elmar and Edenhofer, Ottmar},
	pages = {16},
}

@article{lotzecampen_global_2008,
	title = {Global food demand, productivity growth, and the scarcity of land and water resources: a spatially explicit mathematical programming approach},
	volume = {39},
	copyright = {© 2008 International Association of Agricultural Economists},
	issn = {1574-0862},
	shorttitle = {Global food demand, productivity growth, and the scarcity of land and water resources},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1574-0862.2008.00336.x},
	doi = {10.1111/j.1574-0862.2008.00336.x},
	abstract = {In the coming decades, an increasing competition for global land and water resources can be expected, due to rising demand for food and bio-energy production, biodiversity conservation, and changing production conditions due to climate change. The potential of technological change in agriculture to adapt to these trends is subject to considerable uncertainty. In order to simulate these combined effects in a spatially explicit way, we present a model of agricultural production and its impact on the environment (MAgPIE). MAgPIE is a mathematical programming model covering the most important agricultural crop and livestock production types in 10 economic regions worldwide at a spatial resolution of three by three degrees, i.e., approximately 300 by 300 km at the equator. It takes regional economic conditions as well as spatially explicit data on potential crop yields and land and water constraints into account and derives specific land-use patterns for each grid cell. Shadow prices for binding constraints can be used to valuate resources for which in many places no markets exist, especially irrigation water. In this article, we describe the model structure and validation. We apply the model to possible future scenarios up to 2055 and derive required rates of technological change (i.e., yield increase) in agricultural production in order to meet future food demand.},
	language = {en},
	number = {3},
	urldate = {2019-09-20},
	journal = {Agricultural Economics},
	author = {Lotze‐Campen, Hermann and Müller, Christoph and Bondeau, Alberte and Rost, Stefanie and Popp, Alexander and Lucht, Wolfgang},
	year = {2008},
	keywords = {Agricultural water use, C61, F15, Global food projections, Land use change, Mathematical programming, Q24, Q25, Spatial modeling, Technological change},
	pages = {325--338},
}

@techreport{de_cian_2008_2009,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {The 2008 {Witch} {Model}: {New} {Model} {Features} and {Baseline}},
	shorttitle = {The 2008 {Witch} {Model}},
	url = {https://papers.ssrn.com/abstract=1512475},
	abstract = {WITCH is an energy-economy-climate model developed by the climate change group at FEEM. The model has been extensively used in the past 3 years for the economic analysis of climate change policies. WITCH is a hybrid top-down economic model with a representation of the energy sector of medium complexity. Two distinguishing features of the WITCH model are the representation of endogenous technological change and the game–theoretic set-up. Technological change is driven by innovation and diffusion processes, both of which feature international spillovers. World countries are grouped in 12 regions which interact with each other in a setting of strategic interdependence. This paper describes the updating of the base year data to 2005 and some new features: the inclusion of non-CO2 greenhouse gases and abatement options, the new specification of low carbon technologies and the inclusion of reducing emissions from deforestation and degradation.},
	language = {en},
	number = {ID 1512475},
	urldate = {2019-09-20},
	institution = {Social Science Research Network},
	author = {De Cian, Enrica and Bosetti, Valentina and Sgobbi, Alessandra and Tavoni, Massimo},
	month = nov,
	year = {2009},
	keywords = {Climate Policy, Hybrid Modelling, Integrated Assessment, Technological Change},
}

@misc{noauthor_economics_nodate,
	title = {Economics in the age of big data},
	url = {https://www.gsb.stanford.edu/faculty-research/publications/economics-age-big-data},
	language = {en},
	urldate = {2019-09-20},
	journal = {Stanford Graduate School of Business},
}

@article{schintler_big_2014,
	title = {Big {Data} for {Policy} {Analysis}: {The} {Good}, {The} {Bad}, and {The} {Ugly}},
	volume = {31},
	copyright = {© 2014 by The Policy Studies Organization},
	issn = {1541-1338},
	shorttitle = {Big {Data} for {Policy} {Analysis}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/ropr.12079},
	doi = {10.1111/ropr.12079},
	abstract = {Big data holds tremendous potential for public policy analysis. At the same time, its use prompts a number of issues related to statistical bias, privacy, equity, and governance, among others. Accordingly, there is a need to formulate, evaluate, and implement policies that not only mitigate the risks, but also maximize the benefits of using big data for policy analysis. This poses a number of challenges, which are highlighted in this essay.},
	language = {en},
	number = {4},
	urldate = {2019-09-20},
	journal = {Review of Policy Research},
	author = {Schintler, Laurie A. and Kulkarni, Rajendra},
	year = {2014},
	keywords = {Big Data, policy analysis, policy assessment, privacy, public participation, public policy, statistics},
	pages = {343--348},
}

@techreport{einav_data_2013,
	type = {Working {Paper}},
	title = {The {Data} {Revolution} and {Economic} {Analysis}},
	url = {http://www.nber.org/papers/w19035},
	abstract = {Many believe that "big data" will transform business, government and other aspects of the economy. In this article we discuss how new data may impact economic policy and economic research. Large-scale administrative datasets and proprietary private sector data can greatly improve the way we measure, track and describe economic activity. They also can enable novel research designs that allow researchers to trace the consequences of different events or policies. We outline some of the challenges in accessing and making use of these data. We also consider whether the big data predictive modeling tools that have emerged in statistics and computer science may prove useful in economics.},
	number = {19035},
	urldate = {2019-09-20},
	institution = {National Bureau of Economic Research},
	author = {Einav, Liran and Levin, Jonathan D},
	month = may,
	year = {2013},
	doi = {10.3386/w19035},
}

@article{hampton_big_2013,
	title = {Big data and the future of ecology},
	volume = {11},
	copyright = {© The Ecological Society of America},
	issn = {1540-9309},
	url = {https://esajournals.onlinelibrary.wiley.com/doi/abs/10.1890/120103},
	doi = {10.1890/120103},
	abstract = {The need for sound ecological science has escalated alongside the rise of the information age and “big data” across all sectors of society. Big data generally refer to massive volumes of data not readily handled by the usual data tools and practices and present unprecedented opportunities for advancing science and informing resource management through data-intensive approaches. The era of big data need not be propelled only by “big science” – the term used to describe large-scale efforts that have had mixed success in the individual-driven culture of ecology. Collectively, ecologists already have big data to bolster the scientific effort – a large volume of distributed, high-value information – but many simply fail to contribute. We encourage ecologists to join the larger scientific community in global initiatives to address major scientific and societal problems by bringing their distributed data to the table and harnessing its collective power. The scientists who contribute such information will be at the forefront of socially relevant science – but will they be ecologists?},
	language = {en},
	number = {3},
	urldate = {2019-09-20},
	journal = {Frontiers in Ecology and the Environment},
	author = {Hampton, Stephanie E. and Strasser, Carly A. and Tewksbury, Joshua J. and Gram, Wendy K. and Budden, Amber E. and Batcheller, Archer L. and Duke, Clifford S. and Porter, John H.},
	year = {2013},
	pages = {156--162},
}

@article{marx_biology:_2013,
	title = {Biology: {The} big challenges of big data},
	volume = {498},
	copyright = {2013 Nature Publishing Group},
	issn = {1476-4687},
	shorttitle = {Biology},
	url = {https://www.nature.com/articles/498255a},
	doi = {10.1038/498255a},
	abstract = {As they grapple with increasingly large data sets, biologists and computer scientists uncork new bottlenecks.},
	language = {en},
	urldate = {2019-09-20},
	journal = {Nature},
	author = {Marx, Vivien},
	month = jun,
	year = {2013},
	pages = {255--260},
}

@article{antoniadis_prediction_2016,
	title = {A prediction interval for a function-valued forecast model: {Application} to load forecasting},
	volume = {32},
	issn = {0169-2070},
	shorttitle = {A prediction interval for a function-valued forecast model},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207015001107},
	doi = {10.1016/j.ijforecast.2015.09.001},
	abstract = {Starting from the information contained in the shape of the load curves, we propose a flexible nonparametric function-valued forecast model called KWF (Kernel + Wavelet + Functional) that is well suited to the handling of nonstationary series. The predictor can be seen as a weighted average of the futures of past situations, where the weights increase with the similarity between the past situations and the actual one. In addition, this strategy also provides simultaneous predictions at multiple horizons. These weights induce a probability distribution that can be used to produce bootstrap pseudo predictions. Prediction intervals are then constructed after obtaining the corresponding bootstrap pseudo prediction residuals. We develop two propositions following the KWF strategy directly, and compare it to two alternative methods that arise from proposals by econometricians. The latter involve the construction of simultaneous prediction intervals using multiple comparison corrections through the control of the family-wise error (FWE) or the false discovery rate. Alternatively, such prediction intervals can be constructed by bootstrapping joint probability regions. In this work, we propose to obtain prediction intervals for the KWF model that are valid simultaneously for the H prediction horizons that correspond to the relevant path forecasts, making a connection between functional time series and the econometricians’ framework.},
	number = {3},
	urldate = {2019-09-19},
	journal = {International Journal of Forecasting},
	author = {Antoniadis, Anestis and Brossat, Xavier and Cugliari, Jairo and Poggi, Jean-Michel},
	month = jul,
	year = {2016},
	keywords = {Functional data, Load forecasting, Nonparametric estimation, Prediction interval},
	pages = {939--947},
}

@article{jacques_functional_2014,
	title = {Functional data clustering: a survey},
	volume = {8},
	issn = {1862-5355},
	shorttitle = {Functional data clustering},
	url = {https://doi.org/10.1007/s11634-013-0158-y},
	doi = {10.1007/s11634-013-0158-y},
	abstract = {Clustering techniques for functional data are reviewed. Four groups of clustering algorithms for functional data are proposed. The first group consists of methods working directly on the evaluation points of the curves. The second groups is defined by filtering methods which first approximate the curves into a finite basis of functions and second perform clustering using the basis expansion coefficients. The third groups is composed of methods which perform simultaneously dimensionality reduction of the curves and clustering, leading to functional representation of data depending on clusters. The last group consists of distance-based methods using clustering algorithms based on specific distances for functional data. A software review as well as an illustration of the application of these algorithms on real data are presented.},
	language = {en},
	number = {3},
	urldate = {2019-09-19},
	journal = {Advances in Data Analysis and Classification},
	author = {Jacques, Julien and Preda, Cristian},
	month = sep,
	year = {2014},
	keywords = {62-07, 62H30, 62M99, Basis expansion, Clustering, Functional data, Functional principal component analysis},
	pages = {231--255},
}

@article{sangalli_case_2009,
	title = {A {Case} {Study} in {Exploratory} {Functional} {Data} {Analysis}: {Geometrical} {Features} of the {Internal} {Carotid} {Artery}},
	volume = {104},
	issn = {0162-1459, 1537-274X},
	shorttitle = {A {Case} {Study} in {Exploratory} {Functional} {Data} {Analysis}},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jasa.2009.0002},
	doi = {10.1198/jasa.2009.0002},
	abstract = {This pilot study is a product of the AneuRisk Project, a scientiﬁc program that aims at evaluating the role of vascular geometry and hemodynamics in the pathogenesis of cerebral aneurysms. By means of functional data analyses, we explore the AneuRisk dataset to highlight the relations between the geometric features of the internal carotid artery, expressed by its radius proﬁle and centerline curvature, and the aneurysm location. After introducing a new similarity index for functional data, we eliminate ancillary variability of vessel radius and curvature proﬁles, through an iterative registration procedure. We then reduce data dimension by means of functional principal components analysis. Finally a quadratic discriminant analysis of functional principal components scores allows us to discriminate patients with aneurysms in diﬀerent districts.},
	language = {en},
	number = {485},
	urldate = {2019-09-19},
	journal = {Journal of the American Statistical Association},
	author = {Sangalli, Laura M. and Secchi, Piercesare and Vantini, Simone and Veneziani, Alessandro},
	month = mar,
	year = {2009},
	pages = {37--48},
}

@article{hron_simplicial_2016,
	title = {Simplicial principal component analysis for density functions in {Bayes} spaces},
	volume = {94},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947315001644},
	doi = {10.1016/j.csda.2015.07.007},
	abstract = {Probability density functions are frequently used to characterize the distributional properties of large-scale database systems. As functional compositions, densities primarily carry relative information. As such, standard methods of functional data analysis (FDA) are not appropriate for their statistical processing. The specific features of density functions are accounted for in Bayes spaces, which result from the generalization to the infinite dimensional setting of the Aitchison geometry for compositional data. The aim is to build up a concise methodology for functional principal component analysis of densities. A simplicial functional principal component analysis (SFPCA) is proposed, based on the geometry of the Bayes space B2 of functional compositions. SFPCA is performed by exploiting the centred log-ratio transform, an isometric isomorphism between B2 and L2 which enables one to resort to standard FDA tools. The advantages of the proposed approach with respect to existing techniques are demonstrated using simulated data and a real-world example of population pyramids in Upper Austria.},
	urldate = {2019-09-19},
	journal = {Computational Statistics \& Data Analysis},
	author = {Hron, K. and Menafoglio, A. and Templ, M. and Hrůzová, K. and Filzmoser, P.},
	month = feb,
	year = {2016},
	keywords = {Bayes spaces, Centred log-ratio transformation, Compositional data, Functional principal component analysis},
	pages = {330--350},
}

@article{schneider_random_1988,
	title = {Random approximation of convex sets*},
	volume = {151},
	issn = {00222720},
	url = {http://doi.wiley.com/10.1111/j.1365-2818.1988.tb04682.x},
	doi = {10.1111/j.1365-2818.1988.tb04682.x},
	language = {en},
	number = {3},
	urldate = {2019-09-17},
	journal = {Journal of Microscopy},
	author = {Schneider, Rolf},
	month = sep,
	year = {1988},
	pages = {211--227},
}

@article{noauthor_rates_nodate,
	title = {Rates of {Convergence} for {Random} {Approximations} of {Convex} {Sets}},
	language = {en},
	pages = {10},
}

@article{penrose_laws_2007,
	title = {Laws of large numbers in stochastic geometry with statistical applications},
	volume = {13},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1194625605},
	doi = {10.3150/07-BEJ5167},
	abstract = {Given n independent random marked d-vectors (points) Xi distributed with a common density, define the measure νn=∑iξi, where ξi is a measure (not necessarily a point measure) which stabilizes; this means that ξi is determined by the (suitably rescaled) set of points near Xi. For bounded test functions f on Rd, we give weak and strong laws of large numbers for νn(f). The general results are applied to demonstrate that an unknown set A in d-space can be consistently estimated, given data on which of the points Xi lie in A, by the corresponding union of Voronoi cells, answering a question raised by Khmaladze and Toronjadze. Further applications are given concerning the Gamma statistic for estimating the variance in nonparametric regression.},
	language = {EN},
	number = {4},
	urldate = {2019-09-16},
	journal = {Bernoulli},
	author = {Penrose, Mathew D.},
	month = nov,
	year = {2007},
	mrnumber = {MR2364229},
	zmnumber = {1143.60013},
	keywords = {Voronoi coverage, law of large numbers, nearest neighbours, nonparametric regression, point process, random measure, stabilization},
	pages = {1124--1150},
}

@article{penrose_local_2011,
	title = {Local {Central} {Limit} {Theorems} in {Stochastic} {Geometry}},
	volume = {16},
	issn = {1083-6489},
	url = {https://projecteuclid.org/euclid.ejp/1464820260},
	doi = {10.1214/EJP.v16-968},
	abstract = {We give a general local central limit theorem for the sum of two independent random variables, one of which satisfies a central limit theorem while the other satisfies a local central limit theorem with the same order variance. We apply this result to various quantities arising in stochastic geometry, including: size of the largest component for percolation on a box; number of components, number of edges, or number of isolated points, for random geometric graphs; covered volume for germ-grain coverage models; number of accepted points for finite-input random sequential adsorption; sum of nearest-neighbour distances for a random sample from a continuous multidimensional distribution.},
	language = {EN},
	urldate = {2019-09-16},
	journal = {Electronic Journal of Probability},
	author = {Penrose, Mathew and Peres, Yuval},
	year = {2011},
	mrnumber = {MR2869414},
	zmnumber = {1245.60032},
	keywords = {Local central limit theorem, nearest neighbours, percolation, random geometric graph, stochastic geometry},
	pages = {2509--2544},
}

@article{whitaker_contour_2013,
	title = {Contour {Boxplots}: {A} {Method} for {Characterizing} {Uncertainty} in {Feature} {Sets} from {Simulation} {Ensembles}},
	volume = {19},
	shorttitle = {Contour {Boxplots}},
	doi = {10.1109/TVCG.2013.143},
	abstract = {Ensembles of numerical simulations are used in a variety of applications, such as meteorology or computational solid mechanics, in order to quantify the uncertainty or possible error in a model or simulation. Deriving robust statistics and visualizing the variability of an ensemble is a challenging task and is usually accomplished through direct visualization of ensemble members or by providing aggregate representations such as an average or pointwise probabilities. In many cases, the interesting quantities in a simulation are not dense fields, but are sets of features that are often represented as thresholds on physical or derived quantities. In this paper, we introduce a generalization of boxplots, called contour boxplots, for visualization and exploration of ensembles of contours or level sets of functions. Conventional boxplots have been widely used as an exploratory or communicative tool for data analysis, and they typically show the median, mean, confidence intervals, and outliers of a population. The proposed contour boxplots are a generalization of functional boxplots, which build on the notion of data depth. Data depth approximates the extent to which a particular sample is centrally located within its density function. This produces a center-outward ordering that gives rise to the statistical quantities that are essential to boxplots. Here we present a generalization of functional data depth to contours and demonstrate methods for displaying the resulting boxplots for two-dimensional simulation data in weather forecasting and computational fluid dynamics.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Whitaker, R. T. and Mirzargar, M. and Kirby, R. M.},
	month = dec,
	year = {2013},
	keywords = {Algorithms, Boxplots, Computational modeling, Computer Graphics, Computer Simulation, Data Interpretation, Statistical, Data visualization, Models, Statistical, Numerical models, Shape analysis, Statistical analysis, Uncertainty, Uncertainty visualization, User-Computer Interface, Weather forecasting, aggregate representations, average probability, band depth, center-outward ordering, computational fluid dynamics, confidence intervals, contour boxplots method, data depth notion, data visualisation, ensemble exploration, ensemble variability visualization, ensemble visualization, functional boxplots, learning (artificial intelligence), mean, median, order statistics, outliers, pointwise probability, robust statistics, simulation ensembles, statistical analysis, uncertainty characterization, uncertainty handling, weather forecasting},
	pages = {2713--2722},
}

@article{efron_r._1998,
	title = {R. {A}. {Fisher} in the 21st century ({Invited} paper presented at the 1996 {R}. {A}. {Fisher} {Lecture})},
	volume = {13},
	issn = {0883-4237, 2168-8745},
	url = {https://projecteuclid.org/euclid.ss/1028905930},
	doi = {10.1214/ss/1028905930},
	abstract = {Fisher is the single most important figure in 20th century statistics. This talk examines his influence on modern statistical thinking, trying to predict how Fisherian we can expect the 21st century to be. Fisher's philosophy is characterized as a series of shrewd compromises between the Bayesian and frequentist viewpoints, augmented by some unique characteristics that are particularly useful in applied problems. Several current research topics are examined with an eye toward Fisherian influence, or the lack of it, and what this portends for future statistical developments. Based on the 1996 Fisher lecture, the article closely follows the text of that talk.},
	language = {en},
	number = {2},
	urldate = {2019-09-11},
	journal = {Statistical Science},
	author = {Efron, Bradley},
	month = may,
	year = {1998},
	mrnumber = {MR1647499},
	zmnumber = {1074.01536},
	keywords = {Bayes, Statistical inference, bootstrap, confidence intervals, empirical Bayes, fiducial, frequentist, model selection},
	pages = {95--122},
}

@article{kuelbs_concerns_2013,
	title = {Concerns with functional depth},
	url = {http://arxiv.org/abs/1310.0045},
	abstract = {We study some problems inherent with certain forms of functional depth, in particular, zero depth and lack of consistency.},
	urldate = {2019-09-09},
	journal = {arXiv:1310.0045 [math]},
	author = {Kuelbs, James and Zinn, Joel},
	month = sep,
	year = {2013},
	note = {arXiv: 1310.0045},
	keywords = {60F05 (Primary) 60F17, 62E20 (Secondary), Mathematics - Probability},
}

@article{sering_depth_nodate,
	title = {Depth {Functions} on {General} {Data} {Spaces}, {II}. {Formulation} and {Maximality}, with {Consideration} of the {Tukey}, {Projection}, {Spatial}, and “{Contour}” {Depths}},
	language = {en},
	author = {Serﬂing, Robert},
	pages = {32},
}

@article{sering_depth_nodate-1,
	title = {Depth {Functions} on {General} {Data} {Spaces}, {I}. {Perspectives}, with {Consideration} of “{Density}” and “{Local}” {Depths}},
	language = {en},
	author = {Serﬂing, Robert},
	pages = {27},
}

@article{lopez-pintado_depth-based_2007,
	title = {Depth-based inference for functional data},
	volume = {51},
	issn = {0167-9473},
	url = {http://www.sciencedirect.com/science/article/pii/S0167947306003872},
	doi = {10.1016/j.csda.2006.10.029},
	abstract = {Robust inference tools for functional data are proposed. They are based on the notion of depth for curves. The ideas of trimmed regions, contours and central regions are extended to functions and their structural properties and asymptotic behavior are studied. Next, a scale curve is introduced to describe dispersion in a sample of functions. The computational burden of these techniques is not heavy, so they are also adequate to analyze high-dimensional data. These inferential methods are applied to several real data sets.},
	number = {10},
	urldate = {2019-08-07},
	journal = {Computational Statistics \& Data Analysis},
	author = {López-Pintado, Sara and Romo, Juan},
	month = jun,
	year = {2007},
	keywords = {Data depth, Functional data, Scale curve, Trimmed regions},
	pages = {4957--4968},
}

@article{zuo_general_2000,
	title = {General notions of statistical depth function},
	volume = {28},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1016218226},
	doi = {10.1214/aos/1016218226},
	abstract = {Statistical depth functions are being formulated ad hoc with increasing popularity in nonparametric inference for multivariate data. Here we introduce several general structures for depth functions, classify many existing examples as special cases, and establish results on the possession, or lack thereof, of four key properties desirable for depth functions in general. Roughly speaking, these properties may be described as: affine invariance, maximality at center, monotonicity relative to deepest point, and vanishing at infinity. This provides a more systematic basis for selection of a depth function. In particular, from these and other considerations it is found that the halfspace depth behaves very well overall in comparison with various competitors.},
	language = {EN},
	number = {2},
	urldate = {2019-08-05},
	journal = {The Annals of Statistics},
	author = {Zuo, Yijun and Serfling, Robert},
	month = apr,
	year = {2000},
	mrnumber = {MR1790005},
	zmnumber = {1106.62334},
	keywords = {Statistical depth functions, halfspace depth, multivariate symmetry, simplicial depth},
	pages = {461--482},
}

@article{dubey_functional_2019,
	title = {Functional {Models} for {Time}-{Varying} {Random} {Objects}},
	url = {http://arxiv.org/abs/1907.10829},
	abstract = {Functional data analysis provides a popular toolbox of functional models for the analysis of samples of random functions that are real-valued. In recent years, samples of time-varying object data such as time-varying networks that are not in a vector space have been increasingly collected. These data can be viewed as elements of a general metric space that lacks local or global linear structure and therefore common approaches that have been used with great success for the analysis of functional data, such as functional principal component analysis, cannot be applied. In this paper we propose metric covariance, a novel association measure for paired object data lying in a metric space (Ω, d) that we use to deﬁne a metric auto-covariance function for a sample of random Ω-valued curves, where Ω generally will not have a vector space or manifold structure. The proposed metric auto-covariance function is non-negative deﬁnite when the squared semimetric d2 is of negative type. Then the eigenfunctions of the linear operator with the auto-covariance function as kernel can be used as building blocks for an object functional principal component analysis for Ω-valued functional data, including time-varying probability distributions, covariance matrices and time-dynamic networks. Analogues of functional principal components for time-varying objects are obtained by applying Fre´ chet means and projections of distance functions of the random object trajectories in the directions of the eigenfunctions, leading to real-valued Fre´ chet scores. Using the notion of generalized Fre´ chet integrals, we construct object functional principal components that lie in the metric space Ω. We establish asymptotic consistency of the sample based estimators for the corresponding population targets under mild metric entropy conditions on Ω and continuity of the Ω-valued random curves. These concepts are illustrated with samples of time-varying probability distributions for human mortality, time-varying covariance matrices derived from trading patterns, and time-varying networks that arise from New York taxi trips.},
	language = {en},
	urldate = {2019-07-30},
	journal = {arXiv:1907.10829 [stat]},
	author = {Dubey, Paromita and Mueller, Hans-Georg},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.10829},
	keywords = {Statistics - Methodology},
}

@article{chowdhury_nonparametric_2019,
	title = {Nonparametric depth and quantile regression for functional data},
	volume = {25},
	issn = {1350-7265},
	url = {https://projecteuclid.org/euclid.bj/1544605251},
	doi = {10.3150/17-BEJ991},
	abstract = {We investigate nonparametric regression methods based on spatial depth and quantiles when the response and the covariate are both functions. As in classical quantile regression for finite dimensional data, regression techniques developed here provide insight into the influence of the functional covariate on different parts, like the center as well as the tails, of the conditional distribution of the functional response. Depth and quantile based nonparametric regression methods are useful to detect heteroscedasticity in functional regression. We derive the asymptotic behavior of the nonparametric depth and quantile regression estimates, which depend on the small ball probabilities in the covariate space. Our nonparametric regression procedures are used to analyze a dataset about the influence of per capita GDP on saving rates for 125 countries, and another dataset on the effects of per capita net disposable income on the sale of cigarettes in some states in the US.},
	language = {EN},
	number = {1},
	urldate = {2019-07-29},
	journal = {Bernoulli},
	author = {Chowdhury, Joydeep and Chaudhuri, Probal},
	month = feb,
	year = {2019},
	zmnumber = {07007212},
	keywords = {Bahadur representation, conditional spread, convergence rates, maximal depth set, spatial depth, spatial quantile},
	pages = {395--423},
}

@article{pini_interval-wise_2017,
	title = {Interval-wise testing for functional data},
	volume = {29},
	issn = {1048-5252},
	url = {https://doi.org/10.1080/10485252.2017.1306627},
	doi = {10.1080/10485252.2017.1306627},
	abstract = {In the framework of null hypothesis significance testing for functional data, we propose a procedure able to select intervals of the domain imputable for the rejection of a null hypothesis. An unadjusted p-value function and an adjusted one are the output of the procedure, namely interval-wise testing. Depending on the sort and level α of type-I error control, significant intervals can be selected by thresholding the two p-value functions at level α. We prove that the unadjusted (adjusted) p-value function point-wise (interval-wise) controls the probability of type-I error and it is point-wise (interval-wise) consistent. To enlighten the gain in terms of interpretation of the phenomenon under study, we applied the interval-wise testing to the analysis of a benchmark functional data set, i.e. Canadian daily temperatures. The new procedure provides insights that current state-of-the-art procedures do not, supporting similar advantages in the analysis of functional data with less prior knowledge.},
	number = {2},
	urldate = {2019-07-29},
	journal = {Journal of Nonparametric Statistics},
	author = {Pini, A. and Vantini, S.},
	month = apr,
	year = {2017},
	keywords = {62G09, 62G10, 62H15, 62H99, Inference, canadian temperatures, domain selection, functional data},
	pages = {407--424},
}

@article{fontana_functional_2019,
	title = {Functional {Data} {Analysis} of high-frequency load curves reveals drivers of residential electricity consumption},
	volume = {14},
	copyright = {All rights reserved},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0218702},
	doi = {10.1371/journal.pone.0218702},
	abstract = {Smart energy meters generate real time, high frequency data which can foster demand management and response of consumers and firms, with potential private and social benefits. However, proper statistical techniques are needed to make sense of this large amount of data and translate them into usable recommendations. Here, we apply Functional Data Analysis (FDA), a novel branch of Statistics that analyses functions—to identify drivers of residential electricity load curves. We evaluate a real time feedback intervention which involved about 1000 Italian households for a period of three years. Results of the FDA modelling reveal, for the first time, daytime-indexed patterns of residential electricity consumption which depend on the ownership of specific clusters of electrical appliances and an overall reduction of consumption after the introduction of real time feedback, unrelated to appliance ownership characteristics.},
	language = {en},
	number = {6},
	urldate = {2019-07-09},
	journal = {PLOS ONE},
	author = {Fontana, Matteo and Tavoni, Massimo and Vantini, Simone},
	editor = {Qiu, Yueming},
	month = jun,
	year = {2019},
	pages = {e0218702},
}

@article{marangoni_sensitivity_2017,
	title = {Sensitivity of projected long-term {CO2} emissions across the {Shared} {Socioeconomic} {Pathways}},
	volume = {7},
	issn = {1758-678X, 1758-6798},
	url = {http://www.nature.com/articles/nclimate3199},
	doi = {10.1038/nclimate3199},
	language = {en},
	number = {2},
	urldate = {2019-06-27},
	journal = {Nature Climate Change},
	author = {Marangoni, G. and Tavoni, M. and Bosetti, V. and Borgonovo, E. and Capros, P. and Fricko, O. and Gernaat, D. E. H. J. and Guivarch, C. and Havlik, P. and Huppmann, D. and Johnson, N. and Karkatsoulis, P. and Keppo, I. and Krey, V. and Ó Broin, E. and Price, J. and van Vuuren, D. P.},
	month = feb,
	year = {2017},
	pages = {113--117},
}

@article{cao_dynamic_nodate,
	title = {Dynamic {Pricing} with {Bayesian} {Demand} {Learning} and {Reference} {Price} {Effect}},
	url = {https://www.sciencedirect.com/science/article/pii/S0377221719305260?dgcid=rss_sd_all},
	doi = {10.1016/j.ejor.2019.06.033},
	abstract = {{\textless}p{\textgreater}In this paper, we consider a seller selling a single product over a finite horizon, with the objective of maximizing the expected total discounted revenue by dynamically adjusting posted prices. One distinct feature of our problem is that the customers' arrival rate is unknown to the seller and will be learned in a Bayesian method. Moreover, arriving customer's purchase behavior is affected by reference price. We formulate this problem as a Bayesian dynamic programming. First, we analyze the structural properties of the optimal revenue function and the optimal pricing policy. We find that the problem can be substantially simplified in the case of sufficient inventory and demand learning can be decoupled from pricing decision. Then, we investigate the value of market size (customers' arrival rate) and the effect of the reference price. Furthermore, we conduct several numerical examples to justify our theoretical results, examine the influence of demand learning, and find that ignoring the reference price effect will lose substantial revenue.{\textless}/p{\textgreater}},
	journal = {European Journal of Operational Research},
	author = {Cao, Ping and Zhao, Nenggui and Wu, Jie},
}

@article{briol_probabilistic_nodate,
	title = {Probabilistic {Integration}: {A} {Role} in {Statistical} {Computation}?},
	url = {https://projecteuclid.org/euclid.ss/1555056025},
	doi = {10.1214/18-sts660},
	abstract = {{\textless}strong{\textgreater}François-Xavier Briol{\textless}/strong{\textgreater}, {\textless}strong{\textgreater}Chris J. Oates{\textless}/strong{\textgreater}, {\textless}strong{\textgreater}Mark Girolami{\textless}/strong{\textgreater}, {\textless}strong{\textgreater}Michael A. Osborne{\textless}/strong{\textgreater}, {\textless}strong{\textgreater}Dino Sejdinovic{\textless}/strong{\textgreater}. {\textless}p{\textgreater}{\textless}br/{\textgreater} 
 A research frontier has emerged in scientific computation, wherein discretisation error is regarded as a source of epistemic uncertainty that can be modelled. This raises several statistical challenges, including the design of statistical methods that enable the coherent propagation of probabilities through a (possibly deterministic) computational work-flow, in order to assess the impact of discretisation error on the computer output. This paper examines the case for probabilistic numerical methods in routine statistical computation. Our focus is on numerical integration, where a probabilistic integrator is equipped with a full distribution over its output that reflects the fact that the integrand has been discretised. Our main technical contribution is to establish, for the first time, rates of posterior contraction for one such method. Several substantial applications are provided for illustration and critical evaluation, including examples from statistical modelling, computer graphics and a computer model for an oil reservoir. 
 {\textless}/p{\textgreater}},
	journal = {Statistical Science},
	author = {Briol, François-Xavier and Oates, Chris J. and Girolami, Mark and Osborne, Michael A. and Sejdinovic, Dino},
}

@article{bucher_detecting_nodate,
	title = {Detecting deviations from second-order stationarity in locally stationary functional time series},
	url = {http://link.springer.com/article/10.1007/s10463-019-00721-7?utm_source=researcher_app&utm_medium=referral&utm_campaign=MKEF_USG_Researcher_inbound},
	doi = {10.1007/s10463-019-00721-7},
	abstract = {A time-domain test for the assumption of second-order stationarity of a functional time series is proposed. The test is based on combining individual cumulative sum tests which are designed to be sensitive to changes in the mean, variance and autocovariance operators, respectively. The combination of their dependent {\textless}em class="EmphasisTypeItalic"{\textgreater}p{\textless}/em{\textgreater} values relies on a joint-dependent block multiplier bootstrap of the individual test statistics. Conditions under which the proposed combined testing procedure is asymptotically valid under stationarity are provided. A procedure is proposed to automatically choose the block length parameter needed for the construction of the bootstrap. The finite-sample behavior of the proposed test is investigated in Monte Carlo experiments, and an illustration on a real data set is provided.},
	journal = {Annals of the Institute of Statistical Mathematics},
	author = {Bücher, Axel and Dette, Holger and Heinrichs, Florian},
}

@article{oliveira_prediction_nodate,
	title = {Prediction properties of optimum response surface designs. ({arXiv}:1906.07500v1 [stat.{ME}])},
	url = {http://arxiv.org/abs/1906.07500},
	doi = {arXiv:1906.07500v1},
	abstract = {Prediction capability is considered an important issue in response surface
methodology. Following the line of argument that a design should have several
desirable properties we have extended an existing compound design criterion to
include prediction properties. Prediction of responses and of differences in
response are considered. Point and interval predictions are allowed for.
Extensions of existing graphical tools for inspecting prediction performances
of the designs in the whole region of experimentation are also introduced. The
methods are illustrated with two examples.},
	journal = {arXiv Methodology},
	author = {Oliveira, Heloisa M. de and Oliveira, Cesar B. A. de and Gilmour, Steven G. and Trinca, Luzia A.},
}

@article{lei_distribution-free_2018,
	title = {Distribution-{Free} {Predictive} {Inference} for {Regression}},
	volume = {113},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2017.1307116},
	doi = {10.1080/01621459.2017.1307116},
	abstract = {We develop a general framework for distribution-free predictive inference in regression, using conformal inference. The proposed methodology allows for the construction of a prediction band for the response variable using any estimator of the regression function. The resulting prediction band preserves the consistency properties of the original estimator under standard assumptions, while guaranteeing finite-sample marginal coverage even when these assumptions do not hold. We analyze and compare, both empirically and theoretically, the two major variants of our conformal framework: full conformal inference and split conformal inference, along with a related jackknife method. These methods offer different tradeoffs between statistical accuracy (length of resulting prediction intervals) and computational efficiency. As extensions, we develop a method for constructing valid in-sample prediction intervals called rank-one-out conformal inference, which has essentially the same computational efficiency as split conformal inference. We also describe an extension of our procedures for producing prediction bands with locally varying length, to adapt to heteroscedasticity in the data. Finally, we propose a model-free notion of variable importance, called leave-one-covariate-out or LOCO inference. Accompanying this article is an R package conformalInference that implements all of the proposals we have introduced. In the spirit of reproducibility, all of our empirical results can also be easily (re)generated using this package.},
	number = {523},
	urldate = {2019-06-03},
	journal = {Journal of the American Statistical Association},
	author = {Lei, Jing and G’Sell, Max and Rinaldo, Alessandro and Tibshirani, Ryan J. and Wasserman, Larry},
	month = jul,
	year = {2018},
	keywords = {Distribution-free, Model misspecification, Prediction band, Regression, Variable importance},
	pages = {1094--1111},
}

@article{vovk_nonparametric_2019,
	title = {Nonparametric predictive distributions based on conformal prediction},
	volume = {108},
	issn = {1573-0565},
	url = {https://doi.org/10.1007/s10994-018-5755-8},
	doi = {10.1007/s10994-018-5755-8},
	abstract = {This paper applies conformal prediction to derive predictive distributions that are valid under a nonparametric assumption. Namely, we introduce and explore predictive distribution functions that always satisfy a natural property of validity in terms of guaranteed coverage for IID observations. The focus is on a prediction algorithm that we call the Least Squares Prediction Machine (LSPM). The LSPM generalizes the classical Dempster–Hill predictive distributions to nonparametric regression problems. If the standard parametric assumptions for Least Squares linear regression hold, the LSPM is as efficient as the Dempster–Hill procedure, in a natural sense. And if those parametric assumptions fail, the LSPM is still valid, provided the observations are IID.},
	language = {en},
	number = {3},
	urldate = {2019-06-03},
	journal = {Machine Learning},
	author = {Vovk, Vladimir and Shen, Jieli and Manokhin, Valery and Xie, Min-ge},
	month = mar,
	year = {2019},
	keywords = {Conformal prediction, Least Squares, Nonparametric regression, Predictive distributions, Regression},
	pages = {445--474},
}

@phdthesis{gianluca_zeni_conformal_2019,
	address = {Milano},
	title = {Conformal {Prediction}: {Theory}, {New} {Challenges} and an {Application} to {Mobility} {Flow} {Data} {Forecasting}},
	school = {Politecnico di Milano},
	author = {{Gianluca Zeni}},
	year = {2019},
}

@article{vovk_cross-conformal_2015,
	title = {Cross-conformal predictors},
	volume = {74},
	issn = {1573-7470},
	url = {https://doi.org/10.1007/s10472-013-9368-4},
	doi = {10.1007/s10472-013-9368-4},
	abstract = {Inductive conformal predictors have been designed to overcome the computational inefficiency exhibited by conformal predictors for many underlying prediction algorithms. Whereas computationally efficient, inductive conformal predictors sacrifice different parts of the training set at different stages of prediction, which affects their informational efficiency. This paper introduces the method of cross-conformal prediction, which is a hybrid of the methods of inductive conformal prediction and cross-validation, and studies its validity and informational efficiency empirically. The computational efficiency of cross-conformal predictors is comparable to that of inductive conformal predictors, and they produce valid predictions in our empirical studies.},
	language = {en},
	number = {1},
	urldate = {2019-06-03},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Vovk, Vladimir},
	month = jun,
	year = {2015},
	keywords = {62G15, 68Q32, 68T05, Conformal predictors, Cross-validation, Inductive conformal predictors, Tolerance regions},
	pages = {9--28},
}

@inproceedings{gammerman_learning_1998,
	address = {San Francisco, CA, USA},
	series = {{UAI}'98},
	title = {Learning by {Transduction}},
	isbn = {978-1-55860-555-8},
	url = {http://dl.acm.org/citation.cfm?id=2074094.2074112},
	abstract = {We describe a method for predicting a classification of an object given classifications of the objects in the training set, assuming that the pairs object/classification are generated by an i.i.d. process from a continuous probability distribution. Our method is a modification of Vapnik's support-vector machine; its main novelty is that it gives not only the prediction itself but also a practicable measure of the evidence found in support of that prediction. We also describe a procedure for assigning degrees of confidence to predictions made by the support vector machine. Some experimental results are presented, and possible extensions of the algorithms are discussed.},
	urldate = {2019-06-03},
	booktitle = {Proceedings of the {Fourteenth} {Conference} on {Uncertainty} in {Artificial} {Intelligence}},
	publisher = {Morgan Kaufmann Publishers Inc.},
	author = {Gammerman, A. and Vovk, V. and Vapnik, V.},
	year = {1998},
	note = {event-place: Madison, Wisconsin},
	pages = {148--155},
}

@article{vovk_cross-conformal_2018,
	title = {Cross-conformal predictive distributions},
	volume = {91},
	abstract = {Conformal predictive systems are a recent modiﬁcation of conformal predictors that output, in regression problems, probability distributions for labels of test observations rather than set predictions. The extra information provided by conformal predictive systems may be useful, e.g., in decision making problems. Conformal predictive systems inherit the relative computational ineﬃciency of conformal predictors. In this paper we discuss two computationally eﬃcient versions of conformal predictive systems, which we call split conformal predictive systems and cross-conformal predictive systems, and discuss their advantages and limitations.},
	language = {en},
	journal = {Proceedings of Machine Learning Research},
	author = {Vovk, Vladimir and Nouretdinov, Ilia and Manokhin, Valery and Gammerman, Alex},
	year = {2018},
	pages = {1--15},
}

@article{liu_multivariate_1999,
	title = {Multivariate analysis by data depth: descriptive statistics, graphics and inference},
	volume = {27},
	issn = {0090-5364, 2168-8966},
	shorttitle = {Multivariate analysis by data depth},
	url = {https://projecteuclid.org/euclid.aos/1018031260},
	doi = {10.1214/aos/1018031260},
	abstract = {A data depth can be used to measure the “depth” or “outlyingness” of a given multivariate sample with respect to its underlying distribution. This leads to a natural center-outward ordering of the sample points. Based on this ordering, quantitative and graphical methods are introduced for analyzing multivariate distributional characteristics such as location, scale, bias, skewness and kurtosis, as well as for comparing inference methods. All graphs are one-dimensional curves in the plane and can be easily visualized and interpreted. A “sunburst plot” is presented as a bivariate generalization of the box-plot. DD-(depth versus depth) plots are proposed and examined as graphical inference tools. Some new diagnostic tools for checking multivariate normality are introduced. One of them monitors the exact rate of growth of the maximum deviation from the mean, while the others examine the ratio of the overall dispersion to the dispersion of a certain central region. The affine invariance property of a data depth also leads to appropriate invariance properties for the proposed statistics and methods.},
	language = {en},
	number = {3},
	urldate = {2019-06-03},
	journal = {The Annals of Statistics},
	author = {Liu, Regina Y. and Parelius, Jesse M. and Singh, Kesar},
	month = jun,
	year = {1999},
	mrnumber = {MR1724033},
	zmnumber = {0984.62037},
	keywords = {\$DD\$-plots, Multivariate descriptive statistics, bias, data depth, depth ordering, depth-\$L\$-statistics, kurtosis, location, multivariate normality, multivariate ordering, scale, skewness, sunburst plots},
	pages = {783--858},
}

@article{liu_notion_1988,
	title = {On a notion of simplicial depth},
	volume = {85},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/85/6/1732},
	doi = {10.1073/pnas.85.6.1732},
	abstract = {For a distribution F on Rp and a point x in Rp the simplicial depth D(x), which is the probability that x be inside a random simplex whose vertices are p + 1 independent observations from F, is introduced. D(x) can be viewed as a measure of depth of the point x relative to F, and its empirical version gives rise to a natural ordering of the data points from the center outward. This ordering provides an approach for detecting outliers in a multivariate data cloud and leads to the introduction of affine equivariant multivariate generalizations of the univariate sample median and L-statistics. This sample median is shown to be consistent for the center of any angularly symmetric distribution.},
	language = {en},
	number = {6},
	urldate = {2019-06-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Liu, Regina Y.},
	month = mar,
	year = {1988},
	pmid = {16578830},
	pages = {1732--1734},
}

@article{shafer_tutorial_2008,
	title = {A {Tutorial} on {Conformal} {Prediction}},
	volume = {9},
	issn = {1532-4435},
	url = {http://dl.acm.org/citation.cfm?id=1390681.1390693},
	abstract = {Conformal prediction uses past experience to determine precise levels of confidence in new predictions. Given an error probability ε, together with a method that makes a prediction ŷ of a label y, it produces a set of labels, typically containing ŷ, that also contains y with probability 1 – ε. Conformal prediction can be applied to any method for producing ŷ: a nearest-neighbor method, a support-vector machine, ridge regression, etc. Conformal prediction is designed for an on-line setting in which labels are predicted successively, each one being revealed before the next is predicted. The most novel and valuable feature of conformal prediction is that if the successive examples are sampled independently from the same distribution, then the successive predictions will be right 1 – ε of the time, even though they are based on an accumulating data set rather than on independent data sets. In addition to the model under which successive examples are sampled independently, other on-line compression models can also use conformal prediction. The widely used Gaussian linear model is one of these. This tutorial presents a self-contained account of the theory of conformal prediction and works through several numerical examples. A more comprehensive treatment of the topic is provided in Algorithmic Learning in a Random World, by Vladimir Vovk, Alex Gammerman, and Glenn Shafer (Springer, 2005).},
	urldate = {2019-06-03},
	journal = {J. Mach. Learn. Res.},
	author = {Shafer, Glenn and Vovk, Vladimir},
	month = jun,
	year = {2008},
	pages = {371--421},
}

@article{vovk_universal_2017,
	title = {Universal probability-free prediction},
	volume = {81},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-017-9547-9},
	doi = {10.1007/s10472-017-9547-9},
	language = {en},
	number = {1-2},
	urldate = {2017-12-13},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Vovk, Vladimir and Pavlovic, Dusko},
	month = oct,
	year = {2017},
	pages = {47--70},
}

@article{hebiri_sparse_2010,
	title = {Sparse conformal predictors},
	volume = {20},
	issn = {0960-3174, 1573-1375},
	url = {https://link.springer.com/article/10.1007/s11222-009-9167-2},
	doi = {10.1007/s11222-009-9167-2},
	abstract = {Conformal predictors, introduced by Vovk et al. (Algorithmic Learning in a Random World, Springer, New York, 2005), serve to build prediction intervals by exploiting a notion of conformity of the new data point with previously observed data. We propose a novel method for constructing prediction intervals for the response variable in multivariate linear models. The main emphasis is on sparse linear models, where only few of the covariates have significant influence on the response variable even if the total number of covariates is very large. Our approach is based on combining the principle of conformal prediction with the ℓ1 penalized least squares estimator (LASSO). The resulting confidence set depends on a parameter ε{\textgreater}0 and has a coverage probability larger than or equal to 1−ε. The numerical experiments reported in the paper show that the length of the confidence set is small. Furthermore, as a by-product of the proposed approach, we provide a data-driven procedure for choosing the LASSO penalty. The selection power of the method is illustrated on simulated and real data.},
	language = {en},
	number = {2},
	urldate = {2017-12-18},
	journal = {Statistics and Computing},
	author = {Hebiri, Mohamed},
	month = apr,
	year = {2010},
	pages = {253--266},
}

@inproceedings{papadopoulos_inductive_2002,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Inductive {Confidence} {Machines} for {Regression}},
	isbn = {978-3-540-44036-9 978-3-540-36755-0},
	url = {https://link.springer.com/chapter/10.1007/3-540-36755-1_29},
	doi = {10.1007/3-540-36755-1-29},
	abstract = {The existing methods of predicting with confidence give good accuracy and confidence values, but quite often are computationally inefficient. Some partial solutions have been suggested in the past. Both the original method and these solutions were based on transductive inference. In this paper we make a radical step of replacing transductive inference with inductive inference and define what we call the Inductive Confidence Machine (ICM); our main concern in this paper is the use of ICM in regression problems. The algorithm proposed in this paper is based on the Ridge Regression procedure (which is usually used for outputting bare predictions) and is much faster than the existing transductive techniques. The inductive approach described in this paper may be the only option available when dealing with large data sets.},
	language = {en},
	urldate = {2017-12-18},
	booktitle = {Machine {Learning}: {ECML} 2002},
	publisher = {Springer, Berlin, Heidelberg},
	author = {Papadopoulos, Harris and Proedrou, Kostas and Vovk, Volodya and Gammerman, Alex},
	month = aug,
	year = {2002},
	pages = {345--356},
}

@article{papadopoulos_guest_2015,
	title = {Guest editors’ preface to the special issue on conformal prediction and its applications},
	volume = {74},
	issn = {1012-2443, 1573-7470},
	url = {http://link.springer.com/10.1007/s10472-014-9429-3},
	doi = {10.1007/s10472-014-9429-3},
	language = {en},
	number = {1-2},
	urldate = {2017-12-12},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Papadopoulos, Harris and Vovk, Vladimir and Gammerman, Alexander},
	month = jun,
	year = {2015},
	pages = {1--7},
}

@article{burnaev_efficiency_2014,
	title = {Efficiency of conformalized ridge regression},
	url = {http://arxiv.org/abs/1404.2083},
	abstract = {Conformal prediction is a method of producing prediction sets that can be applied on top of a wide range of prediction algorithms. The method has a guaranteed coverage probability under the standard IID assumption regardless of whether the assumptions (often considerably more restrictive) of the underlying algorithm are satisfied. However, for the method to be really useful it is desirable that in the case where the assumptions of the underlying algorithm are satisfied, the conformal predictor loses little in efficiency as compared with the underlying algorithm (whereas being a conformal predictor, it has the stronger guarantee of validity). In this paper we explore the degree to which this additional requirement of efficiency is satisfied in the case of Bayesian ridge regression; we find that asymptotically conformal prediction sets differ little from ridge regression prediction intervals when the standard Bayesian assumptions are satisfied.},
	urldate = {2017-12-18},
	journal = {arXiv:1404.2083 [cs, stat]},
	author = {Burnaev, Evgeny and Vovk, Vladimir},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.2083},
	keywords = {Computer Science - Learning, Statistics - Machine Learning},
}

@article{lei_distribution-free_2013,
	title = {Distribution-{Free} {Prediction} {Sets}},
	volume = {108},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2012.751873},
	doi = {10.1080/01621459.2012.751873},
	abstract = {This article introduces a new approach to prediction by bringing together two different nonparametric ideas: distribution-free inference and nonparametric smoothing. Specifically, we consider the problem of constructing nonparametric tolerance/prediction sets. We start from the general conformal prediction approach, and we use a kernel density estimator as a measure of agreement between a sample point and the underlying distribution. The resulting prediction set is shown to be closely related to plug-in density level sets with carefully chosen cutoff values. Under standard smoothness conditions, we get an asymptotic efficiency result that is near optimal for a wide range of function classes. But the coverage is guaranteed whether or not the smoothness conditions hold and regardless of the sample size. The performance of our method is investigated through simulation studies and illustrated in a real data example.},
	number = {501},
	urldate = {2017-12-12},
	journal = {Journal of the American Statistical Association},
	author = {Lei, Jing and Robins, James and Wasserman, Larry},
	month = mar,
	year = {2013},
	keywords = {Conformal prediction, Consistency, Density level sets, Finite sample, Kernel density},
	pages = {278--287},
}

@article{chen_discretized_2017,
	title = {Discretized conformal prediction for efficient distribution-free inference},
	url = {http://arxiv.org/abs/1709.06233},
	abstract = {In regression problems where there is no known true underlying model, conformal prediction methods enable prediction intervals to be constructed without any assumptions on the distribution of the underlying data, except that the training and test data are assumed to be exchangeable. However, these methods bear a heavy computational cost-and, to be carried out exactly, the regression algorithm would need to be fitted infinitely many times. In practice, the conformal prediction method is run by simply considering only a finite grid of finely spaced values for the response variable. This paper develops discretized conformal prediction algorithms that are guaranteed to cover the target value with the desired probability, and that offer a tradeoff between computational cost and prediction accuracy.},
	urldate = {2017-12-19},
	journal = {arXiv:1709.06233 [stat]},
	author = {Chen, Wenyu and Chun, Kelli-Jean and Barber, Rina Foygel},
	month = sep,
	year = {2017},
	note = {arXiv: 1709.06233},
	keywords = {Statistics - Methodology},
}

@article{lei_classification_2014,
	title = {Classification with confidence},
	volume = {101},
	issn = {0006-3444},
	url = {https://academic.oup.com/biomet/article/101/4/755/1776679},
	doi = {10.1093/biomet/asu038},
	abstract = {A framework for classification is developed with a notion of confidence. In this framework, a classifier consists of two tolerance regions in the predictor space, with a specified coverage level for each class. The classifier also produces an ambiguous region where the classification needs further investigation. Theoretical analysis reveals interesting structures of the confidence-ambiguity trade-off, and the optimal solution is characterized by extending the Neyman–Pearson lemma. We provide general estimating procedures, along with rates of convergence, based on estimates of the conditional probabilities. The method can be easily implemented with good robustness, as illustrated through theory, simulation and a data example.},
	number = {4},
	urldate = {2017-12-18},
	journal = {Biometrika},
	author = {Lei, Jing},
	month = dec,
	year = {2014},
	pages = {755--769},
}

@article{sun_functional_2011,
	title = {Functional {Boxplots}},
	volume = {20},
	issn = {1061-8600, 1537-2715},
	url = {http://www.tandfonline.com/doi/abs/10.1198/jcgs.2011.09224},
	doi = {10.1198/jcgs.2011.09224},
	language = {en},
	number = {2},
	urldate = {2019-05-28},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Sun, Ying and Genton, Marc G.},
	month = jan,
	year = {2011},
	pages = {316--334},
}

@article{gurevich_test_nodate,
	title = {Test statistics and p-values},
	language = {en},
	author = {Gurevich, Yuri and Vovk, Vladimir},
	pages = {24},
}

@article{vovk_universally_nodate,
	title = {Universally consistent predictive distributions},
	language = {en},
	author = {Vovk, Vladimir},
	pages = {32},
}

@article{vovk_conformal_nodate,
	title = {Conformal predictive decision making},
	language = {en},
	author = {Vovk, Vladimir and Bendtsen, Claus},
	pages = {13},
}

@article{vovk_combining_2018,
	title = {Combining {P}-{Values} {Via} {Averaging}},
	issn = {1556-5068},
	url = {https://www.ssrn.com/abstract=3166304},
	doi = {10.2139/ssrn.3166304},
	language = {en},
	urldate = {2019-05-28},
	journal = {SSRN Electronic Journal},
	author = {Vovk, Vladimir and Wang, Ruodu},
	year = {2018},
}

@article{cai_adaptive_2014,
	title = {Adaptive {Confidence} {Bands} for {Nonparametric} {Regression} {Functions}},
	volume = {109},
	issn = {0162-1459},
	url = {https://doi.org/10.1080/01621459.2013.879260},
	doi = {10.1080/01621459.2013.879260},
	abstract = {This article proposes a new formulation for the construction of adaptive confidence bands (CBs) in nonparametric function estimation problems. CBs, which have size that adapts to the smoothness of the function while guaranteeing that both the relative excess mass of the function lying outside the band and the measure of the set of points where the function lies outside the band are small. It is shown that the bands adapt over a maximum range of Lipschitz classes. The adaptive CB can be easily implemented in standard statistical software with wavelet support. We investigate the numerical performance of the procedure using both simulated and real datasets. The numerical results agree well with the theoretical analysis. The procedure can be easily modified and used for other nonparametric function estimation models. Supplementary materials for this article are available online.},
	number = {507},
	urldate = {2019-05-28},
	journal = {Journal of the American Statistical Association},
	author = {Cai, T. Tony and Low, Mark and Ma, Zongming},
	month = jul,
	year = {2014},
	pmid = {26269661},
	keywords = {Average coverage, Coverage probability, Excess mass, Lower bounds, Noncovered points, Wavelets, White-noise model},
	pages = {1054--1070},
}

@article{lei_distribution-free_2014,
	title = {Distribution-free prediction bands for non-parametric regression},
	volume = {76},
	copyright = {© 2013 Royal Statistical Society},
	issn = {1467-9868},
	url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/rssb.12021},
	doi = {10.1111/rssb.12021},
	abstract = {We study distribution-free, non-parametric prediction bands with a focus on their finite sample behaviour. First we investigate and develop different notions of finite sample coverage guarantees. Then we give a new prediction band by combining the idea of ‘conformal prediction’ with non-parametric conditional density estimation. The proposed estimator, called COPS (conformal optimized prediction set), always has a finite sample guarantee. Under regularity conditions the estimator converges to an oracle band at a minimax optimal rate. A fast approximation algorithm and a data-driven method for selecting the bandwidth are developed. The method is illustrated in simulated and real data examples.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
	author = {Lei, Jing and Wasserman, Larry},
	year = {2014},
	keywords = {Conformal prediction, Finite sample property, Kernel density, Prediction bands},
	pages = {71--96},
}

@article{donoho_one-sided_1988,
	title = {One-{Sided} {Inference} about {Functionals} of a {Density}},
	volume = {16},
	issn = {0090-5364, 2168-8966},
	url = {https://projecteuclid.org/euclid.aos/1176351045},
	doi = {10.1214/aos/1176351045},
	abstract = {This paper discusses the possibility of truly nonparametric inference about functionals of an unknown density. Examples considered include: discrete functionals, such as the number of modes of a density and the number of terms in the true model; and continuous functionals, such as the optimal bandwidth for kernel density estimates or the widths of confidence intervals for adaptive location estimators. For such functionals it is not generally possible to make two-sided nonparametric confidence statements. However, one-sided nonparametric confidence statements are possible: e.g., "I say with 95\% confidence that the underlying distribution has at least three modes." Roughly, this is because the functionals of interest are semicontinuous with respect to the topology induced by a distribution-free metric. Then a neighborhood procedure can be used. The procedure is to find the minimum value of the functional over a neighborhood of the empirical distribution in function space. If this neighborhood is a nonparametric 1−α1−α1 - {\textbackslash}alpha confidence region for the true distribution, the resulting minimum value lowerbounds the true value with a probability of at least 1−α1−α1 - {\textbackslash}alpha. This lower bound has good asymptotic properties in the high-confidence setting αα{\textbackslash}alpha close to 0.},
	language = {EN},
	number = {4},
	urldate = {2019-05-28},
	journal = {The Annals of Statistics},
	author = {Donoho, David L.},
	month = dec,
	year = {1988},
	mrnumber = {MR964930},
	zmnumber = {0665.62040},
	keywords = {One-sided inference, bandwidth selection, convex functionals, density estimation, multimodality, semicontinuity of statistical functionals, subdifferentiability of statistical functionals},
	pages = {1390--1420},
}

@article{bahadur_nonexistence_1956,
	title = {The {Nonexistence} of {Certain} {Statistical} {Procedures} in {Nonparametric} {Problems}},
	volume = {27},
	issn = {0003-4851, 2168-8990},
	url = {https://projecteuclid.org/euclid.aoms/1177728077},
	doi = {10.1214/aoms/1177728077},
	abstract = {Project Euclid - mathematics and statistics online},
	language = {EN},
	number = {4},
	urldate = {2019-05-28},
	journal = {The Annals of Mathematical Statistics},
	author = {Bahadur, R. R. and Savage, Leonard J.},
	month = dec,
	year = {1956},
	mrnumber = {MR84241},
	zmnumber = {0073.14302},
	pages = {1115--1122},
}

@article{barber_conformal_2019,
	title = {Conformal {Prediction} {Under} {Covariate} {Shift}},
	url = {http://arxiv.org/abs/1904.06019},
	abstract = {We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between these two distributions is known---or, in practice, can be estimated accurately with access to a large set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more generally, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.},
	urldate = {2019-05-28},
	journal = {arXiv:1904.06019 [stat]},
	author = {Barber, Rina Foygel and Candes, Emmanuel J. and Ramdas, Aaditya and Tibshirani, Ryan J.},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.06019},
	keywords = {Statistics - Methodology},
}

@article{lei_conformal_2015,
	title = {A conformal prediction approach to explore functional data},
	volume = {74},
	issn = {1573-7470},
	url = {https://doi.org/10.1007/s10472-013-9366-6},
	doi = {10.1007/s10472-013-9366-6},
	abstract = {This paper applies conformal prediction techniques to compute simultaneous prediction bands and clustering trees for functional data. These tools can be used to detect outliers and clusters. Both our prediction bands and clustering trees provide prediction sets for the underlying stochastic process with a guaranteed finite sample behavior, under no distributional assumptions. The prediction sets are also informative in that they correspond to the high density region of the underlying process. While ordinary conformal prediction has high computational cost for functional data, we use the inductive conformal predictor, together with several novel choices of conformity scores, to simplify the computation. Our methods are illustrated on some real data examples.},
	language = {en},
	number = {1},
	urldate = {2019-05-28},
	journal = {Annals of Mathematics and Artificial Intelligence},
	author = {Lei, Jing and Rinaldo, Alessandro and Wasserman, Larry},
	month = jun,
	year = {2015},
	keywords = {62G15, 62G32, Conformal prediction, Functional data, Gaussian mixture, Prediction sets, Simultaneous bands},
	pages = {29--43},
}

@article{lei_fast_2017,
	title = {Fast {Exact} {Conformalization} of {Lasso} using {Piecewise} {Linear} {Homotopy}},
	url = {http://arxiv.org/abs/1708.00427},
	abstract = {Conformal prediction is a general method that converts almost any point predictor to a prediction set. The resulting set keeps good statistical properties of the original estimator under standard assumptions, and guarantees valid average coverage even when the model is misspecified. A main challenge in applying conformal prediction in modern applications is efficient computation, as it generally requires an exhaustive search over the entire output space. In this paper we develop an exact and computationally efficient conformalization of the Lasso and elastic net. The method makes use of a novel piecewise linear homotopy of the Lasso solution under perturbation of a single input sample point. As a by-product, we provide a simpler and better justified online Lasso algorithm, which may be of independent interest. Our derivation also reveals an interesting accuracy-stability trade-off in conformal inference, which is analogous to the bias-variance trade-off in traditional parameter estimation. The practical performance of the new algorithm is demonstrated using both synthetic and real data examples.},
	urldate = {2019-05-28},
	journal = {arXiv:1708.00427 [stat]},
	author = {Lei, Jing},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.00427},
	keywords = {Conformal Prediction, Conformalization, LASSO, Piecewise Linear Homotopy, Statistics - Computation, Statistics - Methodology},
}