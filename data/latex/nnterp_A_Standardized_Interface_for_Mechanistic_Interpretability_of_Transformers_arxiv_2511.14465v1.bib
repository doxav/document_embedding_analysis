
@article{xiong2024benchmarking,
    title={Benchmarking Retrieval-Augmented Generation for Medicine}, 
    author={Guangzhi Xiong and Qiao Jin and Zhiyong Lu and Aidong Zhang},
    journal={arXiv preprint arXiv:2402.13178},
    year={2024}
}
@misc{nanda2022transformerlens,
    title = {TransformerLens},
    author = {Neel Nanda and Joseph Bloom},
    year = {2022},
    howpublished = {\url{https://github.com/TransformerLensOrg/TransformerLens}},
}

@article{Tran2024Bioinstruct,
    author = {Tran, Hieu and Yang, Zhichao and Yao, Zonghai and Yu, Hong},
    title = "{BioInstruct: instruction tuning of large language models for biomedical natural language processing}",
    journal = {Journal of the American Medical Informatics Association},
    pages = {ocae122},
    year = {2024},
    month = {06},
    issn = {1527-974X},
    doi = {10.1093/jamia/ocae122},
    url = {https://doi.org/10.1093/jamia/ocae122},
    eprint = {https://academic.oup.com/jamia/advance-article-pdf/doi/10.1093/jamia/ocae122/58084577/ocae122.pdf},
}


@misc{chen2024huatuogpto1medicalcomplexreasoning,
      title={HuatuoGPT-o1, Towards Medical Complex Reasoning with LLMs}, 
      author={Junying Chen and Zhenyang Cai and Ke Ji and Xidong Wang and Wanlong Liu and Rongsheng Wang and Jianye Hou and Benyou Wang},
      year={2024},
      eprint={2412.18925},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.18925}, 
}

@inproceedings{
sallinen2025llamameditron,
title={Llama-3-Meditron: An Open-Weight Suite of Medical {LLM}s Based on Llama-3.1},
author={Alexandre Sallinen and Antoni-Joan Solergibert and Michael Zhang and Guillaume Boy{\'e} and Maud Dupont-Roc and Xavier Theimer-Lienhard and Etienne Boisson and Bastien Bernath and Hichem Hadhri and Antoine Tran and Tahseen Rabbani and Trevor Brokowski and Meditron Medical Doctor Working Group and Tim G. J. Rudner and Mary-Anne Hartley},
booktitle={Workshop on Large Language Models and Generative AI for Health at AAAI 2025},
year={2025},
url={https://openreview.net/forum?id=ZcD35zKujO}
}
@article{liu2025prorl,
   author    = {Mingjie Liu and Shizhe Diao and Ximing Lu and Jian Hu and Xin Dong and Yejin Choi and Jan Kautz and Yi Dong},
  title={ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models}, 
  journal   = {arXiv preprint},
  year      = {2025},
  archivePrefix = {arXiv},
  primaryClass = {cs.CL},
  url={https://arxiv.org/abs/2505.24864}, 
}

@article{skean2025layer,
  title={Layer by layer: Uncovering hidden representations in language models},
  author={Skean, Oscar and Arefin, Md Rifat and Zhao, Dan and Patel, Niket and Naghiyev, Jalal and LeCun, Yann and Shwartz-Ziv, Ravid},
  journal={arXiv preprint arXiv:2502.02013},
  year={2025}
}
@inproceedings{shah2023modeldiff,
  title={Modeldiff: A framework for comparing learning algorithms},
  author={Shah, Harshay and Park, Sung Min and Ilyas, Andrew and Madry, Aleksander},
  booktitle={International Conference on Machine Learning},
  pages={30646--30688},
  year={2023},
  organization={PMLR}
}
@misc{neuronpedia,
    title = {Neuronpedia: Interactive Reference and Tooling for Analyzing Neural Networks},
    year = {2023},
    note = {Software available from neuronpedia.org},
    url = {https://www.neuronpedia.org},
    author = {Lin, Johnny}
}

@article{davies2025decoding,
  title={Decoding specialised feature neurons in LLMs with the final projection layer},
  author={Davies, Harry J},
  journal={arXiv preprint arXiv:2501.02688},
  year={2025}
}
@article{zhou2021closer,
  title={A closer look at how fine-tuning changes BERT},
  author={Zhou, Yichu and Srikumar, Vivek},
  journal={arXiv preprint arXiv:2106.14282},
  year={2021}
}
@article{betley2025emergent,
  title={Emergent Misalignment: Narrow finetuning can produce broadly misaligned LLMs},
  author={Betley, Jan and Tan, Daniel and Warncke, Niels and Sztyber-Betley, Anna and Bao, Xuchan and Soto, Mart{\'\i}n and Labenz, Nathan and Evans, Owain},
  journal={arXiv preprint arXiv:2502.17424},
  year={2025}
}
@misc{jermyn2024tanh,
	title={Tanh Penalty in Dictionary Learning},
	author={Jermyn, Adam and Templeton, Adly and Batson, Joshua and Bricken, Trenton},
	howpublished={\url{https://transformer-circuits.pub/2024/feb-update/index.html#:~:text=handle%20dying%20neurons.-,Tanh%20Penalty%20in%20Dictionary%20Learning,-Adam%20Jermyn%2C%20Adly}}, 
	year={2024}
}
@article{zhang2023fine,
  title={Fine-tuning happens in tiny subspaces: Exploring intrinsic task-specific subspaces of pre-trained language models},
  author={Zhang, Zhong and Liu, Bang and Shao, Junming},
  journal={arXiv preprint arXiv:2305.17446},
  year={2023}
}
@article{du2025posttrainingmechinterp,
  title={How Post-Training Reshapes LLMs: A Mechanistic View on Knowledge, Truthfulness, Refusal, and Confidence},
  author={Du, Hongzhe and Li, Weikai and Cai, Min and Saraipour, Karim and Zhang, Zimin and Lakkaraju, Himabindu and Sun, Yizhou and Zhang, Shichang},
  journal={arXiv preprint arXiv:2504.02904},
  year={2025}
}
@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}
@article{karvonensaebench,
  title={SAEBench: A comprehensive benchmark for sparse autoencoders, December 2024},
  author={Karvonen, A and Rager, C and Lin, J and Tigges, C and Bloom, J and Chanin, D and Lau, YT and Farrell, E and Conmy, A and Mc-Dougall, C and others},
    howpublished = {\url{https://github.com/saprmarks/dictionary_learning}},
year={2024}
}

@article{alain2016understanding,
  title={Understanding intermediate layers using linear classifier probes},
  author={Alain, Guillaume and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1610.01644},
  year={2016}
}

@inproceedings{reimers-gurevych-2019-sentence,
    title = "Sentence-{BERT}: Sentence Embeddings using {S}iamese {BERT}-Networks",
    author = "Reimers, Nils  and
      Gurevych, Iryna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    url = "https://aclanthology.org/D19-1410/",
    doi = "10.18653/v1/D19-1410",
    pages = "3982--3992",
}
@article{bing_chat_2023,
   author = {Rose, Kevin},
   title = {A Conversation With Bing’s Chatbot Left Me Deeply Unsettled},
   year = {2023},
   journal = {New York Times},
   note = {Available at: \url{https://www.nytimes.com/2023/02/16/technology/bing-chatbot-microsoft-chatgpt.html} (Accessed: {February 12th, 2025})},
   type = {Newspaper Article}
}
@article{lindsey2024sparse,
  title={Sparse crosscoders for cross-layer features and model diffing},
  author={Lindsey, Jack and Templeton, Adly and Marcus, Jonathan and Conerly, Thomas and Batson, Joshua and Olah, Christopher},
  journal={Transformer Circuits Thread},
  year={2024},
  url={https://transformer-circuits.pub/2024/crosscoders/index.html}
}
@article{mishracrosscodermodeldiff25,
  title={Insights on Crosscoder Model Diffing},
  author={Mishra-Sharma, Siddharth and Bricken, Trenton and Lindsey, Jack and Jermyn, Adam and Marcus, Jonathan and Rivoire, Kelley and Olah, Christopher and Henighan, Thomas},
  journal={Transformer Circuits Thread},
  year={2025},
  url={https://transformer-circuits.pub/2025/crosscoder-diffing-update/index.html}
}
@inproceedings{bussmann2024batchtopk,
  title={BatchTopK Sparse Autoencoders},
  author={Bart Bussmann and Patrick Leask and Neel Nanda},
  booktitle={NeurIPS 2024 Workshop on Scientific Methods for Understanding Deep Learning},
  year={2024},
  url={https://openreview.net/forum?id=d4dpOCqybL}
}
@article{bricken2024stagewise,
  title={Stage-Wise Model Diffing},
  author={Bricken, Trenton and Mishra-Sharma, Siddharth and Marcus, Jonathan and Jermyn, Adam and Olah, Christopher and Rivoire, Kelley and Henighan, Thomas},
  journal={Transformer Circuits Thread},
  year={2024},
  url={https://transformer-circuits.pub/2024/model-diffing/index.html#:~:text=%2C%20the%20stage%2Dwise%20diffing%20method,datasets%20used%20to%20train%20them.}
}
@article{khayatan2025analyzingfinetuningrepresentationshift,
      title={Analyzing Fine-tuning Representation Shift for Multimodal LLMs Steering alignment}, 
      author={Pegah Khayatan and Mustafa Shukor and Jayneel Parekh and Matthieu Cord},
      year={2025},
      eprint={2501.03012},
      journal={arXiv},
      url={https://arxiv.org/abs/2501.03012}, 
}
@article{team2024gemma,
  title={Gemma 2: Improving open language models at a practical size},
  author={Riviere, Morgane and Pathak, Shreya and Sessa, Pier Giuseppe and Hardin, Cassidy and Bhupatiraju, Surya and Hussenot, L{\'e}onard and Mesnard, Thomas and Shahriari, Bobak and Ram{\'e}, Alexandre and others},
  journal={arXiv preprint arXiv:2408.00118},
  year={2024}
}
@article{meinke2025frontiermodelscapableincontext,
      title={Frontier Models are Capable of In-context Scheming}, 
      author={Alexander Meinke and Bronson Schoen and Jérémy Scheurer and Mikita Balesni and Rusheb Shah and Marius Hobbhahn},
      year={2025},
      journal={arXiv},
      url={https://arxiv.org/abs/2412.04984}, 
}
@misc{marks2024dictionarylearning,
   title = {dictionary learning},
   author = {Samuel Marks and Adam Karvonen and Aaron Mueller},
   year = {2024},
   howpublished = {\url{https://github.com/saprmarks/dictionary_learning}},
}
@inproceedings{wu2024pyvene,
    title = "pyvene: A Library for Understanding and Improving {P}y{T}orch Models via Interventions",
    author = "Wu, Zhengxuan and Geiger, Atticus and Arora, Aryaman and Huang, Jing and Wang, Zheng and Goodman, Noah and Manning, Christopher and Potts, Christopher",
    editor = "Chang, Kai-Wei and Lee, Annie and Rajani, Nazneen",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 3: System Demonstrations)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.naacl-demo.16",
    pages = "158--165",
}
@article{fiottokaufman2024nnsightndifdemocratizingaccess,
      title={NNsight and NDIF: Democratizing Access to Foundation Model Internals}, 
      author={Jaden Fiotto-Kaufman and Alexander R Loftus and Eric Todd and Jannik Brinkmann and Caden Juang and Koyena Pal and Can Rager and Aaron Mueller and Samuel Marks and Arnab Sen Sharma and Francesca Lucchetti and Michael Ripa and Adam Belfki and Nikhil Prakash and Sumeet Multani and Carla Brodley and Arjun Guha and Jonathan Bell and Byron Wallace and David Bau},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2407.14561}, 
}
@article{zheng2024lmsyschat1mlargescalerealworldllm,
      title={LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset}, 
      author={Lianmin Zheng and Wei-Lin Chiang and Ying Sheng and Tianle Li and Siyuan Zhuang and Zhanghao Wu and Yonghao Zhuang and Zhuohan Li and Zi Lin and Eric P. Xing and Joseph E. Gonzalez and Ion Stoica and Hao Zhang},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2309.11998}, 
}
@inproceedings{
    rafailov2023direct,
    title={Direct Preference Optimization: {Y}our Language Model is Secretly a Reward Model},
    author={Rafael Rafailov and Archit Sharma and Eric Mitchell and Christopher D Manning and Stefano Ermon and Chelsea Finn},
    booktitle={Thirty-seventh Conference on Neural Information Processing Systems},
    year={2023},
    url={https://openreview.net/forum?id=HPuSIXJaa9}
}
@article{leong2025safeguardedshipsrunaground,
      title={Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region}, 
      author={Chak Tou Leong and Qingyu Yin and Jian Wang and Wenjie Li},
      year={2025},
      eprint={2502.13946},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      journal={arXiv},
      url={https://arxiv.org/abs/2502.13946}, 
}
@inproceedings{Ouyang2024training,
    author = {Ouyang, Long and Wu, Jeff and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll L. and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul and Leike, Jan and Lowe, Ryan},
    title = {Training language models to follow instructions with human feedback},
    year = {2024},
    isbn = {9781713871088},
    address = {Red Hook, NY, USA},
    booktitle = {Proceedings of the 36th International Conference on Neural Information Processing Systems},
    articleno = {2011},
    numpages = {15},
    location = {New Orleans, LA, USA},
    series = {NIPS '22}
}
@article{penedo2023refinedwebdatasetfalconllm,
      title={The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only}, 
      author={Guilherme Penedo and Quentin Malartic and Daniel Hesslow and Ruxandra Cojocaru and Alessandro Cappelli and Hamza Alobeidli and Baptiste Pannier and Ebtesam Almazrouei and Julien Launay},
      year={2023},
      journal={arXiv},
      url={https://arxiv.org/abs/2306.01116}, 
}
@article{ding2023enhancing,
  title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations},
  author={Ding, Ning and Chen, Yulin and Xu, Bokai and Qin, Yujia and Zheng, Zhi and Hu, Shengding and Liu, Zhiyuan and Sun, Maosong and Zhou, Bowen},
  journal={arXiv preprint arXiv:2305.14233},
  year={2023}
}
@inproceedings{
  shah2024investigating,
  title={Investigating Language Model Dynamics using Meta-Tokens},
  author={Shah, Alok N. and Ramji, Keshav and Gupta, Khush and Gaur, Vedant},
  booktitle={Second NeurIPS Workshop on Attributing Model Behavior at Scale},
  year={2024},
  url={https://openreview.net/forum?id=pFjEYaZtZl}
}
@article{
  luo2024jailbreak,
  title={Jailbreak Instruction-Tuned Large Language Models via {MLP} Re-weighting},
  author={Yifan Luo and Zhennan Zhou and Meitan Wang and Bin Dong},
  year={2024},
  journal={OpenReview},
  url={https://openreview.net/forum?id=P5qCqYWD53}
}
@article{
  wang2024loss,
  title={On the loss of context-awareness in general instruction finetuning},
  author={Yihan Wang and Andrew Bai and Nanyun Peng and Cho-Jui Hsieh},
  year={2024},
  journal={OpenReview},
  url={https://openreview.net/forum?id=eDnslTIWSt}
}
@article{golovanevsky2024what,
  author={Michal Golovanevsky and William Rudman and Vedant Palit and Ritambhara Singh and Carsten Eickhoff},
  title={What Do VLMs NOTICE? A Mechanistic Interpretability Pipeline for Noise-free Text-Image Corruption and Evaluation},
  year={2024},
  journal={CoRR},
  volume={abs/2406.16320},
  url={https://doi.org/10.48550/arXiv.2406.16320}
}
@inproceedings{vaswani2017attention,
 author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Attention is All you Need},
 url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
 volume = {30},
 year = {2017}
}
@article{qi2024safetyalignmentjusttokens,
      title={Safety Alignment Should Be Made More Than Just a Few Tokens Deep}, 
      author={Xiangyu Qi and Ashwinee Panda and Kaifeng Lyu and Xiao Ma and Subhrajit Roy and Ahmad Beirami and Prateek Mittal and Peter Henderson},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2406.05946}, 
}
@inproceedings{
  neerudu2023on,
  title={On Robustness of Finetuned Transformer-based {NLP} Models},
  author={Pavan Kalyan Reddy Neerudu and Subba Reddy Oota and mounika marreddy and venkateswara Rao Kagita and Manish Gupta},
  booktitle={The 2023 Conference on Empirical Methods in Natural Language Processing},
  year={2023},
  url={https://openreview.net/forum?id=YWbEDZh5ga}
}
@article{phang2021finetunedtransformersclusterssimilar,
      title={Fine-Tuned Transformers Show Clusters of Similar Representations Across Layers}, 
      author={Jason Phang and Haokun Liu and Samuel R. Bowman},
      year={2021},
      journal={arXiv},
      url={https://arxiv.org/abs/2109.08406}, 
}
@article{paulo2024automaticallyinterpretingmillionsfeatures,
      title={Automatically Interpreting Millions of Features in Large Language Models}, 
      author={Gonçalo Paulo and Alex Mallen and Caden Juang and Nora Belrose},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2410.13928}, 
}
@article{gurarieh2025enhancingautomatedinterpretabilityoutputcentric,
      title={Enhancing Automated Interpretability with Output-Centric Feature Descriptions}, 
      author={Yoav Gur-Arieh and Roy Mayan and Chen Agassy and Atticus Geiger and Mor Geva},
      year={2025},
      journal={arXiv},
      url={https://arxiv.org/abs/2501.08319}, 
}
@article{arora-etal-2018-linear,
    title = "Linear Algebraic Structure of Word Senses, with Applications to Polysemy",
    author = "Arora, Sanjeev  and
      Li, Yuanzhi  and
      Liang, Yingyu  and
      Ma, Tengyu  and
      Risteski, Andrej",
    editor = "Lee, Lillian  and
      Johnson, Mark  and
      Toutanova, Kristina  and
      Roark, Brian",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "6",
    year = "2018",
    address = "Cambridge, MA",
    url = "https://aclanthology.org/Q18-1034/",
    doi = "10.1162/tacl_a_00034",
    pages = "483--495",
}
@inproceedings{
  pochinkov2024extracting,
  title={Extracting Paragraphs from {LLM} Token Activations},
  author={Nicky Pochinkov and Angelo Benoit and Lovkush Agarwal and Zainab Ali Majid and Lucile Ter-Minassian},
  booktitle={MINT: Foundation Model Interventions},
  year={2024},
  url={https://openreview.net/forum?id=4b675AHcqq}
}
@article{wright2024addressing,
  title={Addressing Feature Suppression in {SAEs}},
  author={Wright, Benjamin and Sharkey, Lee},
  journal={LessWrong},
  year={2024},
  url={https://www.lesswrong.com/posts/3JuSjTZyMzaSeTxKk/addressing-feature-suppression-in-saes}
}
@inproceedings{tigges-etal-2024-language,
    title = "Language Models Linearly Represent Sentiment",
    author = "Tigges, Curt and Hollinsworth, Oskar John and
      Geiger, Atticus  and
      Nanda, Neel",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    url = "https://aclanthology.org/2024.blackboxnlp-1.5/",
    doi = "10.18653/v1/2024.blackboxnlp-1.5",
    pages = "58--87",
}
@article{kissane_open_2024,
	title = {Open Source Replication of {Anthropic}’s Crosscoder paper for model-diffing},
	url = {https://www.lesswrong.com/posts/srt6JXsRMtmqAJavD/open-source-replication-of-anthropic-s-crosscoder-paper-for},
	language = {en},
	urldate = {2025-02-06},
	author = {Kissane, Connor and Krzyzanowski, Robert and Conmy, Arthur and Nanda, Neel},
	month = oct,
  journal={LessWrong},
	year = {2024},
	file = {Snapshot:/Users/julian/Zotero/storage/D4F45EXK/open-source-replication-of-anthropic-s-crosscoder-paper-for.html:text/html},
}
@inproceedings{huben2024sparse,
	title={Sparse Autoencoders Find Highly Interpretable Features in Language Models},
	author={Robert Huben and Hoagy Cunningham and Logan Riggs Smith and Aidan Ewart and Lee Sharkey},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=F76bwRSLeK}
}
@article{bricken2023monosemanticity,
	title={Towards Monosemanticity: Decomposing Language Models With Dictionary Learning},
	author={Bricken, Trenton and Templeton, Adly and Batson, Joshua and Chen, Brian and Jermyn, Adam and Conerly, Tom and Turner, Nick and Anil, Cem and Denison, Carson and Askell, Amanda and Lasenby, Robert and Wu, Yifan and Kravec, Shauna and Schiefer, Nicholas and Maxwell, Tim and Joseph, Nicholas and Hatfield-Dodds, Zac and Tamkin, Alex and Nguyen, Karina and McLean, Brayden and Burke, Josiah E and Hume, Tristan and Carter, Shan and Henighan, Tom and Olah, Christopher},
	year={2023},
	journal={Transformer Circuits Thread},
	url={https://transformer-circuits.pub/2023/monosemantic-features/index.html}
}
@inproceedings{
dunefsky2024transcoders,
title={Transcoders find interpretable {LLM} feature circuits},
author={Jacob Dunefsky and Philippe Chlenski and Neel Nanda},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=J6zHcScAo0}
}
@inproceedings{
makelov2024towards,
title={Towards Principled Evaluations of Sparse Autoencoders for Interpretability and Control},
author={Aleksandar Makelov and Georg Lange and Neel Nanda},
booktitle={ICLR 2024 Workshop on Secure and Trustworthy Large Language Models},
year={2024},
url={https://openreview.net/forum?id=MHIX9H8aYF}
}
@inproceedings{
rajamanoharan2024improving,
title={Improving Sparse Decomposition of Language Model Activations with Gated Sparse Autoencoders},
author={Senthooran Rajamanoharan and Arthur Conmy and Lewis Smith and Tom Lieberum and Vikrant Varma and Janos Kramar and Rohin Shah and Neel Nanda},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=zLBlin2zvW}
}
    @article{templeton2024scaling,
       title={Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet},
       author={Templeton, Adly and Conerly, Tom and Marcus, Jonathan and Lindsey, Jack and Bricken, Trenton and Chen, Brian and Pearce, Adam and Citro, Craig and Ameisen, Emmanuel and Jones, Andy and Cunningham, Hoagy and Turner, Nicholas L and McDougall, Callum and MacDiarmid, Monte and Freeman, C. Daniel and Sumers, Theodore R. and Rees, Edward and Batson, Joshua and Jermyn, Adam and Carter, Shan and Olah, Chris and Henighan, Tom},
       year={2024},
       journal={Transformer Circuits Thread},
       url={https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html}
    }
@article{ferrando2024primerinnerworkingstransformerbased,
      title={A Primer on the Inner Workings of Transformer-based Language Models}, 
      author={Javier Ferrando and Gabriele Sarti and Arianna Bisazza and Marta R. Costa-jussà},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2405.00208}, 
}
@article{sharkey2025openproblemsmechanisticinterpretability,
      title={Open Problems in Mechanistic Interpretability}, 
      author={Lee Sharkey and Bilal Chughtai and Joshua Batson and Jack Lindsey and Jeff Wu and Lucius Bushnaq and Nicholas Goldowsky-Dill and Stefan Heimersheim and Alejandro Ortega and Joseph Bloom and Stella Biderman and Adria Garriga-Alonso and Arthur Conmy and Neel Nanda and Jessica Rumbelow and Martin Wattenberg and Nandi Schoots and Joseph Miller and Eric J. Michaud and Stephen Casper and Max Tegmark and William Saunders and David Bau and Eric Todd and Atticus Geiger and Mor Geva and Jesse Hoogland and Daniel Murfet and Tom McGrath},
      year={2025},
      journal={arXiv},
      url={https://arxiv.org/abs/2501.16496}, 
}
@article{mueller2024questrightmediatorhistory,
      title={The Quest for the Right Mediator: A History, Survey, and Theoretical Grounding of Causal Interpretability}, 
      author={Aaron Mueller and Jannik Brinkmann and Millicent Li and Samuel Marks and Koyena Pal and Nikhil Prakash and Can Rager and Aruna Sankaranarayanan and Arnab Sen Sharma and Jiuding Sun and Eric Todd and David Bau and Yonatan Belinkov},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2408.01416}, 
}

@inproceedings{
    wang2023interpretability,
    title={Interpretability in the Wild: a Circuit for Indirect Object Identification in {GPT}-2 Small},
    author={Kevin Ro Wang and Alexandre Variengien and Arthur Conmy and Buck Shlegeris and Jacob Steinhardt},
    booktitle={The Eleventh International Conference on Learning Representations },
    year={2023},
    url={https://openreview.net/forum?id=NpsVSN6o4ul}
}
@article{elhage2022superposition,
   title={Toy Models of Superposition},
   author={Elhage, Nelson and Hume, Tristan and Olsson, Catherine and Schiefer, Nicholas and Henighan, Tom and Kravec, Shauna and Hatfield-Dodds, Zac and Lasenby, Robert and Drain, Dawn and Chen, Carol and Grosse, Roger and McCandlish, Sam and Kaplan, Jared and Amodei, Dario and Wattenberg, Martin and Olah, Christopher},
   year={2022},
   journal={Transformer Circuits Thread},
   url={https://transformer-circuits.pub/2022/toy_model/index.html}
}
@inproceedings{yun-etal-2021-transformer,
    title = "Transformer visualization via dictionary learning: contextualized embedding as a linear superposition of transformer factors",
    author = "Yun, Zeyu  and
      Chen, Yubei  and
      Olshausen, Bruno  and
      LeCun, Yann",
    editor = "Agirre, Eneko  and
      Apidianaki, Marianna  and
      Vuli{\'c}, Ivan",
    booktitle = "Proceedings of Deep Learning Inside Out (DeeLIO): The 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures",
    month = jun,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.deelio-1.1/",
    doi = "10.18653/v1/2021.deelio-1.1",
    pages = "1--10",
}
@article{minder2024controllable,
  title={Controllable Context Sensitivity and the Knob Behind It},
  author={Minder, Julian and Du, Kevin and Stoehr, Niklas and Monea, Giovanni and Wendler, Chris and West, Robert and Cotterell, Ryan},
  journal={arXiv preprint arXiv:2411.07404},
  year={2024}
}
@inproceedings{
	jain2024mechanistically,
	title={Mechanistically analyzing the effects of fine-tuning on procedurally defined tasks},
	author={Samyak Jain and Robert Kirk and Ekdeep Singh Lubana and Robert P. Dick and Hidenori Tanaka and Tim Rockt{\"a}schel and Edward Grefenstette and David Krueger},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=A0HKeKl4Nl}
}
@inproceedings{hao-etal-2020-investigating,
    title = "Investigating Learning Dynamics of {BERT} Fine-Tuning",
    author = "Hao, Yaru  and
      Dong, Li  and
      Wei, Furu  and
      Xu, Ke",
    editor = "Wong, Kam-Fai  and
      Knight, Kevin  and
      Wu, Hua",
    booktitle = "Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing",
    month = dec,
    year = "2020",
    address = "Suzhou, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.aacl-main.11/",
    doi = "10.18653/v1/2020.aacl-main.11",
    pages = "87--92",
    abstract = "The recently introduced pre-trained language model BERT advances the state-of-the-art on many NLP tasks through the fine-tuning approach, but few studies investigate how the fine-tuning process improves the model performance on downstream tasks. In this paper, we inspect the learning dynamics of BERT fine-tuning with two indicators. We use JS divergence to detect the change of the attention mode and use SVCCA distance to examine the change to the feature extraction mode during BERT fine-tuning. We conclude that BERT fine-tuning mainly changes the attention mode of the last layers and modifies the feature extraction mode of the intermediate and last layers. Moreover, we analyze the consistency of BERT fine-tuning between different random seeds and different datasets. In summary, we provide a distinctive understanding of the learning dynamics of BERT fine-tuning, which sheds some light on improving the fine-tuning results."
}
@inproceedings{mosbach-2023-analyzing,
    title = "Analyzing Pre-trained and Fine-tuned Language Models",
    author = "Mosbach, Marius",
    editor = "Elazar, Yanai  and
      Ettinger, Allyson  and
      Kassner, Nora  and
      Ruder, Sebastian  and
      A. Smith, Noah",
    booktitle = "Proceedings of the Big Picture Workshop",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.bigpicture-1.10",
    doi = "10.18653/v1/2023.bigpicture-1.10",
    pages = "123--134",
}
@inproceedings{kovaleva-etal-2019-revealing,
    title = "Revealing the Dark Secrets of {BERT}",
    author = "Kovaleva, Olga  and
      Romanov, Alexey  and
      Rogers, Anna  and
      Rumshisky, Anna",
    editor = "Inui, Kentaro  and
      Jiang, Jing  and
      Ng, Vincent  and
      Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    url = "https://aclanthology.org/D19-1445/",
    doi = "10.18653/v1/D19-1445",
    pages = "4365--4374",
}
@inproceedings{wu-etal-2024-language,
    title = "From Language Modeling to Instruction Following: Understanding the Behavior Shift in {LLM}s after Instruction Tuning",
    author = "Wu, Xuansheng  and
      Yao, Wenlin  and
      Chen, Jianshu  and
      Pan, Xiaoman  and
      Wang, Xiaoyang  and
      Liu, Ninghao  and
      Yu, Dong",
    editor = "Duh, Kevin  and
      Gomez, Helena  and
      Bethard, Steven",
    booktitle = "Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)",
    month = jun,
    year = "2024",
    address = "Mexico City, Mexico",
    url = "https://aclanthology.org/2024.naacl-long.130",
    doi = "10.18653/v1/2024.naacl-long.130",
    pages = "2341--2369",
}
@inproceedings{merchant-etal-2020-happens,
    title = "What Happens To {BERT} Embeddings During Fine-tuning?",
    author = "Merchant, Amil  and
      Rahimtoroghi, Elahe  and
      Pavlick, Ellie  and
      Tenney, Ian",
    editor = "Alishahi, Afra  and
      Belinkov, Yonatan  and
      Chrupa{\l}a, Grzegorz  and
      Hupkes, Dieuwke  and
      Pinter, Yuval  and
      Sajjad, Hassan",
    booktitle = "Proceedings of the Third BlackboxNLP Workshop on Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2020",
    address = "Online",
    url = "https://aclanthology.org/2020.blackboxnlp-1.4",
    doi = "10.18653/v1/2020.blackboxnlp-1.4",
    pages = "33--44",
}
@inproceedings{
	prakash2024finetuning,
	title={Fine-Tuning Enhances Existing Mechanisms: A Case Study on Entity Tracking},
	author={Nikhil Prakash and Tamar Rott Shaham and Tal Haklay and Yonatan Belinkov and David Bau},
	booktitle={The Twelfth International Conference on Learning Representations},
	year={2024},
	url={https://openreview.net/forum?id=8sKcAWOf2D}
}
@inproceedings{lee2024mechanistic,
  author = {Lee, Andrew and Bai, Xiaoyan and Pres, Itamar and Wattenberg, Martin and Kummerfeld, Jonathan K. and Mihalcea, Rada},
  title = {A mechanistic understanding of alignment algorithms: {A} case study on {DPO} and toxicity},
  year = {2024},
  booktitle = {Proceedings of the 41st International Conference on Machine Learning},
  articleno = {1052},
  numpages = {18},
  location = {Vienna, Austria},
  series = {ICML'24}
}
@article{qwen2025qwen25technicalreport,
      title={Qwen2.5 Technical Report}, 
      author={Qwen and : and An Yang and Baosong Yang and Beichen Zhang and Binyuan Hui and Bo Zheng and Bowen Yu and Chengyuan Li and Dayiheng Liu and Fei Huang and Haoran Wei and Huan Lin and Jian Yang and Jianhong Tu and Jianwei Zhang and Jianxin Yang and Jiaxi Yang and Jingren Zhou and Junyang Lin and Kai Dang and Keming Lu and Keqin Bao and Kexin Yang and Le Yu and Mei Li and Mingfeng Xue and Pei Zhang and Qin Zhu and Rui Men and Runji Lin and Tianhao Li and Tianyi Tang and Tingyu Xia and Xingzhang Ren and Xuancheng Ren and Yang Fan and Yang Su and Yichang Zhang and Yu Wan and Yuqiong Liu and Zeyu Cui and Zhenru Zhang and Zihan Qiu},
      year={2025},
      journal={arXiv},
      url={https://arxiv.org/abs/2412.15115}, 
}
@article{olah2020zoom,
  author = {Olah, Chris and Cammarata, Nick and Schubert, Ludwig and Goh, Gabriel and Petrov, Michael and Carter, Shan},
  title = {Zoom In: An Introduction to Circuits},
  journal = {Distill},
  year = {2020},
  note = {https://distill.pub/2020/circuits/zoom-in},
  doi = {10.23915/distill.00024.001}
}
@article{elhage2021mathematical,
   title={A Mathematical Framework for Transformer Circuits},
   author={Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
   year={2021},
   journal={Transformer Circuits Thread},
   note={https://transformer-circuits.pub/2021/framework/index.html}
}
@mastersthesis{minder2024understanding,
  title={Understanding the Surfacing of Capabilities in Language Models},
  author={Minder, Julian},
  year={2024},
  school={ETH Zurich}
}
@article{thasarathan2025universalsparseautoencodersinterpretable,
      title={Universal Sparse Autoencoders: Interpretable Cross-Model Concept Alignment}, 
      author={Harrish Thasarathan and Julian Forsyth and Thomas Fel and Matthew Kowal and Konstantinos Derpanis},
      year={2025},
      eprint={2502.03714},
      journal={arXiv},
      url={https://arxiv.org/abs/2502.03714}, 
}
@article{openai2024openaio1card,
      title={OpenAI o1 System Card}, 
      author={OpenAI and Aaron Jaech and Adam Kalai and Adam Lerer and Adam Richardson and Ahmed El-Kishky and Aiden Low and Alec Helyar and Aleksander Madry and Alex Beutel and Alex Carney and Alex Iftimie and Alex Karpenko and Alex Tachard Passos and Alexander Neitz and Alexander Prokofiev and Alexander Wei and Allison Tam and Ally Bennett and Ananya Kumar and Andre Saraiva and Andrea Vallone and Andrew Duberstein and Andrew Kondrich and Andrey Mishchenko and Andy Applebaum and Angela Jiang and Ashvin Nair and Barret Zoph and Behrooz Ghorbani and Ben Rossen and Benjamin Sokolowsky and Boaz Barak and Bob McGrew and Borys Minaiev and Botao Hao and Bowen Baker and Brandon Houghton and Brandon McKinzie and Brydon Eastman and Camillo Lugaresi and Cary Bassin and Cary Hudson and Chak Ming Li and Charles de Bourcy and Chelsea Voss and Chen Shen and Chong Zhang and Chris Koch and Chris Orsinger and Christopher Hesse and Claudia Fischer and Clive Chan and Dan Roberts and Daniel Kappler and Daniel Levy and Daniel Selsam and David Dohan and David Farhi and David Mely and David Robinson and Dimitris Tsipras and Doug Li and Dragos Oprica and Eben Freeman and Eddie Zhang and Edmund Wong and Elizabeth Proehl and Enoch Cheung and Eric Mitchell and Eric Wallace and Erik Ritter and Evan Mays and Fan Wang and Felipe Petroski Such and Filippo Raso and Florencia Leoni and Foivos Tsimpourlas and Francis Song and Fred von Lohmann and Freddie Sulit and Geoff Salmon and Giambattista Parascandolo and Gildas Chabot and Grace Zhao and Greg Brockman and Guillaume Leclerc and Hadi Salman and Haiming Bao and Hao Sheng and Hart Andrin and Hessam Bagherinezhad and Hongyu Ren and Hunter Lightman and Hyung Won Chung and Ian Kivlichan and Ian O'Connell and Ian Osband and Ignasi Clavera Gilaberte and Ilge Akkaya and Ilya Kostrikov and Ilya Sutskever and Irina Kofman and Jakub Pachocki and James Lennon and Jason Wei and Jean Harb and Jerry Twore and Jiacheng Feng and Jiahui Yu and Jiayi Weng and Jie Tang and Jieqi Yu and Joaquin Quiñonero Candela and Joe Palermo and Joel Parish and Johannes Heidecke and John Hallman and John Rizzo and Jonathan Gordon and Jonathan Uesato and Jonathan Ward and Joost Huizinga and Julie Wang and Kai Chen and Kai Xiao and Karan Singhal and Karina Nguyen and Karl Cobbe and Katy Shi and Kayla Wood and Kendra Rimbach and Keren Gu-Lemberg and Kevin Liu and Kevin Lu and Kevin Stone and Kevin Yu and Lama Ahmad and Lauren Yang and Leo Liu and Leon Maksin and Leyton Ho and Liam Fedus and Lilian Weng and Linden Li and Lindsay McCallum and Lindsey Held and Lorenz Kuhn and Lukas Kondraciuk and Lukasz Kaiser and Luke Metz and Madelaine Boyd and Maja Trebacz and Manas Joglekar and Mark Chen and Marko Tintor and Mason Meyer and Matt Jones and Matt Kaufer and Max Schwarzer and Meghan Shah and Mehmet Yatbaz and Melody Y. Guan and Mengyuan Xu and Mengyuan Yan and Mia Glaese and Mianna Chen and Michael Lampe and Michael Malek and Michele Wang and Michelle Fradin and Mike McClay and Mikhail Pavlov and Miles Wang and Mingxuan Wang and Mira Murati and Mo Bavarian and Mostafa Rohaninejad and Nat McAleese and Neil Chowdhury and Neil Chowdhury and Nick Ryder and Nikolas Tezak and Noam Brown and Ofir Nachum and Oleg Boiko and Oleg Murk and Olivia Watkins and Patrick Chao and Paul Ashbourne and Pavel Izmailov and Peter Zhokhov and Rachel Dias and Rahul Arora and Randall Lin and Rapha Gontijo Lopes and Raz Gaon and Reah Miyara and Reimar Leike and Renny Hwang and Rhythm Garg and Robin Brown and Roshan James and Rui Shu and Ryan Cheu and Ryan Greene and Saachi Jain and Sam Altman and Sam Toizer and Sam Toyer and Samuel Miserendino and Sandhini Agarwal and Santiago Hernandez and Sasha Baker and Scott McKinney and Scottie Yan and Shengjia Zhao and Shengli Hu and Shibani Santurkar and Shraman Ray Chaudhuri and Shuyuan Zhang and Siyuan Fu and Spencer Papay and Steph Lin and Suchir Balaji and Suvansh Sanjeev and Szymon Sidor and Tal Broda and Aidan Clark and Tao Wang and Taylor Gordon and Ted Sanders and Tejal Patwardhan and Thibault Sottiaux and Thomas Degry and Thomas Dimson and Tianhao Zheng and Timur Garipov and Tom Stasi and Trapit Bansal and Trevor Creech and Troy Peterson and Tyna Eloundou and Valerie Qi and Vineet Kosaraju and Vinnie Monaco and Vitchyr Pong and Vlad Fomenko and Weiyi Zheng and Wenda Zhou and Wes McCabe and Wojciech Zaremba and Yann Dubois and Yinghai Lu and Yining Chen and Young Cha and Yu Bai and Yuchen He and Yuchen Zhang and Yunyun Wang and Zheng Shao and Zhuohan Li},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2412.16720}, 
}

@article{deepseekai2025deepseekr1incentivizingreasoningcapability,
      title={DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning}, 
      author={DeepSeek-AI and Daya Guo and Dejian Yang and Haowei Zhang and Junxiao Song and Ruoyu Zhang and Runxin Xu and Qihao Zhu and Shirong Ma and Peiyi Wang and Xiao Bi and Xiaokang Zhang and Xingkai Yu and Yu Wu and Z. F. Wu and Zhibin Gou and Zhihong Shao and Zhuoshu Li and Ziyi Gao and Aixin Liu and Bing Xue and Bingxuan Wang and Bochao Wu and Bei Feng and Chengda Lu and Chenggang Zhao and Chengqi Deng and Chenyu Zhang and Chong Ruan and Damai Dai and Deli Chen and Dongjie Ji and Erhang Li and Fangyun Lin and Fucong Dai and Fuli Luo and Guangbo Hao and Guanting Chen and Guowei Li and H. Zhang and Han Bao and Hanwei Xu and Haocheng Wang and Honghui Ding and Huajian Xin and Huazuo Gao and Hui Qu and Hui Li and Jianzhong Guo and Jiashi Li and Jiawei Wang and Jingchang Chen and Jingyang Yuan and Junjie Qiu and Junlong Li and J. L. Cai and Jiaqi Ni and Jian Liang and Jin Chen and Kai Dong and Kai Hu and Kaige Gao and Kang Guan and Kexin Huang and Kuai Yu and Lean Wang and Lecong Zhang and Liang Zhao and Litong Wang and Liyue Zhang and Lei Xu and Leyi Xia and Mingchuan Zhang and Minghua Zhang and Minghui Tang and Meng Li and Miaojun Wang and Mingming Li and Ning Tian and Panpan Huang and Peng Zhang and Qiancheng Wang and Qinyu Chen and Qiushi Du and Ruiqi Ge and Ruisong Zhang and Ruizhe Pan and Runji Wang and R. J. Chen and R. L. Jin and Ruyi Chen and Shanghao Lu and Shangyan Zhou and Shanhuang Chen and Shengfeng Ye and Shiyu Wang and Shuiping Yu and Shunfeng Zhou and Shuting Pan and S. S. Li and Shuang Zhou and Shaoqing Wu and Shengfeng Ye and Tao Yun and Tian Pei and Tianyu Sun and T. Wang and Wangding Zeng and Wanjia Zhao and Wen Liu and Wenfeng Liang and Wenjun Gao and Wenqin Yu and Wentao Zhang and W. L. Xiao and Wei An and Xiaodong Liu and Xiaohan Wang and Xiaokang Chen and Xiaotao Nie and Xin Cheng and Xin Liu and Xin Xie and Xingchao Liu and Xinyu Yang and Xinyuan Li and Xuecheng Su and Xuheng Lin and X. Q. Li and Xiangyue Jin and Xiaojin Shen and Xiaosha Chen and Xiaowen Sun and Xiaoxiang Wang and Xinnan Song and Xinyi Zhou and Xianzu Wang and Xinxia Shan and Y. K. Li and Y. Q. Wang and Y. X. Wei and Yang Zhang and Yanhong Xu and Yao Li and Yao Zhao and Yaofeng Sun and Yaohui Wang and Yi Yu and Yichao Zhang and Yifan Shi and Yiliang Xiong and Ying He and Yishi Piao and Yisong Wang and Yixuan Tan and Yiyang Ma and Yiyuan Liu and Yongqiang Guo and Yuan Ou and Yuduan Wang and Yue Gong and Yuheng Zou and Yujia He and Yunfan Xiong and Yuxiang Luo and Yuxiang You and Yuxuan Liu and Yuyang Zhou and Y. X. Zhu and Yanhong Xu and Yanping Huang and Yaohui Li and Yi Zheng and Yuchen Zhu and Yunxian Ma and Ying Tang and Yukun Zha and Yuting Yan and Z. Z. Ren and Zehui Ren and Zhangli Sha and Zhe Fu and Zhean Xu and Zhenda Xie and Zhengyan Zhang and Zhewen Hao and Zhicheng Ma and Zhigang Yan and Zhiyu Wu and Zihui Gu and Zijia Zhu and Zijun Liu and Zilin Li and Ziwei Xie and Ziyang Song and Zizheng Pan and Zhen Huang and Zhipeng Xu and Zhongyu Zhang and Zhen Zhang},
      year={2025},
      journal={arXiv},
      eprint={2501.12948},
      url={https://arxiv.org/abs/2501.12948}, 
}
@article{sharma2023understandingsycophancylanguagemodels,
      title={Towards Understanding Sycophancy in Language Models}, 
      author={Mrinank Sharma and Meg Tong and Tomasz Korbak and David Duvenaud and Amanda Askell and Samuel R. Bowman and Newton Cheng and Esin Durmus and Zac Hatfield-Dodds and Scott R. Johnston and Shauna Kravec and Timothy Maxwell and Sam McCandlish and Kamal Ndousse and Oliver Rausch and Nicholas Schiefer and Da Yan and Miranda Zhang and Ethan Perez},
      year={2023},
      journal={arXiv},
      url={https://arxiv.org/abs/2310.13548}, 
}
@article{grattafiori2024llama3herdmodels,
      title={The Llama 3 Herd of Models}, 
      author={Aaron Grattafiori and Abhimanyu Dubey and Abhinav Jauhri and Abhinav Pandey and Abhishek Kadian and Ahmad Al-Dahle and Aiesha Letman and Akhil Mathur and Alan Schelten and Alex Vaughan and Amy Yang and Angela Fan and Anirudh Goyal and Anthony Hartshorn and Aobo Yang and Archi Mitra and Archie Sravankumar and Artem Korenev and Arthur Hinsvark and Arun Rao and Aston Zhang and Aurelien Rodriguez and Austen Gregerson and Ava Spataru and Baptiste Roziere and Bethany Biron and Binh Tang and Bobbie Chern and Charlotte Caucheteux and Chaya Nayak and Chloe Bi and Chris Marra and Chris McConnell and Christian Keller and Christophe Touret and Chunyang Wu and Corinne Wong and Cristian Canton Ferrer and Cyrus Nikolaidis and Damien Allonsius and Daniel Song and Danielle Pintz and Danny Livshits and Danny Wyatt and David Esiobu and Dhruv Choudhary and Dhruv Mahajan and Diego Garcia-Olano and Diego Perino and Dieuwke Hupkes and Egor Lakomkin and Ehab AlBadawy and Elina Lobanova and Emily Dinan and Eric Michael Smith and Filip Radenovic and Francisco Guzmán and Frank Zhang and Gabriel Synnaeve and Gabrielle Lee and Georgia Lewis Anderson and Govind Thattai and Graeme Nail and Gregoire Mialon and Guan Pang and Guillem Cucurell and Hailey Nguyen and Hannah Korevaar and Hu Xu and Hugo Touvron and Iliyan Zarov and Imanol Arrieta Ibarra and Isabel Kloumann and Ishan Misra and Ivan Evtimov and Jack Zhang and Jade Copet and Jaewon Lee and Jan Geffert and Jana Vranes and Jason Park and Jay Mahadeokar and Jeet Shah and Jelmer van der Linde and Jennifer Billock and Jenny Hong and Jenya Lee and Jeremy Fu and Jianfeng Chi and Jianyu Huang and Jiawen Liu and Jie Wang and Jiecao Yu and Joanna Bitton and Joe Spisak and Jongsoo Park and Joseph Rocca and Joshua Johnstun and Joshua Saxe and Junteng Jia and Kalyan Vasuden Alwala and Karthik Prasad and Kartikeya Upasani and Kate Plawiak and Ke Li and Kenneth Heafield and Kevin Stone and Khalid El-Arini and Krithika Iyer and Kshitiz Malik and Kuenley Chiu and Kunal Bhalla and Kushal Lakhotia and Lauren Rantala-Yeary and Laurens van der Maaten and Lawrence Chen and Liang Tan and Liz Jenkins and Louis Martin and Lovish Madaan and Lubo Malo and Lukas Blecher and Lukas Landzaat and Luke de Oliveira and Madeline Muzzi and Mahesh Pasupuleti and Mannat Singh and Manohar Paluri and Marcin Kardas and Maria Tsimpoukelli and Mathew Oldham and Mathieu Rita and Maya Pavlova and Melanie Kambadur and Mike Lewis and Min Si and Mitesh Kumar Singh and Mona Hassan and Naman Goyal and Narjes Torabi and Nikolay Bashlykov and Nikolay Bogoychev and Niladri Chatterji and Ning Zhang and Olivier Duchenne and Onur Çelebi and Patrick Alrassy and Pengchuan Zhang and Pengwei Li and Petar Vasic and Peter Weng and Prajjwal Bhargava and Pratik Dubal and Praveen Krishnan and Punit Singh Koura and Puxin Xu and Qing He and Qingxiao Dong and Ragavan Srinivasan and Raj Ganapathy and Ramon Calderer and Ricardo Silveira Cabral and Robert Stojnic and Roberta Raileanu and Rohan Maheswari and Rohit Girdhar and Rohit Patel and Romain Sauvestre and Ronnie Polidoro and Roshan Sumbaly and Ross Taylor and Ruan Silva and Rui Hou and Rui Wang and Saghar Hosseini and Sahana Chennabasappa and Sanjay Singh and Sean Bell and Seohyun Sonia Kim and Sergey Edunov and Shaoliang Nie and Sharan Narang and Sharath Raparthy and Sheng Shen and Shengye Wan and Shruti Bhosale and Shun Zhang and Simon Vandenhende and Soumya Batra and Spencer Whitman and Sten Sootla and Stephane Collot and Suchin Gururangan and Sydney Borodinsky and Tamar Herman and Tara Fowler and Tarek Sheasha and Thomas Georgiou and Thomas Scialom and Tobias Speckbacher and Todor Mihaylov and Tong Xiao and Ujjwal Karn and Vedanuj Goswami and Vibhor Gupta and Vignesh Ramanathan and Viktor Kerkez and Vincent Gonguet and Virginie Do and Vish Vogeti and Vítor Albiero and Vladan Petrovic and Weiwei Chu and Wenhan Xiong and Wenyin Fu and Whitney Meers and Xavier Martinet and Xiaodong Wang and Xiaofang Wang and Xiaoqing Ellen Tan and Xide Xia and Xinfeng Xie and Xuchao Jia and Xuewei Wang and Yaelle Goldschlag and Yashesh Gaur and Yasmine Babaei and Yi Wen and Yiwen Song and Yuchen Zhang and Yue Li and Yuning Mao and Zacharie Delpierre Coudert and Zheng Yan and Zhengxing Chen and Zoe Papakipos and Aaditya Singh and Aayushi Srivastava and Abha Jain and Adam Kelsey and Adam Shajnfeld and Adithya Gangidi and Adolfo Victoria and Ahuva Goldstand and Ajay Menon and Ajay Sharma and Alex Boesenberg and Alexei Baevski and Allie Feinstein and Amanda Kallet and Amit Sangani and Amos Teo and Anam Yunus and Andrei Lupu and Andres Alvarado and Andrew Caples and Andrew Gu and Andrew Ho and Andrew Poulton and Andrew Ryan and Ankit Ramchandani and Annie Dong and Annie Franco and Anuj Goyal and Aparajita Saraf and Arkabandhu Chowdhury and Ashley Gabriel and Ashwin Bharambe and Assaf Eisenman and Azadeh Yazdan and Beau James and Ben Maurer and Benjamin Leonhardi and Bernie Huang and Beth Loyd and Beto De Paola and Bhargavi Paranjape and Bing Liu and Bo Wu and Boyu Ni and Braden Hancock and Bram Wasti and Brandon Spence and Brani Stojkovic and Brian Gamido and Britt Montalvo and Carl Parker and Carly Burton and Catalina Mejia and Ce Liu and Changhan Wang and Changkyu Kim and Chao Zhou and Chester Hu and Ching-Hsiang Chu and Chris Cai and Chris Tindal and Christoph Feichtenhofer and Cynthia Gao and Damon Civin and Dana Beaty and Daniel Kreymer and Daniel Li and David Adkins and David Xu and Davide Testuggine and Delia David and Devi Parikh and Diana Liskovich and Didem Foss and Dingkang Wang and Duc Le and Dustin Holland and Edward Dowling and Eissa Jamil and Elaine Montgomery and Eleonora Presani and Emily Hahn and Emily Wood and Eric-Tuan Le and Erik Brinkman and Esteban Arcaute and Evan Dunbar and Evan Smothers and Fei Sun and Felix Kreuk and Feng Tian and Filippos Kokkinos and Firat Ozgenel and Francesco Caggioni and Frank Kanayet and Frank Seide and Gabriela Medina Florez and Gabriella Schwarz and Gada Badeer and Georgia Swee and Gil Halpern and Grant Herman and Grigory Sizov and Guangyi and Zhang and Guna Lakshminarayanan and Hakan Inan and Hamid Shojanazeri and Han Zou and Hannah Wang and Hanwen Zha and Haroun Habeeb and Harrison Rudolph and Helen Suk and Henry Aspegren and Hunter Goldman and Hongyuan Zhan and Ibrahim Damlaj and Igor Molybog and Igor Tufanov and Ilias Leontiadis and Irina-Elena Veliche and Itai Gat and Jake Weissman and James Geboski and James Kohli and Janice Lam and Japhet Asher and Jean-Baptiste Gaya and Jeff Marcus and Jeff Tang and Jennifer Chan and Jenny Zhen and Jeremy Reizenstein and Jeremy Teboul and Jessica Zhong and Jian Jin and Jingyi Yang and Joe Cummings and Jon Carvill and Jon Shepard and Jonathan McPhie and Jonathan Torres and Josh Ginsburg and Junjie Wang and Kai Wu and Kam Hou U and Karan Saxena and Kartikay Khandelwal and Katayoun Zand and Kathy Matosich and Kaushik Veeraraghavan and Kelly Michelena and Keqian Li and Kiran Jagadeesh and Kun Huang and Kunal Chawla and Kyle Huang and Lailin Chen and Lakshya Garg and Lavender A and Leandro Silva and Lee Bell and Lei Zhang and Liangpeng Guo and Licheng Yu and Liron Moshkovich and Luca Wehrstedt and Madian Khabsa and Manav Avalani and Manish Bhatt and Martynas Mankus and Matan Hasson and Matthew Lennie and Matthias Reso and Maxim Groshev and Maxim Naumov and Maya Lathi and Meghan Keneally and Miao Liu and Michael L. Seltzer and Michal Valko and Michelle Restrepo and Mihir Patel and Mik Vyatskov and Mikayel Samvelyan and Mike Clark and Mike Macey and Mike Wang and Miquel Jubert Hermoso and Mo Metanat and Mohammad Rastegari and Munish Bansal and Nandhini Santhanam and Natascha Parks and Natasha White and Navyata Bawa and Nayan Singhal and Nick Egebo and Nicolas Usunier and Nikhil Mehta and Nikolay Pavlovich Laptev and Ning Dong and Norman Cheng and Oleg Chernoguz and Olivia Hart and Omkar Salpekar and Ozlem Kalinli and Parkin Kent and Parth Parekh and Paul Saab and Pavan Balaji and Pedro Rittner and Philip Bontrager and Pierre Roux and Piotr Dollar and Polina Zvyagina and Prashant Ratanchandani and Pritish Yuvraj and Qian Liang and Rachad Alao and Rachel Rodriguez and Rafi Ayub and Raghotham Murthy and Raghu Nayani and Rahul Mitra and Rangaprabhu Parthasarathy and Raymond Li and Rebekkah Hogan and Robin Battey and Rocky Wang and Russ Howes and Ruty Rinott and Sachin Mehta and Sachin Siby and Sai Jayesh Bondu and Samyak Datta and Sara Chugh and Sara Hunt and Sargun Dhillon and Sasha Sidorov and Satadru Pan and Saurabh Mahajan and Saurabh Verma and Seiji Yamamoto and Sharadh Ramaswamy and Shaun Lindsay and Shaun Lindsay and Sheng Feng and Shenghao Lin and Shengxin Cindy Zha and Shishir Patil and Shiva Shankar and Shuqiang Zhang and Shuqiang Zhang and Sinong Wang and Sneha Agarwal and Soji Sajuyigbe and Soumith Chintala and Stephanie Max and Stephen Chen and Steve Kehoe and Steve Satterfield and Sudarshan Govindaprasad and Sumit Gupta and Summer Deng and Sungmin Cho and Sunny Virk and Suraj Subramanian and Sy Choudhury and Sydney Goldman and Tal Remez and Tamar Glaser and Tamara Best and Thilo Koehler and Thomas Robinson and Tianhe Li and Tianjun Zhang and Tim Matthews and Timothy Chou and Tzook Shaked and Varun Vontimitta and Victoria Ajayi and Victoria Montanez and Vijai Mohan and Vinay Satish Kumar and Vishal Mangla and Vlad Ionescu and Vlad Poenaru and Vlad Tiberiu Mihailescu and Vladimir Ivanov and Wei Li and Wenchen Wang and Wenwen Jiang and Wes Bouaziz and Will Constable and Xiaocheng Tang and Xiaojian Wu and Xiaolan Wang and Xilun Wu and Xinbo Gao and Yaniv Kleinman and Yanjun Chen and Ye Hu and Ye Jia and Ye Qi and Yenda Li and Yilin Zhang and Ying Zhang and Yossi Adi and Youngjin Nam and Yu and Wang and Yu Zhao and Yuchen Hao and Yundi Qian and Yunlu Li and Yuzi He and Zach Rait and Zachary DeVito and Zef Rosnbrick and Zhaoduo Wen and Zhenyu Yang and Zhiwei Zhao and Zhiyu Ma},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2407.21783}, 
}
@article{greenblatt2024alignmentfakinglargelanguage,
      title={Alignment faking in large language models}, 
      author={Ryan Greenblatt and Carson Denison and Benjamin Wright and Fabien Roger and Monte MacDiarmid and Sam Marks and Johannes Treutlein and Tim Belonax and Jack Chen and David Duvenaud and Akbir Khan and Julian Michael and Sören Mindermann and Ethan Perez and Linda Petrini and Jonathan Uesato and Jared Kaplan and Buck Shlegeris and Samuel R. Bowman and Evan Hubinger},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2412.14093}, 
}
@InProceedings{pmlr-v108-radiya-dixit20a,
  title = 	 {How fine can fine-tuning be?  {Learning} efficient language models},
  author =       {Radiya-Dixit, Evani and Wang, Xin},
  booktitle = 	 {Proceedings of the Twenty Third International Conference on Artificial Intelligence and Statistics},
  pages = 	 {2435--2443},
  year = 	 {2020},
  editor = 	 {Chiappa, Silvia and Calandra, Roberto},
  volume = 	 {108},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {26--28 Aug},
  url = 	 {https://proceedings.mlr.press/v108/radiya-dixit20a.html},
}
@inproceedings{aghajanyan-etal-2021-intrinsic,
    title = "Intrinsic Dimensionality Explains the Effectiveness of Language Model Fine-Tuning",
    author = "Aghajanyan, Armen  and
      Gupta, Sonal  and
      Zettlemoyer, Luke",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    url = "https://aclanthology.org/2021.acl-long.568",
    doi = "10.18653/v1/2021.acl-long.568",
    pages = "7319--7328",
}
@article{arditi2024refusallanguagemodelsmediated, 
    title={Refusal in Language Models Is Mediated by a Single Direction},
    author={Andy Arditi and Oscar Obeso and Aaquib Syed and Daniel Paleka and Nina Panickssery and Wes Gurnee and Neel Nanda},
    journal={OpenReview},
    year={2024},
    eprint={2406.11717},
    archiveprefix={arXiv},
    primaryclass={cs.LG},
    url={https://openreview.net/forum?id=EqF16oDVFf},
}
@inproceedings{
    gao2025scaling,
    title={Scaling and evaluating sparse autoencoders},
    author={Leo Gao and Tom Dupre la Tour and Henk Tillman and Gabriel Goh and Rajan Troll and Alec Radford and Ilya Sutskever and Jan Leike and Jeffrey Wu},
    booktitle={The Thirteenth International Conference on Learning Representations},
    year={2025},
    url={https://openreview.net/forum?id=tcsZt9ZNKD}
}
@misc{kissane_base_2024,
	title = {Base {LLMs} refuse too},
	url = {https://www.lesswrong.com/posts/YWo2cKJgL7Lg8xWjj/base-llms-refuse-too},
	language = {en},
	urldate = {2024-11-13},
	author = {Kissane, Connor and robertzk and Conmy, Arthur and Nanda, Neel},
	month = sep,
	year = {2024},
}
@misc{gabgoh_thoughvector,
	title = {Decoding The Thought Vector},
	url = {https://gabgoh.github.io/ThoughtVectors/},
	language = {en},
	urldate = {2025-02-26},
	author = {Goh,Gabriel},
	year = {2016},
}@article{engels2024decomposingdarkmattersparse,
      title={Decomposing The Dark Matter of Sparse Autoencoders}, 
      author={Joshua Engels and Logan Riggs and Max Tegmark},
      year={2024},
      eprint={2410.14670},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
	  journal={arXiv},
      url={https://arxiv.org/abs/2410.14670}, 
}
@inproceedings{engels2025not,
	title={Not All Language Model Features Are Linear},
	author={Joshua Engels and Eric J Michaud and Isaac Liao and Wes Gurnee and Max Tegmark},
	booktitle={The Thirteenth International Conference on Learning Representations},
	year={2025},
	url={https://openreview.net/forum?id=d63a4AM4hb}
}
@article{chaudhary2024evaluatingopensourcesparseautoencoders,
      title={Evaluating Open-Source Sparse Autoencoders on Disentangling Factual Knowledge in GPT-2 Small}, 
      author={Maheep Chaudhary and Atticus Geiger},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2409.04478}, 
}
@article{farrell2024applyingsparseautoencodersunlearn,
      title={Applying sparse autoencoders to unlearn knowledge in language models}, 
      author={Eoin Farrell and Yeu-Tong Lau and Arthur Conmy},
      year={2024},
      journal={arXiv},
      url={https://arxiv.org/abs/2410.19278}, 
}
@inproceedings{
    wei2022finetuned,
    title={Finetuned Language Models are Zero-Shot Learners},
    author={Jason Wei and Maarten Bosma and Vincent Zhao and Kelvin Guu and Adams Wei Yu and Brian Lester and Nan Du and Andrew M. Dai and Quoc V Le},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=gEZrGCozdqR}
}
@inproceedings{
    sanh2022multitask,
    title={Multitask Prompted Training Enables Zero-Shot Task Generalization},
    author={Victor Sanh and Albert Webson and Colin Raffel and Stephen Bach and Lintang Sutawika and Zaid Alyafeai and Antoine Chaffin and Arnaud Stiegler and Arun Raja and Manan Dey and M Saiful Bari and Canwen Xu and Urmish Thakker and Shanya Sharma Sharma and Eliza Szczechla and Taewoon Kim and Gunjan Chhablani and Nihal Nayak and Debajyoti Datta and Jonathan Chang and Mike Tian-Jian Jiang and Han Wang and Matteo Manica and Sheng Shen and Zheng Xin Yong and Harshit Pandey and Rachel Bawden and Thomas Wang and Trishala Neeraj and Jos Rozen and Abheesht Sharma and Andrea Santilli and Thibault Fevry and Jason Alan Fries and Ryan Teehan and Teven Le Scao and Stella Biderman and Leo Gao and Thomas Wolf and Alexander M Rush},
    booktitle={International Conference on Learning Representations},
    year={2022},
    url={https://openreview.net/forum?id=9Vrb9D0WI4}
}
@inproceedings{
    wang-etal-2022-super,
    title = "Super-{N}atural{I}nstructions: {G}eneralization via Declarative Instructions on 1600+ {NLP} Tasks",
    author = "Wang, Yizhong  and
      Mishra, Swaroop  and
      Alipoormolabashi, Pegah  and
      Kordi, Yeganeh  and
      Mirzaei, Amirreza  and
      Naik, Atharva  and
      Ashok, Arjun  and
      Dhanasekaran, Arut Selvan  and
      Arunkumar, Anjana  and
      Stap, David  and
      Pathak, Eshaan  and
      Karamanolakis, Giannis  and
      Lai, Haizhi  and
      Purohit, Ishan  and
      Mondal, Ishani  and
      Anderson, Jacob  and
      Kuznia, Kirby  and
      Doshi, Krima  and
      Pal, Kuntal Kumar  and
      Patel, Maitreya  and
      Moradshahi, Mehrad  and
      Parmar, Mihir  and
      Purohit, Mirali  and
      Varshney, Neeraj  and
      Kaza, Phani Rohitha  and
      Verma, Pulkit  and
      Puri, Ravsehaj Singh  and
      Karia, Rushang  and
      Doshi, Savan  and
      Sampat, Shailaja Keyur  and
      Mishra, Siddhartha  and
      Reddy A, Sujan  and
      Patro, Sumanta  and
      Dixit, Tanay  and
      Shen, Xudong",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    url = "https://aclanthology.org/2022.emnlp-main.340",
    doi = "10.18653/v1/2022.emnlp-main.340",
    pages = "5085--5109",
}
@article{grosse2023studyinglargelanguagemodel,
      title={Studying Large Language Model Generalization with Influence Functions}, 
      author={Roger Grosse and Juhan Bae and Cem Anil and Nelson Elhage and Alex Tamkin and Amirhossein Tajdini and Benoit Steiner and Dustin Li and Esin Durmus and Ethan Perez and Evan Hubinger and Kamilė Lukošiūtė and Karina Nguyen and Nicholas Joseph and Sam McCandlish and Jared Kaplan and Samuel R. Bowman},
      year={2023},
      journal={arXiv},  
      url={https://arxiv.org/abs/2308.03296}, 
}
@inproceedings{bolukbasi2016lsh, 
    author={Bolukbasi, Tolga and Chang, Kai-Wei and Zou, James and Saligrama, Venkatesh and Kalai, Adam},
    booktitle={Advances in Neural Information Processing Systems},
    editor={D. Lee and M. Sugiyama and U. Luxburg and I. Guyon and R. Garnett},
    pages={},
    title={Man is to Computer Programmer as Woman is to Homemaker? {Debiasing} Word Embeddings},
    url={https://proceedings.neurips.cc/paper_files/paper/2016/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
    volume={29},
    year={2016},
}
@inproceedings{vargas-cotterell-2020-exploring,
    title = "Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation",
    author = "Vargas, Francisco  and
      Cotterell, Ryan",
    editor = "Webber, Bonnie  and
      Cohn, Trevor  and
      He, Yulan  and
      Liu, Yang",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
    month = nov,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2020.emnlp-main.232/",
    doi = "10.18653/v1/2020.emnlp-main.232",
    pages = "2902--2913",
}
@inproceedings{wang2023concept, 
    title={Concept Algebra for (Score-Based) Text-Controlled Generative Models},
    author={Wang, Zihao and Gui, Lin and Negrea, Jeffrey and Veitch, Victor},
    booktitle={Advances in Neural Information Processing Systems},
    editor={A. Oh and T. Naumann and A. Globerson and K. Saenko and M. Hardt and S. Levine},
    pages={35331--35349},
    publisher={Curran Associates, Inc.},
    url={https://proceedings.neurips.cc/paper_files/paper/2023/file/6f125214c86439d107ccb58e549e828f-Paper-Conference.pdf},
    volume={36},
    year={2023},
}
@inproceedings{wolf2020transformers,
  title={Transformers: State-of-the-Art Natural Language Processing},
  author={Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  booktitle={Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
  pages={38--45},
  year={2020},
  publisher={Association for Computational Linguistics},
  address={Online},
  url={https://aclanthology.org/2020.emnlp-demos.6/},
  doi={10.18653/v1/2020.emnlp-demos.6}
}

@inproceedings{conmy2023automatedcircuit,
  title={Towards Automated Circuit Discovery for Mechanistic Interpretability},
  author={Conmy, Arthur and Mavor-Parker, Augustine N. and Lynch, Aengus and Heimersheim, Stefan and Garriga-Alonso, Adrià},
  booktitle={Advances in Neural Information Processing Systems},
  volume={36},
  year={2023},
  url={https://arxiv.org/abs/2304.14997}
}

@misc{nostalgebraist2020logitlens,
  author={nostalgebraist},
  title={interpreting {GPT}: the logit lens},
  year={2020},
  month={August},
  howpublished={LessWrong},
  url={https://www.lesswrong.com/posts/AcKRB8wDpdaN6v6ru/interpreting-gpt-the-logit-lens}
}

@inproceedings{ghandeharioun2024patchscope,
  title={Patchscopes: A Unifying Framework for Inspecting Hidden Representations of Language Models},
  author={Ghandeharioun, Asma and Caciularu, Avi and Pearce, Adam and Dixon, Lucas and Geva, Mor},
  booktitle={Forty-first International Conference on Machine Learning},
  year={2024},
  url={https://arxiv.org/abs/2401.06102}
}