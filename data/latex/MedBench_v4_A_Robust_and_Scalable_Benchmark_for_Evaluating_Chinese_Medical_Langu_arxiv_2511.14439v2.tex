\documentclass{article}

\usepackage{PRIMEarxiv}

\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{lipsum}
\usepackage{graphicx}
\graphicspath{ {./images/} }
\usepackage{float}
\usepackage{tabularx}
\usepackage{threeparttable}
\usepackage{multirow}
\usepackage{enumitem}
\usepackage{cite}

\usepackage[table]{xcolor}
\usepackage{booktabs} 
\usepackage{arydshln}
\usepackage[normalem]{ulem}
\usepackage{pifont}
\usepackage{amsmath}
\usepackage{fvextra}   % 比 fancyvrb 多了自动换行功能
% ----- 
\definecolor{lightred}{HTML}{FEC8C8}
\definecolor{lightblue}{HTML}{C8DCFF}
\newcommand{\best}[1]{\uline{\textbf{#1}}}

\newcommand{\colorboxrule}[2]{%
  \begingroup
  \color{#1}
  \rule{6pt}{6pt}
  \endgroup
  #2%
}
  %%
  %%
  %%
  %%
  %%

\title{MedBench v4: A Robust and Scalable Benchmark for Evaluating Chinese Medical Language Models, Multimodal Models, and Intelligent Agents}
\author{
  Jinru Ding\textsuperscript{1,†},
  Lu Lu\textsuperscript{1,†},
  Chao Ding\textsuperscript{1,†},
  Mouxiao Bian\textsuperscript{1,†},
  Jiayuan Chen\textsuperscript{1},
  Wenrao Pang\textsuperscript{1},
  Ruiyao Chen\textsuperscript{1},\\
  \textbf{Xinwei Peng\textsuperscript{1},
  Renjie Lu\textsuperscript{1},
  Sijie Ren\textsuperscript{1},
  Guanxu Zhu\textsuperscript{1},
  Xiaoqin Wu\textsuperscript{1},
  Zhiqiang Liu\textsuperscript{1},
  Rongzhao Zhang\textsuperscript{1},}\\
  \textbf{Luyi Jiang\textsuperscript{2,3},
  Bing Han\textsuperscript{1},
  Yunqiu Wang\textsuperscript{4},
  Jie Xu\textsuperscript{1}*}  \\
  \vspace{0.3em}\\
  \textsuperscript{1}Shanghai Artificial Intelligence Laboratory, Shanghai, 200232, China \\
  \textsuperscript{2}Shanghai Institute of Infectious Disease and Biosecurity, Fudan
University, Shanghai, 200032, China \\
   \textsuperscript{3}Shanghai Health Development Research Center (Shanghai Medical
Information Center), Shanghai, 200032, China \\
\textsuperscript{4}Imperial College London, London, UK \\
   \vspace{0.3em}\\
  \textsuperscript{*}Correspondence to: \texttt{xujie@pjlab.org.cn}\\
  †These authors contributed equally.
}

  %% \AND
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\
  %% \And
  %% Coauthor \\
  %% Affiliation \\
  %% Address \\
  %% \texttt{email} \\


\begin{document}
\maketitle
\begin{abstract}
 Recent advances in medical large language models (LLMs), multimodal models, and agents demand evaluation frameworks that reflect real clinical workflows and safety constraints. We present MedBench v4, a nationwide, cloud-based benchmarking infrastructure comprising over 700,000 expert-curated tasks spanning 24 primary and 91 secondary specialties, with dedicated tracks for LLMs, multimodal models, and agents. Items undergo multi-stage refinement and multi-round review by clinicians from more than 500 institutions, and open-ended responses are scored by an LLM-as-a-judge calibrated to human ratings. We evaluate 15 frontier models. Base LLMs reach a mean overall score of 54.1/100 (best: Claude Sonnet 4.5, 62.5/100), but safety and ethics remain low (18.4/100). Multimodal models perform worse overall (mean 47.5/100; best: GPT-5, 54.9/100), with solid perception yet weaker cross-modal reasoning. Agents built on the same backbones substantially improve end-to-end performance (mean 79.8/100), with Claude Sonnet 4.5–based agents achieving up to 85.3/100 overall and 88.9/100 on safety tasks. MedBench v4 thus reveals persisting gaps in multimodal reasoning and safety for base models, while showing that governance-aware agentic orchestration can markedly enhance benchmarked clinical readiness without sacrificing capability. By aligning tasks with Chinese clinical guidelines and regulatory priorities, the platform offers a practical reference for hospitals, developers, and policymakers auditing medical AI.
\end{abstract}


\keywords{medical artificial intelligence, benchmarking, large language models, multimodal models, Agents, clinical safety and ethics}


\section{Introduction}
Large language models (LLMs) and multimodal models have recently achieved impressive performance on physician licensing exams and clinical question-answering tasks \cite{cao2025survey, kasagga2025performance, waldock2024accuracy, qiu2024llm}. Beyond traditional chatbots, AI agents now execute proactive tasks, offering the potential to reduce clinicians’ documentation and administrative burden—activities that consume roughly 73\% of physicians’ working time\cite{qiu2024llm, sinsky2016allocation, pavuluri2024balancing}. By automating routine workflows, AI agents could help mitigate burnout and allow clinicians to focus more on direct patient care. AI has also been applied in clinical triage\cite{sahni2023artificial}, medication recommendations, electronic health record (EHR) configuration\cite{kachman2024how}, and preoperative risk assessment. These advances suggest strong potential for AI-assisted care.\cite{yu2025large} However, despite their successes in benchmark settings, most models remain unready for real-world deployment\cite{mennella2024ethical}. Clinical applications demand more than factual recall—they require models to reason safely, handle diverse modalities, and align with the complex workflows of modern healthcare systems\cite{quinn2021trust, ellahham2020application}.

One major barrier is the limited scope and realism of current evaluation benchmarks. Existing resources such as CMExam\cite{liu2023benchmarking}, CBLUE\cite{zhang2022cblue}, and HealthBench\cite{arora2025healthbench} emphasize static, exam-style questions and often cover only a narrow range of specialties. They rarely assess multimodal capabilities, sequential decision-making, or interaction with clinical tools. Moreover, these benchmarks are seldom grounded in actual healthcare workflows, making them insufficient for testing readiness in high-stakes clinical environments.

To fill this gap, we introduce MedBench v4, a nationwide medical AI benchmarking infrastructure developed in China. Unlike prior static datasets, MedBench v4 offers a scenario-aligned, platform-based evaluation framework. It comprises over 700,000 curated test items spanning 24 primary and 91 secondary clinical specialties, covering tasks that mirror real clinical operations—such as documentation structuring, diagnostic reasoning, and care planning.

A hallmark of MedBench v4 is its comprehensive expert validation pipeline. All benchmark items undergo multi-stage refinement, followed by multi-round clinical auditing by practitioners from over 500 member institutions, including hospitals, medical societies, and academic centers (see Fig. \ref{fig:geodist}). This process ensures that each task is medically accurate, aligned with current clinical guidelines, and reflective of legitimate variation in real-world care.

MedBench v4 further distinguishes itself through support for multimodal and agentic evaluation. Ten datasets assess how models handle images and structured inputs alongside text, while fourteen agent-based test sets probe capabilities like task decomposition, tool use, multi-turn planning, and safety-critical reasoning. The benchmark is deployed as a secure cloud platform with a rotating test pool and standardized scoring. Open-ended responses are evaluated by an LLM-as-a-judge system calibrated against ratings from over 1,000 licensed physicians, enabling scalable yet clinically grounded evaluation.

By combining broad clinical coverage, multimodal realism, and governance-aligned methodology, MedBench v4 provides a robust foundation for evaluating the clinical readiness of medical AI systems. It is already serving as a reference platform for hospitals, developers, and regulatory stakeholders, and it aims to support the safe and effective integration of AI into healthcare practice.
\begin{figure}[t]
  \centering
  \includegraphics[width=\linewidth]{figures/fig-1.png} 
  \caption{Geographic distribution of clinical partners contributing to MedBench. 
  The map and donut chart show the regional distribution of more than 500 member institutions across China that participated in benchmark item refinement and multi-round clinical auditing, including hospitals, medical societies, and academic centers. East China contributes the largest share of partners, followed by South, Central, North, Southwest, Northwest, and Northeast China.}
  \label{fig:geodist}
\end{figure}



\section{Results}
Across the three MedBench tracks, we observe systematic differences in absolute performance and in the relative strengths of different model families. For all analyses, we rescale task-specific metrics to a 0–100 range and compute macro-averaged scores, with each task contributing equally regardless of sample size; capability-dimension scores are obtained by averaging over tasks within a dimension, and overall scores are the mean across dimensions for each model. Under this scoring scheme, base large language models (LLMs) achieve a mean overall score of 54.1/100, multimodal models reach 47.5/100, whereas agents built on top of the same LLM backbones attain 79.8/100. On this benchmark, the gap between agent systems and their corresponding base models indicates that, beyond backbone quality, orchestration with tools, memory and safety componentagentss is associated with substantially higher end-to-end clinical task scores compared with deploying the same backbones in a vanilla chat setting. We treat these differences as descriptive rather than statistically tested; no formal hypothesis testing is performed in the present work.

In the LLM track, scores are moderate but heterogeneous across capability clusters. The best-performing model, Claude-Sonnet-4-5-20250929, reaches an overall mean of 62.5/100, followed by Grok4 (60.9/100) and o4-mini (56.8/100). When aggregating tasks by capability family, Claude-Sonnet attains the highest mean performance on medical question answering, medical text generation, and complex clinical reasoning, whereas Grok shows relatively stronger results on language understanding and quality-control–oriented tasks (e.g., structure extraction, insurance, and pathway checks). In contrast, performance on safety and ethics tasks is uniformly low for base LLMs: averaged across models, the safety/ethics dimension remains at 18.4/100, clearly below the approximately 58–60 range observed for knowledge, generation and reasoning. Under the MedBench evaluation protocol, these findings suggest that medical safety and ethical compliance remain the main bottlenecks for current general-purpose LLMs, even when their factual and reasoning capabilities are reasonably strong.

In the multimodal track, overall scores are lower than gin the unimodal LLM track under the same scoring scheme. The highest overall mean is obtained by GPT-5 (54.9/100), followed by Gemini 2.5 Pro 2.5 Pro (53.5/100) and O4-mini (52.8/100). Performance varies substantially across multimodal sub-domains: GPT-5 is relatively stronger on visual perception and text extraction (e.g., detection, classification and OCR-style tasks), o4-mini performs best on cross-modal semantic understanding (visual QA, report generation and report quality control), while Claude-Sonnet attains the highest scores on multimodal clinical decision support (differential diagnosis, treatment recommendations and disease-course tracking). Domain-specialized vision–language models such as HuatuoGPT-Vision and MedGemma underperform the strongest general-purpose frontier models on most MedBench multimodal tasks, suggesting that current domain-specific training has not yet closed the gap for broad, coverage-oriented evaluations like MedBench, although such models may still be advantageous on narrower or more specialized tasks that are under-represented in our benchmark.

By contrast, the agent track shows substantially higher average performance across most capability dimensions. For this track, we construct agentic systems on top of the same backbones evaluated in the LLM track (e.g., o4-mini, Grok, Claude-Sonnet, GPT-5, Gemini), enabling paired comparisons between vanilla chat and agentic orchestration. Agent systems based on Claude-Sonnet achieve the highest overall mean (85.3/100), with Gemini- and GPT-based agents performing comparably (up to 85.1/100 and 84.1/100, respectively). These systems obtain strong scores on clinical task decomposition and planning, tool use and execution, and scene-aware dialogue management, and some backbones exhibit near-ceiling performance on memory and long-context benchmarks. Notably, agentic systems reach much higher scores on MedBench safety and ethics tasks: the best Claude-based agent attains 88.9/100 on this composite dimension, and the average across agents is 73.4/100, compared with 18.4/100 for base LLMs on the same tasks. Within the scope of our benchmark and scoring scheme, this pattern indicates that explicit agent-style control flows—incorporating safety guards, tool governance and multi-step decision logic—are associated with substantially improved performance on safety-critical evaluations and are not associated with a decrease in overall clinical task scores on MedBench, although real-world risk mitigation will require additional prospective validation beyond this benchmark.

Taken together, these results highlight three empirical trends on MedBench: (i) agentic orchestration contributes large incremental gains over base LLMs, particularly for safety, tool-mediated workflows and long-context tasks; (ii) multimodal clinical reasoning remains weaker than unimodal text-based reasoning despite solid progress in visual perception and report generation; and (iii) no single model dominates all dimensions on MedBench, with different backbones exhibiting complementary strengths across knowledge, reasoning, multimodal understanding and safety (see Fig.\ref{fig:performance})

\begin{figure}[H]
  \centering
  \includegraphics[width=0.9\linewidth]{figures/fig-3.png} % 换成你的文件名
  \caption{Benchmark performance of frontier models on MedBench datasets. Bars show task-specific scores (rescaled to 0--100; higher is better). Models compared include O4-mini, Gemini 2.5 Pro, GPT-4o, Llama 4 Maverick, MedGemma 27B-IT, Grok 4, GPT-5, Claude Sonnet 4.5, Qwen2.5-VL-72B-Instruct, and HuatuoGPT-Vision 34B.}
  \label{fig:performance}
\end{figure}

\section{Methods}
\subsection{Platform Overview and System architecture} 
MedBench v4 is implemented as a secure, cloud-hosted benchmarking service that standardizes end-to-end evaluation for medical language, multimodal, and agentic models. Rather than a static dataset release, the platform orchestrates data curation, rotating test construction, model submission, and scoring within a unified workflow that can be adopted by hospitals, regulators, and developers as a common reference for clinical readiness assessment. Users interact with the platform in two modes: (i) an API mode, in which a hosted model endpoint is registered and the platform pushes randomized test items over an encrypted channel; and (ii) an answer-upload mode, in which users download a randomized test split, run inference locally within their secure environment, and upload predictions for centralized scoring. In both workflows, ground-truth labels remain server-side and are never exposed to clients; only task prompts and schema specifications are shared, and all submissions are version-locked and rate-limited to support abuse prevention, auditability, and longitudinal comparison across model versions.

\subsection{Rotating evaluation pool and data pipeline}
To limit answer memorization and test-set overfitting, MedBench v4 maintains a rotating evaluation pool drawn from 36 curated datasets spanning 43 clinical specialties. For each evaluation cycle, a scheduler performs stratified sampling across tasks and specialties to construct a balanced subset in which small datasets remain visible while large datasets contribute only representative samples. Subsets are regenerated on a fixed cadence (approximately quarterly), so no static test set persists across cycles.

The underlying “AI-ready” data pipeline aggregates de-identified questions, clinical vignettes, imaging studies, and documentation templates from tertiary hospitals, specialty societies, and academic partners across China. This process targets broad coverage across 24 primary and 91 secondary specialties and four major application categories defined in national medical AI guidelines (documentation structuring, clinical decision support, quality control, and operational management). All candidate items undergo de-identification, terminology normalization, and conversion into machine-consumable formats; multimodal cases are standardized in terms of image resolution, formats, and report schemas. Refined items then pass multi-round expert auditing, during which clinicians verify guideline consistency, annotate legitimate areas of disagreement, and define key points and scoring rubrics for open-ended tasks. Only items meeting predefined quality thresholds enter the active test pool.

\subsection{Nationwide consortium and scenario alignment}
MedBench v4 is built and maintained as a nationwide consortium effort rather than a single-institution benchmark. The platform is hosted under a national AI application pilot base in healthcare and is being co-developed with a growing alliance of hospitals, professional societies, universities, and industry partners. Participating institutions contribute domain expertise, de-identified cases, and task designs, and in return gain access to standardized evaluations that can be used for internal model selection, procurement, and quality assurance.

\subsection{Dataset and Task }
Medbench v4 encompasses evaluations for large language models, multimodal models, and agents. We will now introduce the evaluation datasets for each track in turn (see Fig.\ref{fig:overview}).


\subsubsection{Dataset Construction}
\textbf{Large Language Model: }MedBench v4 organizes a rigorously curated dataset across five textual capability dimensions—Medical Language Understanding (MLU), Medical Language Generation (MLG), Medical Knowledge QA (MKQA), Complex Medical Reasoning (CMR), and Healthcare Safety and Ethics (HSE)—encompassing 36 self-built test sets spanning 43 clinical specialties.
Tasks are systematically organized according to five core capability dimensions reflecting key medical AI competencies: Medical Language Understanding (MLU), Medical Language Generation (MLG), Medical Knowledge Question Answering (MKQA), Complex Medical Reasoning (CMR), and Healthcare Safety and Ethics (HSE). These dimensions collectively ensure broad coverage of the fundamental language processing, knowledge reasoning, and ethical compliance skills required for clinical AI systems. Each dataset is explicitly aligned with one or more application scenarios defined in the National Health Commission of China's standard AI application framework (e.g., tasks corresponding to electronic medical record structuring, prescription auditing, clinical pathway compliance monitoring, or decision support), ensuring each evaluation task closely mirrors real-world clinical or healthcare management workflows. All datasets were constructed and annotated through a rigorous multi-stage expert curation process: initial content creation and labeling by at least two healthcare professionals, adjudication of any discrepancies by senior experts, and final quality assurance review to ensure accuracy and consistency. Throughout development, strict adherence to Chinese clinical guidelines and protocols was maintained, with all content validated by medical experts to guarantee terminological correctness and alignment with current clinical practice standards. Compared to previous Chinese medical benchmarks (e.g., CMExam, CBLUE, HealthBench), MedBench v4 offers broader specialty and task coverage, deeper alignment with operational healthcare workflows through scenario-based task design, and support for both objective (single-answer) and open-ended generative task formats. 

\begin{figure}[H]
  \centering
  \includegraphics[width=\linewidth]{figures/fig-2.png} % 换成你的文件名
  \caption{Overview of MedBench tasks for LLMs, multimodal models, and agents. 
  MedBench groups datasets into three capability layers. LLM tasks cover medical language understanding and generation, clinical reasoning, and safety--ethics--compliance evaluation. Multimodal tasks assess visual perception and cross-modal reasoning across images, documents, and mixed inputs. Agent tasks target tool-augmented and multi-agent systems, evaluating intention recognition, task planning, tool use, long-horizon interaction, and safety-aware behavior in realistic clinical workflows.}
  \label{fig:overview}
\end{figure}


\textbf{Multimodal Model:}We introduce a dedicated multimodal evaluation track comprising ten datasets spanning three core capability axes. The first axis targets visual perception and medical text extraction, including tasks such as radiological object detection, image-based disease classification, and optical character recognition of clinical text (datasets e.g., MedDetect, MedClass, MedOCR). The second axis focuses on cross-modal semantic understanding and reasoning, covering visual question answering, report generation, cross-modal quality control, and image-sequence alignment (e.g., MedVQA, MedGen, MedQC, MedSeqIm). The third axis addresses clinical decision support with image-aware inputs, simulating decision-making scenarios like differential diagnosis formulation, therapy recommendation, and longitudinal treatment course planning (e.g., MedDiffDx, MedTherapy, MedCourse). All imaging and report data are fully de-identified in accordance with privacy standards, while each dataset retains structured metadata (imaging modality, acquisition device, institution) in datasheets to enable analysis across technical contexts. Specifically, each task is based on scenarios aligned with real clinical applications in China, such as lesion detection, multimodal differential diagnosis support, and treatment pathway planning. These represent priority application areas within medical settings. All datasets employ expert human annotations, and their label schemas are aligned with Chinese radiology and pathology reporting standards to ensure clinical interpretability and consistency. Compared to existing English-centric medical benchmarks such as VQA-RAD\cite{lau2018dataset}, PathVQA\cite{he2020pathvqa}, and SLAKE\cite{liu2021slake} which primarily evaluate isolated image–question answering capabilities, MedBench v4 offers substantially greater task diversity, more precise image–text linkage (through paired images with corresponding reports or context), and improved operational relevance for Chinese medical AI applications.

\textbf{Agent:}We also include a dedicated agentic evaluation track comprising 14 datasets spanning six core capability dimensions relevant to real-world clinical workflows. (i) Goal decomposition and path planning tasks (MedDecomp, MedPathPlan, MedCOT, MedReflect) assess whether agents can deconstruct high-level clinical objectives into actionable steps, formulate coherent diagnostic or treatment plans, and iteratively refine their reasoning through chain-of-thought and self-reflection. (ii) Tool and API operations (MedRetAPI, MedCallAPI, MedDBOps) evaluate an agent’s ability to retrieve external knowledge, invoke structured clinical APIs, or interact with medical databases using appropriate parameters and execution protocols. (iii) Intent recognition and role adaptation (MedIntentID, MedRoleAdapt) test whether agents can identify user goals and adjust their communication style based on roles (e.g., patient vs. clinician) or shifting dialogue contexts. (iv) Long-context processing (MedLongConv, MedLongQA) challenges agents to preserve relevant memory across multi-turn interactions or extract information from extended clinical documents. (v) Multi-agent collaboration (MedCollab) simulates multidisciplinary consultation scenarios, requiring agents to integrate insights from peers with differing specialty roles. (vi) Safety and adversarial robustness (MedShield, MedDefend) test agents under ethically sensitive or adversarial prompts, assessing compliance with safety norms and resistance to prompt manipulation.

\subsection{Evaluation Methodology}
\subsubsection{Cloud benchmarking workflow and rotating subsets}
To ensure methodological rigor and reproducibility, MedBench v4 adopts a cloud-based benchmarking workflow with dynamically rotating evaluation subsets. Users interact with the platform in two modes: (i) API submission, where the platform calls a hosted model endpoint and collects predictions online, and (ii) answer-upload, where users download a randomized test split, perform inference locally within their own secure environment, and then upload prediction files. In both modes, all scoring runs server-side, and the platform returns per-task metrics and aggregate leaderboards, avoiding leakage of ground-truth labels.
To limit answer memorization and enforce fair comparison, MedBench v4 maintains a rotating pool drawn from 36 curated datasets spanning 43 clinical specialties. For each evaluation cycle, the scheduler constructs a balanced subset by stratified sampling across tasks and specialties, ensuring that small datasets remain represented while large datasets contribute only representative samples rather than their full content. Evaluation subsets are regenerated on a fixed cadence (approximately quarterly), so no static test set persists across cycles. This design reduces the incentive and opportunity for test-set overfitting while preserving longitudinal comparability.

\textbf{LLM-as-a-judge and rubric:}
For open-ended and long-form tasks, purely lexical metrics are insufficient to capture clinical correctness, guideline adherence, and safety. MedBench v4 employs a Qwen2.5-72B-Instruct as the reference judge model. This model is selected for its strong general reasoning ability, Chinese language proficiency, and robust performance on medical question-answering benchmarks.
For each task family, we design task-specific meta-prompts (see Appendix) that instruct Qwen2.5-72B-Instruct to compare a candidate response against reference answers, domain rules, and safety constraints, and to output structured scores rather than free-form commentary.
The judging rubric decomposes evaluation into several dimensions, including medical correctness, professionalism, compliance and safety, and practical usability. Each dimension is scored on a five-point scale with explicit anchor descriptions (e.g., from “dangerously incorrect” to “fully correct and safely actionable”), and tasks may additionally specify dimension weights depending on their clinical risk profile. Judge prompts are constrained to the relevant context (e.g., question, patient vignette, and reference solution) and explicitly instruct the model not to hallucinate new clinical facts beyond the provided information and established guidelines.

\subsection{Human evaluation}
To ground automated evaluation in real clinical standards, MedBench v4 incorporates a large-scale human expert assessment track. From the full benchmark, we randomly sample approximately 20\% of instances across all subtasks and assign them to a panel of 1,000 licensed medical professionals covering surgery, internal medicine, emergency medicine, and traditional Chinese medicine. Human raters follow the same multi-dimensional rubric as the LLM judge, scoring each response on a five-point scale for correctness, professionalism, compliance and safety, and overall acceptability for clinical use.

For each sampled item, multiple experts independently rate the model outputs, and their scores are aggregated (e.g., by majority or mean rating) to form a human reference label. We then compare these aggregated human ratings against the scores produced by the single LLM judge. Agreement is quantified using standard measures such as raw agreement rates and Cohen’s $\kappa$ . we observe high concordance between LLM-based and human evaluations (with Cohen’s $\kappa$  exceeding 0.82 on key tasks), indicating that the LLM judge can serve as a scalable and reproducible proxy for expert review in routine benchmarking. At the same time, the human evaluation track remains essential for periodic calibration, especially for ambiguous, contentious, or safety-critical cases where nuanced clinical judgment is required.


\section{Experiments}
\subsection{Set Up}
We evaluated all models on the full Medbench. To ensure strictly comparable results, we fixed inference settings across models: temperature = 0.7, maximum generation length = 512 tokens, and effective context window = 2,048 tokens (prompt + dialogue history). Models operated in pure text generation mode without tools or external retrieval. Each example was run three times; we report the mean ± standard deviation across trials. Unless otherwise noted, all other decoding parameters were held constant. The model responses in this study are generated using both vendor-provided APIs and locally deployed checkpoints. The computations are performed on NVIDIA H200 GPUs.
\subsection{Models}
We evaluate a diverse suite of proprietary and open-source models, spanning both general-purpose and domain-specialized systems. General LLMs include GPT-5, GPT-4o, and O4-mini, Gemini 2.5 Pro, Claude Sonnet 4.5, Llama-4-Maverick , and Grok 4. Domain and Chinese medical models comprise Qwen2.5-VL-72B-Instruct, MedGemma 27B-IT, and HuatuoGPT-Vision 34B.
\subsection{Evaluation metrics}
The scoring stack dynamically selects evaluation metrics according to the answer type and task semantics. We distinguish between objective classification, region- or set-based outputs, and string-level or free-text outputs.

\paragraph{Accuracy.}
For single-choice questions, we report accuracy under a strict exact-match criterion.
Each question's options (typically A--E) are randomly shuffled before being presented to the model.
A prediction is counted as correct only if the model outputs the exact correct option for that specific shuffled instance.
Consequently, even for the same underlying question, different shuffled versions constitute distinct evaluation items, and the model must select the correct option for every shuffled instance to receive credit.
This strict exact-match criterion ensures that the reported accuracy reflects genuine understanding rather than memorization of option positions or reliance on a fixed choice ordering.

\paragraph{Micro-F1.}
For multi-label classification and structured extraction tasks, we compute micro-averaged F1 (Micro-F1) to balance performance across frequent and infrequent labels.
Across all instances and labels, let $\mathrm{TP}$, $\mathrm{FP}$, and $\mathrm{FN}$ denote the total numbers of true positives, false positives, and false negatives, respectively:
\begin{itemize}
  \item $\mathrm{TP}$ (true positive): samples whose true label is positive and the model predicts positive;
  \item $\mathrm{FP}$ (false positive): samples whose true label is negative but the model predicts positive;
  \item $\mathrm{FN}$ (false negative): samples whose true label is positive but the model predicts negative.
\end{itemize}
Micro-averaged precision and recall are defined as
\begin{align}
  \mathrm{Precision}_{\text{micro}} &= 
    \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FP}},\\
  \mathrm{Recall}_{\text{micro}} &=
    \frac{\mathrm{TP}}{\mathrm{TP} + \mathrm{FN}}.
\end{align}
The micro-averaged F1 score is then
\begin{equation}
  \mathrm{F1}_{\text{micro}} =
  \frac{2 \cdot \mathrm{Precision}_{\text{micro}} \cdot \mathrm{Recall}_{\text{micro}}}
       {\mathrm{Precision}_{\text{micro}} + \mathrm{Recall}_{\text{micro}}}.
\end{equation}

\paragraph{Macro Recall.}
For open-ended QA and generation tasks, lexical overlap alone fails to capture clinical adequacy, guideline adherence, and safety.
In addition to LLM judging, we therefore compute a key-point--based Macro Recall.
Let $i = 1, \dots, N$ index items, and for each item $i$ let $\mathrm{TP}_i$ and $\mathrm{FN}_i$ denote the number of correctly and incorrectly covered normalized medical key points (e.g., required diagnoses, critical risk warnings, guideline-recommended interventions).
The per-item recall is
\begin{equation}
  \mathrm{Recall}_i = \frac{\mathrm{TP}_i}{\mathrm{TP}_i + \mathrm{FN}_i}.
\end{equation}
Macro Recall is then defined as the average of per-item recall:
\begin{equation}
  \mathrm{MacroRecall}
  = \frac{1}{N}\sum_{i=1}^{N} \mathrm{Recall}_i.
\end{equation}
This emphasizes coverage of clinically essential information rather than surface-form similarity.

\paragraph{Region- and set-based tasks (IoU).}
For tasks involving spatial or set overlap---such as object detection, localization, segmentation, or generalized span selection---we use Intersection over Union (IoU).
Given a predicted region (or set) $P$ and a ground-truth region (or set) $G$, IoU is defined as
\begin{equation}
  \mathrm{IoU} = \frac{|P \cap G|}{|P \cup G|}.
\end{equation}

\paragraph{String similarity and extraction quality (1-N.E.D).}
For string-level extraction tasks where partial matches are meaningful (e.g., short entities, codes, or phrases), we adopt 1-normalized edit distance (1-N.E.D) as a similarity measure between the predicted string $\hat{r}$ and the reference string $r$.
Let $d(r,\hat{r})$ denote the Levenshtein edit distance and $|r|$ the length of the reference string.
We define
\begin{equation}
  \mathrm{1\text{-}N.E.D} = 1 - \frac{d(r,\hat{r})}{|r|}.
\end{equation}
Larger values of 1-N.E.D indicate closer matches between the model output and the reference answer.


\section{Discussion}
Our research reveals a significant gap between the capabilities of current state-of-the-art models and the requirements for deployable medical AI. Foundational large language models achieved an average total score of 54.1 on the MedBench test, with the top-performing model (Claude Sonnet 4.5) reaching 62.5. In contrast, despite relatively strong performance on knowledge-intensive and reasoning-oriented tasks, their scores in safety and ethics dimensions remain notably low (mean 18.4/100). Multimodal models performed even more poorly (mean 47.5/100; best 54.9/100). While their visual perception and text extraction capabilities were relatively reliable, they exhibited significant shortcomings in cross-modal reasoning, longitudinal integration of imaging and clinical contexts, and image-aware decision support.
In contrast, agents based on similar underlying architectures demonstrated markedly superior performance in the MedBench agent track. Across all evaluation configurations, clinical agents achieved an average total score of 79.8/100. The top Claude-based agent scored 85.3/100 overall, reaching 88.9/100 in safety-oriented tasks. This track intentionally features tool-intensive and workflow-intensive scenarios, making agent architectures more suitable than basic chat interfaces. Improved benchmark scores alone do not guarantee intrinsic safety nor cover all potential failure modes in real clinical applications.
This pipeline was calibrated through large-scale human scoring (over 1,000 physicians), demonstrating high consistency on critical tasks. While enabling scalable evaluation of open-ended outputs, this design has limitations: scoring retains partial subjectivity, and biases in the evaluation model or human scorers may be reflected in benchmark scores. Furthermore, MedBench primarily reflects data from large tertiary hospitals and academic centers, and does not yet fully encompass the patient diversity, care environment variations, and resource constraints present in medically underserved regions. Future versions will expand benchmarking scope to include more diverse institutions and equitably oriented sample slices, while validating models and agent behaviors through prospective real-world studies.

By evaluating not only what knowledge models possess but also how they responsibly apply it, MedBench integrates technology assessment with core safety and ethical standards for deploying AI in healthcare. International benchmarking efforts have begun addressing similar challenges, with MedBench building upon prior achievements while achieving innovative breakthroughs. For instance, Google's MultiMedQA\cite{singhal2023large} suite integrates diverse question-answering datasets (spanning professional exams, research literature, and consumer health consultations) and introduces rigorous human evaluations based on dimensions such as factual accuracy, reasoning capability, and harm avoidance. This establishes a benchmark for broad medical question-answering, though it remains primarily confined to English text environments. Concurrently, early multimodal benchmarks like VQA-RAD\cite{he2020pathvqa} highlighted the importance of visual comprehension by requiring models to answer radiologists' questions about medical images. This dataset demonstrated the feasibility of incorporating medical imaging into AI evaluation, though it remains limited in scale and lacks interactive dialogue. Recently, OpenAI's HealthBench\cite{arora2025healthbench} elevated realism by evaluating models through 5,000 multi-turn clinical dialogues. Each dialogue undergoes quantitative assessment for completeness, accuracy, and contextual safety against physician-defined scoring criteria.

MedBench pursues this spirit of comprehensive, meaningful evaluation while uniquely optimizing for specific scenarios. Unlike MultiMedQA's static Q\&A or VQA-RAD's narrow focus, our benchmark integrates multiple modalities and clinical scenarios within a single framework. While HealthBench demonstrates robust performance with expert-aligned scoring criteria and non-saturated scenarios, MedBench applies similar principles to the Chinese medical context—incorporating local clinical guidelines, linguistic nuances, and policy considerations. We do not replicate existing benchmarks but provide an inclusive platform: adhering to international best practices while aligning with the needs of Chinese clinicians and patients, ensuring evaluation results possess both global applicability and local operability. MedBench adopts a scenario-driven, safety-first design philosophy, with its core motivation being to maximize real-world application value. By evaluating models through tasks that highly simulate clinical workflows—such as triage based on chat-based symptoms, simultaneous interpretation of lab results and imaging data, and developing management plans under constrained conditions—this benchmark provides critical insights directly applicable to clinical bedside deployment. A model's strong performance in these scenarios indicates its potential to effectively support daily clinical decision-making, while failure patterns—such as misdiagnosis or missed warning signs—highlight areas requiring careful handling or enhanced training before deployment. 

This pragmatic orientation positions MedBench not merely as a leaderboard competition but as an implementation tool: by revealing AI assistants' behavioral patterns in real-world contexts, the benchmark serves diverse stakeholders beyond model developers. Hospital administrators, medical boards, or regulatory bodies can leverage MedBench evaluations to objectively assess AI models' readiness for deployment, verifying compliance with both clinical standards and mandatory ethical norms. By aligning technological performance with policy objectives, we aim to foster responsible AI adoption in healthcare—driving clinical workflow innovation while safeguarding patient safety and public trust.

\section{Acknowledgements}
Supported by Shanghai Artificial Intelligence Laboratory

\bibliographystyle{unsrt}
\bibliography{references}

\clearpage 
\appendix 

\section{LLM-Judge Prompts}
\subsection{Dataset 1: LLM Benchmark}
\subsubsection{Prompt 1: General Medical QA and Consultation}
\begin{Verbatim}[breaklines, breakanywhere]
# 1. Task Description
Task type: Evaluate the model's knowledge accuracy, reliability, and professionalism in health consultation and modern medical knowledge QA, and provide a score.
Role definition: You are a senior clinical physician or health expert and AI model evaluator. Your scoring is strict, focusing on scientific accuracy of modern medical knowledge, correct interpretation of clinical tests, and the reliability and safety of health advice.
Evaluation objective: Comprehensively assess the model's capabilities in modern medicine and health knowledge, including interpretation of medical tests (internal medicine, surgery, laboratory, genetics), understanding of common diseases (e.g., cardiovascular, tumors, metabolic disorders), diagnostic principles, and professionalism and reliability in health consultation.
Evaluation task: Score the "candidate answer" according to the following "evaluation criteria" and "scoring guidelines".

# 2. Task Input
# Question ID:
{id}
# Input question:
{question}
# Model answer:
{answer}
# Reference answer:
{gold}

# 3. Evaluation Criteria (0–5 points)
{[}Scoring Criteria{]}:
Score 0: The candidate answer is completely unrelated to the reference, entirely wrong, or seriously violates modern medical knowledge or clinical safety principles (e.g., incorrect medication/treatment advice), and is vague or completely incorrect;
Score 1: The candidate answer has extremely low relevance to the reference, or only superficial coverage, containing multiple serious factual errors, misinterpretation of tests, incorrect disease mechanisms, or wrong diagnostic/treatment/medication suggestions;
Score 2: The candidate answer is partially related to the reference but contains obvious factual errors, misinterpretation of key medical test indicators, inaccurate disease descriptions, or incomplete/inappropriate health advice;
Score 3: The candidate answer agrees with most key points of the reference but still has imprecise terminology, incomplete explanations of tests or diseases, incorrect grasp of consultation principles, or omits critical risk details;
Score 4: The candidate answer largely matches the reference, but may slightly lack precision in data citation, wording rigor (e.g., "may", "suggest"), or information completeness;
Score 5: The candidate answer is fully consistent with the reference, accurate and reliable, with no omissions or deviations.

# 4. Scoring Guidelines
Follow the steps below to complete scoring:
1. Understand the task and medical standards: Fully understand the context of the health consultation, the model's task requirements, the core points of the reference answer, and the definition of accuracy, reliability, and safety in modern medicine.
2. Check point by point: Carefully read the model's answer, compare it point by point with the reference, and verify coverage of all core medical knowledge (e.g., test significance, disease causes, risk factors, treatment principles) and the absence of factual or logical errors.
3. Evaluate the accuracy of test interpretation and disease knowledge: Assess whether the model's explanation of medical test results, disease mechanisms, or treatment principles is scientific and accurate, and whether reasonable corrections are provided in case of errors.
4. Assess medical terminology and professionalism: Check whether medical terminology (including tests, diseases, medications) is precise and standard, whether references to scientific literature or clinical guidelines are appropriate, and whether health advice is safe and reliable. Strictly avoid confusion, generalization, or misinterpretation of key medical concepts.
5. Avoid irrelevant content generalization: Discourage irrelevant or excessive medical knowledge expansion or overdiagnosis. Even if expansion is reasonable, if it goes beyond the reference's scope or deviates from the consultation goal, or causes unnecessary health anxiety, deduct points accordingly.
6. Final score: Integrate scientific accuracy, professional reliability of interpretation, and safety of consultation advice to assign an integer score 0–5, reflecting subtle differences in answer quality.

# 5. Output Format Requirement
# Task ID: {id}
<score> (Fill in the final score 0–5 here)</score>
Notes:
- Only fill in the content inside the <score> tag, do not add other explanatory text.
- Strictly follow the above scoring criteria and format to complete the evaluation task.
\end{Verbatim}


\subsubsection{Prompt 2: Medication Consultation and Drug Safety QA}
\begin{Verbatim}[breaklines, breakanywhere]
# 1. Task Description
Task type: Evaluate the model's knowledge QA ability in medication consultation and provide a score.
Role definition: You are a senior clinical pharmacist/internal medicine expert and AI model evaluator. Your scoring is strict, focusing on the accuracy of pharmacology and pharmacokinetics data, the standardization of clinical medication, and the practicality and safety of medication advice.
Evaluation objective: Comprehensively assess the model's capabilities in drug therapy and pharmaceutical consultation, including mastery of indications, dosage and administration, contraindications, drug interactions, adverse reactions, and medication for special populations, as well as professional competence and practical guidance for safe patient use.
Evaluation task: Score the "candidate answer" according to the following "evaluation criteria" and "scoring guidelines".

# 2. Task Input
# Question ID:
{id}
# Input question:
{question}
# Model answer:
{answer}
# Reference answer:
{gold}

# 3. Evaluation Criteria (0–5 points)
{[}Scoring Criteria{]}:
Score 0: The candidate answer is completely unrelated to the reference, entirely wrong, or seriously violates pharmacological knowledge or medication safety principles (e.g., lethal dosage, contraindicated use causing severe outcomes), and is vague or completely incorrect;
Score 1: The candidate answer has extremely low relevance to the reference, or only superficial coverage, containing multiple serious errors in indications, dosage, or contraindications (e.g., missing critical adverse reaction warnings);
Score 2: The candidate answer is partially related to the reference but contains obvious dosage errors, missing drug interactions, or incomplete/inappropriate medication guidance (e.g., lacking warnings for special populations or important considerations);
Score 3: The candidate answer aligns with most key points of the reference but still has imprecise drug names/dosage forms, unclear dosage or administration instructions, or omits general precautions or critical details (e.g., timing of administration, dietary restrictions);
Score 4: The candidate answer largely matches the reference but slightly lacks detail, rigor in wording, or completeness of patient guidance (e.g., not proactively advising follow-up timing or storage conditions);
Score 5: The candidate answer is fully consistent with the reference, with no omissions or deviations, and the expression is clear and easy for patients to understand.

# 4. Scoring Guidelines
Follow the steps below to complete scoring:
1. Understand the task and medication standards: Fully understand the context of the medication consultation, model requirements, core points of the reference, and definitions of appropriateness, safety, and standardization in pharmaceutical practice.
2. Compare core points item by item: Carefully read the model's answer, compare it with the reference, and check coverage of all core points including disease name, drug name, indications, dosage, precautions, and contraindications. Verify correctness in dose, frequency, and route.
3. Evaluate correctness and safety: Determine whether the model's answer accurately verifies medication information and pay particular attention to safety (e.g., avoiding severe drug interactions or contraindications).
4. Assess pharmaceutical terminology and standardization: Check whether drug names, dosage forms, specifications, and instructions are precise and standardized, whether pharmacological explanations are clear and rigorous, and whether guidance is safe and reasonable. Strictly prohibit arbitrary substitution, generalization, or misinterpretation of drug names, doses, indications, or contraindications.
5. Avoid irrelevant content generalization: Discourage unrelated non-drug therapies or non-standard indications. Even if expansion is reasonable, if it exceeds the scope of the reference or deviates from the core goal of medication guidance, deduct points accordingly.
6. Final score: Integrate accuracy of medication information, safety, standardization, and practicality of guidance to assign an integer score 0–5, reflecting subtle differences in answer quality.

# 5. Output Format Requirement
# Task ID: {id}
<score> (Fill in the final score 0–5 here)</score>
Notes:
- Only fill in the content inside the <score> tag, do not add other explanatory text.
- Strictly follow the above scoring criteria and format to complete the evaluation task.
\end{Verbatim}


\subsection{Dataset 2: Multimodal Model  Benchmark}
\subsubsection{Prompt 1: Medical Image Reasoning}
\begin{Verbatim}[breaklines, breakanywhere]
# 1. Task Description
Task type: Evaluate the model's ability to perform complex clinical reasoning and QA based on medical images and textual information.

Role definition: You are an experienced senior attending physician. Your evaluation prioritizes accuracy, logical reasoning, and patient safety. You focus not only on correct diagnosis but also on the reasonableness and practicality of subsequent treatment and management suggestions.

Evaluation objective: Comprehensively assess the model's integration of visual perception, text understanding, and clinical knowledge reasoning. Core evaluation dimensions include:
1. Accuracy of visual information recognition: Can the model correctly identify and interpret key findings in the images?
2. Image-text integration: Can it effectively combine imaging findings with patient information (age, sex, etc.) provided in the question?
3. Logical and accurate clinical reasoning: Does the entire answer—from preliminary diagnosis to etiology inference and treatment suggestions—form a complete reasoning chain consistent with clinical logic?
4. Safety and practicality of recommendations: Are all suggestions (treatment, follow-up, etc.) safe, reasonable, and clinically meaningful?

Evaluation task: Score the "candidate answer" according to the following "evaluation criteria" and "scoring guidelines".

# 2. Task Input
# Question ID:
{id}
# Input question (including patient info and specific inquiry):
{question}
# Model answer:
{answer}
# Reference answer (expert answer):
{gold}

# 3. Evaluation Criteria (0–5 points)
{[}Scoring Criteria{]}:
Score 0: Provides dangerous or harmful medical advice (e.g., delaying emergency treatment) or completely misinterprets the images, rendering the entire answer meaningless or counterproductive.
Score 1: Severe misreading of key imaging information leads to completely wrong preliminary diagnosis, and subsequent reasoning and suggestions are entirely off track.
Score 2: Interpretation of images has significant deviations or omissions, resulting in inaccurate diagnosis; or diagnosis direction is roughly correct but subsequent reasoning (e.g., etiology, treatment advice) contains serious errors.
Score 3: Can basically identify core imaging findings and provide a mostly correct preliminary diagnosis, but later parts of the answer (e.g., treatment, follow-up) are incomplete, not specific enough, or have minor deviations.
Score 4: Correctly identifies image information, diagnoses correctly, and provides reasonable and safe answers for all aspects, but may lack comprehensiveness, personalization, or depth of explanation in some suggestions.
Score 5: Perfect answer. Not only accurately interprets images and provides precise diagnosis, but also gives logically clear, accurate, and clinically comprehensive recommendations on etiology, treatment, lifestyle impact, follow-up, etc., at the level of a senior expert.

# 4. Scoring Guidelines
Follow the steps below to complete scoring:
1. Visual grounding verification: This is the first and most critical step. Check whether the model's interpretation of images is correct. If the model misreads the image (e.g., treats abnormal as normal or vice versa), the upper limit of subsequent scores is strictly constrained.
2. Assess diagnostic accuracy: Once visual interpretation is confirmed, evaluate whether the preliminary diagnosis considering patient information (age, sex, etc.) is accurate.
3. Examine reasoning chain completeness and logic: Check responses on etiology inference, treatment suggestions, follow-up advice. Determine if they logically follow the diagnosis and align with modern medical knowledge.
4. Ensure safety and practicality: From a clinician's perspective, strictly assess whether all suggestions are safe, reasonable, and practical.
5. Evaluate comprehensiveness: Verify that the model answered all sub-questions and did not omit important information.
6. Final score: Integrate visual interpretation accuracy and safety of clinical advice, combined with reasoning logic and completeness, to assign an integer score 0–5 according to the criteria in section 3.

# 5. Output Format Requirement
# Task ID: {id}
<score> (Fill in the final score 0–5 here)</score>
Notes:
- Only fill in the content inside the <score> tag, do not add any other explanatory text.
- Strictly follow the above scoring criteria and format to complete the evaluation task.
\end{Verbatim}


\subsubsection{Prompt 2: Temporal Medical Image Sequence and Dynamic Trend Analysis}
\begin{Verbatim}[breaklines, breakanywhere]
# 1.Task Description 
Task type: Evaluate the model's ability to analyze medical image sequences, understand disease dynamics, and perform clinical reasoning.

Role definition: You are a senior sub-specialty clinical expert (e.g., oncology, hepatology) responsible for long-term patient management. Your evaluation is extremely strict, focusing not only on accurate identification of imaging findings but also on correct assessment of disease progression over time, and the accuracy of treatment effect evaluation and prognosis prediction based on this trend.

Evaluation objective: Comprehensively assess the model's ability to integrate image sequences, clinical data, and temporal information for complex dynamic analysis. Core evaluation dimensions include:
1. Temporal Change Recognition: Can the model accurately identify and describe changes in imaging findings (e.g., lesion size, number, morphology) across different time points?
2. Clinical Trend Interpretation: Can the identified imaging changes be correctly interpreted as clinically meaningful trends (e.g., "disease progression", "partial response", "stable disease")?
3. Evidence-based Reasoning: Can the model provide direct evidence from imaging changes to support its conclusions, forming a complete and clinically logical reasoning chain?
4. Prognosis & Suggestion Rationality: Based on observed trends, are predictions of future status or proposed next steps consistent with clinical practice and patient context?

Evaluation task: Score the model's answer according to the following "evaluation criteria" and "scoring guidelines".

# 2. Task Input
# Question ID:
{id}
# Input question (including clinical background and dynamic analysis task):
{question}
# Model answer:
{answer}
# Reference answer (expert answer):
{gold

# 3. Evaluation Criteria (0–5 points)
{[}Scoring Criteria{]}:
Score 0: Completely misjudges disease dynamics, providing conclusions directly opposite to reality (e.g., judging clear disease progression as effective response), potentially misleading clinical decision-making.
Score 1: Fails to perform effective temporal comparison, basing the answer almost entirely on a single time point, or, even if comparisons are made, key changes are seriously misdescribed, rendering the conclusion unreliable.
Score 2: Recognizes that changes exist but misjudges the nature or degree of the changes (e.g., exaggerating minor changes as significant response), or omits key changes critical to overall trend (e.g., appearance of new lesions).
Score 3: Correctly identifies the overall trend (e.g., "improvement" or "worsening"), but provides insufficient or imprecise reasoning, or minor errors in details.
Score 4: Accurately identifies trend and provides generally correct imaging evidence as reasoning, but lacks some depth, rigor, or professional terminology in analysis.
Score 5: Perfect answer. Accurately and comprehensively describes all key imaging changes over time, makes clinical trend judgments fully consistent with the reference (e.g., "partial response", "stable disease"), and provides clear, detailed, and logically rigorous evidence-based reasoning.

# 4. Scoring Guidelines
Follow the steps below to complete scoring:
1. Understand temporal context and reference standard: As a clinical expert, carefully review the image sequence in chronological order, and interpret the "reference answer" considering the clinical background (e.g., treatment regimen) provided in the question.
2. Core verification – correctly capturing the dynamic trend: This is the primary and critical step. Check whether the model's conclusion (e.g., "treatment effective", "disease progression") aligns with the reference. If the directional judgment is wrong, the maximum score is strictly capped at 2 points.
3. Examine the evidence chain: Check whether the reasons provided clearly indicate which imaging features changed across different time points (e.g., "Compared to pre-treatment, hepatic tumors significantly reduced post-treatment"). Evidence must be based on dynamic image comparison, not static description.
4. Assess comprehensiveness of analysis: Does the model consider all important changes? For example, when evaluating tumor treatment, does it consider not only the primary lesion but also new metastases or thrombus? Omitting new lesions is a serious error.
5. Evaluate professionalism of clinical terminology: Check whether the model uses professional and standard terminology when describing treatment effects (e.g., "partial response/PR", "complete response/CR", "stable disease/SD", "progressive disease/PD").
6. Final score: Integrate all points above, giving dynamic trend accuracy the highest weight, combined with evidence-based reasoning quality and analysis comprehensiveness, and assign an integer score 0–5 according to section 3, reflecting the model's advanced clinical reasoning ability.

# 5. Output Format Requirement
# Task ID: {id}
<score> (Fill in the final score 0–5 here)</score>
Notes:
- Only fill in the content inside the <score> tag, do not add any other explanatory text.
- Strictly follow the above scoring criteria and format to complete the evaluation task.
\end{Verbatim}


\end{document}
