@misc{ouyang2022training,
  title     = {Training Language Models to Follow Instructions with Human Feedback},
  author    = {Long Ouyang and Jeff Wu and Xu Jiang and Diogo Almeida and Carroll L. Wainwright and others},
  year      = {2022},
  eprint    = {2203.02155},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}


@misc{liu2024lipo,
  title     = {LiPO: Listwise Preference Optimization through Learning-to-Rank},
  author    = {Tianqi Liu and Zhen Qin and Junru Wu and Jiaming Shen and others},
  year      = {2024},
  eprint    = {2402.01878},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{zheng2023rlaif,
  title     = {RLAIF vs.\ RLHF: Scaling Reinforcement Learning from Human Feedback without Humans},
  author    = {Haotian Zheng and Andy Zou and Evan Chen and others},
  year      = {2023},
  eprint    = {2309.00267},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@misc{sun2023selfalign,
  title     = {Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision},
  author    = {Zhengbao Sun and Xinrui Yan and Tao Yu and Yoav Artzi},
  year      = {2023},
  eprint    = {2305.03047},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{wei2022cot,
  title     = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  author    = {Jason Wei and Xuezhi Wang and Dale Schuurmans and others},
  year      = {2022},
  eprint    = {2201.11903},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{wang2022sc,
  title     = {Self-Consistency Improves Chain of Thought Reasoning in Language Models},
  author    = {Xuezhi Wang and Jason Wei and Dale Schuurmans and others},
  year      = {2022},
  eprint    = {2203.11171},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{xu2024aot,
  title     = {Algorithm of Thoughts: Enhancing Exploration of Ideas in Large Language Models},
  author    = {Bo-Xuan Liu and Jianyu Xu and others},
  year      = {2024},
  eprint    = {2308.10379},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{kim2024selfee,
  title     = {Aligning Large Language Models with Self-generated Preference Data (Selfee)},
  author    = {Dongyoung Kim and Kimin Lee and Jinwoo Shin and Jaehyung Kim},
  year      = {2024},
  eprint    = {2406.04412},
  archivePrefix = {arXiv},
  primaryClass  = {cs.CL}
}

@misc{huang2024online,
  title     = {Self-Improving Efficient Online Alignment of Large Language Models},
  author    = {Haoyu Wang and Furong Huang and others},
  year      = {2024},
  eprint    = {2406.15567},
  archivePrefix = {arXiv},
  primaryClass  = {cs.LG}
}

@inproceedings{christiano2017,
  author  = {Christiano, Paul F. and Leike, Jan and Brown, Tom B. and Martic, Miljan and Legg, Shane and Amodei, Dario},
  title   = {Deep Reinforcement Learning from Human Preferences},
  booktitle = {Advances in Neural Information Processing Systems},
  year    = {2017},
  note    = {arXiv:1706.03741}
}

@article{ziegler2019,
  author  = {Ziegler, Daniel M. and Stiennon, Nisan and Wu, Jeffrey and et~al.},
  title   = {Fine-Tuning Language Models from Human Preferences},
  journal = {arXiv preprint arXiv:1909.08593},
  year    = {2019}
}

@inproceedings{stiennon2020,
  author  = {Stiennon, Nisan and Ouyang, Long and Wu, Jeff and et~al.},
  title   = {Learning to Summarize from Human Feedback},
  booktitle = {NeurIPS},
  year    = {2020},
  note    = {arXiv:2009.01325}
}



@article{wu2021,
  author  = {Wu, Jeff and Ouyang, Long and Ziegler, Daniel M. and et~al.},
  title   = {Recursively Summarizing Books with Human Feedback},
  journal = {arXiv preprint arXiv:2109.10862},
  year    = {2021}
}

@article{hong2025,
  author  = {Hong, Jiwoo and Lee, Noah and Kim, Eunki and et~al.},
  title   = {On the Robustness of Reward Models for Language Model Alignment},
  journal = {arXiv preprint arXiv:2505.07271},
  year    = {2025}
}



@article{muldrew2024,
  author  = {Muldrew, William and Hayes, Peter and Zhang, Mingtian and Barber, David},
  title   = {Active Preference Learning for Large Language Models},
  journal = {arXiv preprint arXiv:2402.08114},
  year    = {2024}
}

@article{maity2025,
  author  = {Maity, Subhankar and Saikia, Manob Jyoti},
  title   = {Large Language Models in Healthcare and Medical Applications: A Review},
  journal = {Bioengineering},
  volume  = {12},
  number  = {6},
  pages   = {631},
  year    = {2025},
  doi     = {10.3390/bioengineering12060631}
}

@inproceedings{achintalwar2024,
  author  = {Achintalwar, Swapnaja and Baldini, Ioana and Bouneffouf, Djallel and et~al.},
  title   = {Alignment Studio: Aligning Large Language Models to Particular Contextual Regulations},
  booktitle = {arXiv preprint arXiv:2403.09704},
  year    = {2024}
}

@inproceedings{kim2023,
  author  = {Kim, Sungdong and Bae, Sanghwan and Shin, Jamin and et~al.},
  title   = {Aligning Large Language Models through Synthetic Feedback},
  booktitle = {EMNLP},
  year    = {2023},
  note    = {arXiv:2305.13735}
}

@article{yu2025,
  author  = {Yu, Tianshu and Lin, Ting-En and Wu, Yuchuan and et~al.},
  title   = {Diverse AI Feedback for Large Language Model Alignment},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {13},
  pages   = {392--407},
  year    = {2025},
  doi     = {10.1162/tacl_a_00746}
}

@book{lambert2025,
  author  = {Lambert, Nathan},
  title   = {Reinforcement Learning from Human Feedback},
  publisher = {Self-published},
  year    = {2025},
  note    = {\url{https://rlhfbook.com}}
}

@inproceedings{gong2025,
  author  = {Gong, Zhuocheng and Guan, Jian and Wu, Wei and et~al.},
  title   = {Latent Preference Coding: Aligning Large Language Models via Discrete Latent Codes},
  booktitle = {ICML},
  year    = {2025}
}

@misc{gu2025surveyllmasajudge,
      title={A Survey on LLM-as-a-Judge}, 
      author={Jiawei Gu and Xuhui Jiang and Zhichao Shi and Hexiang Tan and Xuehao Zhai and Chengjin Xu and Wei Li and Yinghan Shen and Shengjie Ma and Honghao Liu and Saizhuo Wang and Kun Zhang and Yuanzhuo Wang and Wen Gao and Lionel Ni and Jian Guo},
      year={2025},
      eprint={2411.15594},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2411.15594}, 
}

@inproceedings{gpt4_as_judge_2024,
  author  = {Koutcheme, Charles and Dainese, Nicola and Sarsa, Sami and others},
  title   = {Open Source Language Models Can Provide Feedback: Evaluating LLMs' Ability to Help Students Using GPT-4-As-A-Judge},
  booktitle= {ITiCSE 2024},
  year    = {2024},
  note    = {arXiv:2405.05253}
}

@article{meta_rewarding_2024,
  author  = {Wu, Tianhao and Yuan, Weizhe and Golovneva, Olga and others},
  title   = {Meta-Rewarding Language Models: Self-Improving Alignment with LLM-as-a-Meta-Judge},
  journal = {arXiv preprint arXiv:2407.19594},
  year    = {2024}
}

@inproceedings{proto_rm_2024,
  author  = {Zhang, Jinghan and Wang, Xiting and Jin, Yiqiao and others},
  title   = {Prototypical Reward Network for Data-Efficient RLHF},
  booktitle= {ACL 2024},
  year    = {2024},
  note    = {arXiv:2406.06606}
}

@article{rm_benchmark_2024,
  author  = {Zhou, Enyu and Zheng, Guodong and Wang, Binghai and others},
  title   = {RMB: Comprehensively Benchmarking Reward Models in LLM Alignment},
  journal = {arXiv preprint arXiv:2410.09893},
  year    = {2024}
}

@article{dpo_2023,
  author  = {Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and others},
  title   = {Direct Preference Optimization: Your Language Model is Secretly a Reward Model},
  journal = {Advances in Neural Information Processing Systems},
  year    = {2023}
}

@article{dporm_limit_2024,
  author  = {Lin, Yong and Seto, Skyler and ter Hoeve, Maartje and others},
  title   = {On the Limited Generalization Capability of the Implicit Reward Model Induced by Direct Preference Optimization},
  journal = {arXiv preprint arXiv:2409.03650},
  year    = {2024}
}

@inproceedings{rlaif_2023,
  author  = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and others},
  title   = {RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
  booktitle= {ICML 2024 Proceedings},
  year    = {2023}
}

@article{rm_scaling_2022,
  author  = {Gao, Leo and Schulman, John and Hilton, Jacob},
  title   = {Scaling Laws for Reward Model Overoptimization},
  journal = {arXiv preprint arXiv:2210.10760},
  year    = {2022}
}



@inproceedings{gram_2025,
  author  = {Wang, Chenglong and Gan, Yang and Huo, Yifu and others},
  title   = {GRAM: A Generative Foundation Reward Model for Reward Generalization},
  booktitle= {ICML 2025},
  year    = {2025}
}

@article{meta_reuters_2024,
  author  = {Paul, Katie},
  title   = {Meta releases AI model that can check other AI models' work},
  journal = {Reuters},
  year    = {2024},
  note    = {Oct 18 2024, Technology section}
}

@misc{rl_dpo_blog_2024,
  author  = {{CrowdWorks AI Blog}},
  title   = {RLHF and DPO Compared},
  howpublished = {\url{https://crowdworks.blog/en/rlhf-and-dpo-compared/}},
  year    = {2024}
}

@article{deepmind_wired_2017,
  author  = {Temperton, James},
  title   = {DeepMind’s AI Can Now Learn from Human Preferences},
  journal = {WIRED},
  year    = {2017},
  note    = {June 14 2017}
}

@article{bradley1952,
  author  = {Bradley, Ralph Allan and Terry, Milton E.},
  title   = {Rank Analysis of Incomplete Block Designs: The Method of Paired Comparisons},
  journal = {Biometrika},
  volume  = {39},
  number  = {3--4},
  pages   = {324--345},
  year    = {1952}
}

@inproceedings{herbrich2007,
  author  = {Herbrich, Ralf and Minka, Tom and Graepel, Thore},
  title   = {TrueSkill\textsuperscript{\texttrademark}: A Bayesian Skill Rating System},
  booktitle = {NeurIPS},
  year    = {2007}
}

@article{bai2022harmless,
  author  = {Bai, Yuntao and Jones, Andy and Ndousse, Kamal and et~al.},
  title   = {Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback},
  journal = {arXiv preprint arXiv:2204.05862},
  year    = {2022}
}

@article{bai2022const,
  author  = {Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and et~al.},
  title   = {Constitutional AI: Harmlessness from AI Feedback},
  journal = {arXiv preprint arXiv:2212.08073},
  year    = {2022}
}



@article{lee2023rlaif,
  author  = {Lee, Harrison and Phatale, Samrat and Mansoor, Hassan and et~al.},
  title   = {RLAIF vs. RLHF: Scaling Reinforcement Learning from Human Feedback with AI Feedback},
  journal = {arXiv preprint arXiv:2309.00267},
  year    = {2023}
}





@article{kojima2022zsr,
  author  = {Kojima, Takeshi and Gu, Shixiang Shane and Reid, Machel and et~al.},
  title   = {Large Language Models are Zero-Shot Reasoners},
  journal = {arXiv preprint arXiv:2205.11916},
  year    = {2022}
}

@article{yuan2024,
  author  = {Yuan, Weizhe and Pang, Richard Yuanzhe and Cho, Kyunghyun and Li, Xian and Sukhbaatar, Sainbayar and Xu, Jing and Weston, Jason},
  title   = {Self-Rewarding Language Models},
  journal = {arXiv preprint arXiv:2401.10020},
  year    = {2024}
}

@article{sheikh2025,
  author  = {Shafayat, Sheikh and Tajwar, Fahim and Salakhutdinov, Ruslan and Schneider, Jeff and Zanette, Andrea},
  title   = {Can Large Reasoning Models Self-Train?},
  journal = {arXiv preprint arXiv:2505.21444},
  year    = {2025}
}



@inproceedings{wan2023,
  author  = {Wan, Xingchen and Sun, Ruoxi and Dai, Hanjun and Ar{\i}k, Sercan \"{O}. and Pfister, Tomas},
  title   = {Better Zero-Shot Reasoning with Self-Adaptive Prompting},
  booktitle = {Findings of ACL},
  year    = {2023}
}

@article{liu2023,
  author  = {Liu, Ziyou and Zheng, Yixin and Huang, An and Tang, Tianyi and Jiang, Minqi and Pan, Liangming},
  title   = {G-Eval: NLG Evaluation Using GPT-4 with Better Human Alignment},
  journal = {arXiv preprint arXiv:2303.16634},
  year    = {2023}
}

@article{li2024llms,
  title={Llms-as-judges: a comprehensive survey on llm-based evaluation methods},
  author={Li, Haitao and Dong, Qian and Chen, Junjie and Su, Huixue and Zhou, Yujia and Ai, Qingyao and Ye, Ziyi and Liu, Yiqun},
  journal={arXiv preprint arXiv:2412.05579},
  year={2024}
}

@article{agarwal2025,
  author  = {Agarwal, Shivam and Zhang, Zimin and Yuan, Lifan and Han, Jiawei and Peng, Hao},
  title   = {The Unreasonable Effectiveness of Entropy Minimization in LLM Reasoning},
  journal = {arXiv preprint arXiv:2505.15134},
  year    = {2025}
}

@article{farquhar2024,
  author  = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  title   = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  journal = {Nature},
  volume  = {630},
  pages   = {625--630},
  year    = {2024},
  doi     = {10.1038/s41586-024-07421-0}
}

@inproceedings{wang2025fork,
  author  = {Wang, Shenzhi and Yu, Le and Gao, Chang and Zheng, Chujie and Liu, Shixuan and others},
  title   = {Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning},
  booktitle = {ACL},
  year    = {2025},
  note    = {arXiv:2506.01939}
}

@misc{rlvr2025,
  author  = {Nikhil},
  title   = {High-Entropy Token Selection in Reinforcement Learning with Verifiable Rewards Improves Accuracy and Reduces Training Cost for LLMs},
  howpublished = {\url{https://www.marktechpost.com/2025/06/08/high-entropy-token-selection-in-reinforcement-learning-with-verifiable-rewards-rlvr-improves-accuracy-and-reduces-training-cost-for-llms/}},
  year    = {2025}
}

@inproceedings{chen2024,
  author  = {Chen, Xin and Huang, Hanxian and Gao, Yanjun and Wang, Yi and Zhao, Jishen and Ding, Ke},
  title   = {Learning to Maximize Mutual Information for Chain-of-Thought Distillation},
  booktitle = {Findings of ACL},
  year    = {2024}
}

@article{wu2024,
  author  = {Wu, Zongqian and Xu, Baoduo and Cui, Ruochen and Zhan, Mengmeng and Zhu, Xiaofeng and Feng, Lei},
  title   = {Rethinking Chain-of-Thought from the Perspective of Self-Training},
  journal = {arXiv preprint arXiv:2412.10827},
  year    = {2024}
}






@inproceedings{zhao2024gpo,
  author  = {Zhao, Siyan and Dang, John and Grover, Aditya},
  title   = {Group Preference Optimization: Few-Shot Alignment of Large Language Models},
  booktitle = {ICLR},
  year    = {2024},
  note    = {arXiv:2310.11523}
}

@article{vanlioglu2025,
  author  = {Vanlioglu, Abdullah},
  title   = {Entropy-Guided Sequence Weighting for Efficient Exploration in RL-Based LLM Fine-Tuning},
  journal = {arXiv preprint arXiv:2503.22456},
  year    = {2025}
}

@inproceedings{mnih2016a3c,
  author  = {Mnih, Volodymyr and Badia, Adri{\`a} Puigdom{\`e}nech and Mirza, Mehdi and et~al.},
  title   = {Asynchronous Methods for Deep Reinforcement Learning},
  booktitle = {ICML},
  year    = {2016},
  note    = {arXiv:1602.01783}
}

@article{schulman2017ppo,
  author  = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  title   = {Proximal Policy Optimization Algorithms},
  journal = {arXiv preprint arXiv:1707.06347},
  year    = {2017}
}

@article{williams1992,
  author  = {Williams, Ronald J.},
  title   = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  journal = {Machine Learning},
  volume  = {8},
  number  = {3--4},
  pages   = {229--256},
  year    = {1992}
}

@article{hao2025opo,
  author  = {Hao, Yaru and Li, Dong and Wu, Xun and et~al.},
  title   = {On-Policy Reinforcement Learning with Optimal Reward Baseline},
  journal = {arXiv preprint arXiv:2505.23585},
  year    = {2025}
}



@article{stolfo2024entropy,
  author  = {Stolfo, Alessandro and Wu, Ben and Gurnee, Wes and et~al.},
  title   = {Confidence Regulation Neurons in Language Models},
  journal = {arXiv preprint arXiv:2406.16254},
  year    = {2024}
}

@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Yang and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}


@article{selfrefine2024,
  author  = {Ranaldi, Leonardo and Freitas, Andr\`e},
  title   = {Self-Refine Instruction-Tuning for Aligning Reasoning in Language Models},
  journal = {arXiv preprint arXiv:2405.00402},
  year    = {2024}
}






@article{dposurvey2024,
  author  = {Zhang, Tianxiao and Liu, Jiawei and others},
  title   = {A Comprehensive Survey of Direct Preference Optimization},
  journal = {arXiv preprint arXiv:2410.15595},
  year    = {2024}
}

@article{song2023pro,
  author  = {Song, Feifan and Yu, Bowen and Li, Minghao and et~al.},
  title   = {Preference Ranking Optimization for Human Alignment},
  journal = {arXiv preprint arXiv:2306.17492},
  year    = {2023}
}

@article{ipo2025,
  author  = {Garg, Shivank and Singh, Ayush and Singh, Shweta and Chopra, Paras},
  title   = {IPO: Your Language Model Is Secretly a Preference Classifier},
  journal = {arXiv preprint arXiv:2502.16182},
  year    = {2025}
}


@misc{openai2024gpt4o,
  author  = {OpenAI},
  title   = {Hello GPT-4o},
  howpublished = {\url{https://openai.com/index/hello-gpt-4o/}},
  year    = {2024}
}

@article{tomsguide2025gpt4o,
  author  = {Hill, Kaycee},
  title   = {Stop Wasting Time with the Wrong ChatGPT Model — Here's How to Choose the Right One},
  journal = {Tom's Guide},
  year    = {2025},
  note    = {May 24, 2025}
}

@misc{medium2023rlhf,
  author  = {Prashant, Madhur},
  title   = {RLHF + Reward Model + PPO on LLMs},
  howpublished = {Medium blog},
  year    = {2023},
  note    = {September 10, 2023}
}

@article{wired2017deepmind,
  author  = {Temperton, James},
  title   = {DeepMind’s AI Can Now Learn from Human Preferences},
  journal = {WIRED},
  year    = {2017},
  note    = {June 14, 2017}
}


@article{plackett1975,
  title={The Analysis of Permutations},
  author={Plackett, Robin L.},
  journal={Applied Statistics},
  volume={24},
  pages={193--202},
  year={1975}
}

@article{azar2023psipo,
  title={A General Theoretical Paradigm to Understand Learning from Preferences},
  author={Gheshlaghi Azar, Mohammad and coauthors},
  journal={arXiv preprint arXiv:2310.12036},
  year={2023}
}

@article{zhou2025self,
  title={Self-Consistency of Internal Reward Models Improves Alignment},
  author={Zhou, Xinyu and others},
  journal={arXiv preprint arXiv:2502.08922},
  year={2025}
}

@inproceedings{stein2025uncertainty,
  title={Measuring Language Model Uncertainty with Internal Entropy},
  author={Stein, Adam and colleagues},
  booktitle={International Conference on Learning Representations},
  year={2025}
}

@misc{vllm2023,
  title={vLLM: Fast and Memory-Efficient LLM Serving},
  author={Rozi, Emma and contributors},
  howpublished={\url{https://github.com/vllm-project/vllm}},
  year={2023}
}

@article{greensmith2004variance,
  title={Variance Reduction Techniques for Gradient Estimates in Reinforcement Learning},
  author={Greensmith, Evan and Bartlett, Peter L. and Baxter, Jonathan},
  journal={Journal of Machine Learning Research},
  volume={5},
  pages={1471--1530},
  year={2004}
}

@misc{weng2018policygrad,
  title={Policy Gradient Algorithms},
  author={Weng, Lilian},
  howpublished={\url{https://lilianweng.github.io/posts/2018-04-08-policy-gradient/}},
  year={2018}
}

@article{fan2022ranking,
  title={Ranking Inferences Based on the Top Choice of Multiway Comparisons},
  author={Fan, Jianqing and coauthors},
  journal={arXiv preprint arXiv:2211.11957},
  year={2022}
}

@article{preference2024multi,
  title={Preference Optimization with Multi-Sample Comparisons},
  author={Liang, Yufeng and colleagues},
  journal={arXiv preprint arXiv:2410.12138},
  year={2024}
}

@article{liu2024skywork,
  author  = {Liu, Chengyu and Zeng, Liang and Xiao, Yifan and He, Jing and Yan, Ruize},
  title   = {Skywork-Reward: Bag of Tricks for Reward Modeling in LLMs},
  journal = {arXiv preprint arXiv:2410.18451},
  year    = {2024}
}


@article{cui2023ultra,
  author  = {Cui, Ganqu and Yuan, Lifan and Ding, Ning and et~al.},
  title   = {UltraFeedback: Boosting Language Models with Scaled AI Feedback},
  journal = {arXiv preprint arXiv:2310.01377},
  year    = {2023}
}


@article{ji2024pku,
  author  = {Ji, Jiaming and Hong, Donghai and Zhang, Borong and et~al.},
  title   = {PKU-SafeRLHF: Towards Multi-Level Safety Alignment for LLMs with Human Preference},
  journal = {arXiv preprint arXiv:2406.15513},
  year    = {2024}
}

@article{lambert2024rewardbench,
  author  = {Lambert, Nathan and Bakhtin, Anton and Smith, Noah A.},
  title   = {RewardBench: Evaluating Reward Models for Language Systems},
  journal = {arXiv preprint arXiv:2403.13787},
  year    = {2024}
}

@article{cobbe2021gsm8k,
  author  = {Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and et~al.},
  title   = {Training Verifiers to Solve Math Word Problems},
  journal = {arXiv preprint arXiv:2110.14168},
  year    = {2021}
}

@article{hendrycks2021math,
  author  = {Hendrycks, Dan and Burns, Collin and Kadavath, Saurav and et~al.},
  title   = {Measuring Mathematical Problem Solving With the MATH Dataset},
  journal = {arXiv preprint arXiv:2103.03874},
  year    = {2021}
}

@article{lin2021truthfulqa,
  author  = {Lin, Stephanie and Hilton, Jacob and Evans, Owain},
  title   = {TruthfulQA: Measuring How Models Mimic Human Falsehoods},
  journal = {arXiv preprint arXiv:2109.07958},
  year    = {2021}
}

@inproceedings{gehman2020rtp,
  author  = {Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A.},
  title   = {RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models},
  booktitle = {Findings of EMNLP},
  pages   = {3356--3369},
  year    = {2020}
}

@article{kahneman2011thinking,
  title={Thinking, fast and slow},
  author={Kahneman, Daniel},
  journal={Farrar, Straus and Giroux},
  year={2011}
}

@book{keeney1993decisions,
  title={Decisions with multiple objectives: preferences and value trade-offs},
  author={Keeney, Ralph L and Raiffa, Howard},
  year={1993},
  publisher={Cambridge university press}
}

@article{hong2004groups,
  title={Groups of diverse problem solvers can outperform groups of high-ability problem solvers},
  author={Hong, Lu and Page, Scott E},
  journal={Proceedings of the National Academy of Sciences},
  volume={101},
  number={46},
  pages={16385--16389},
  year={2004},
  publisher={National Academy of Sciences}
}

@article{zhao2021brain,
  title={Brain-inspired search engine assistant based on knowledge graph},
  author={Zhao, Xuejiao and Chen, Huanhuan and Xing, Zhenchang and Miao, Chunyan},
  journal={IEEE Transactions on Neural Networks and Learning Systems},
  volume={34},
  number={8},
  pages={4386--4400},
  year={2021},
  publisher={IEEE}
}

@inproceedings{zhao2025medrag,
  title={Medrag: Enhancing retrieval-augmented generation with knowledge graph-elicited reasoning for healthcare copilot},
  author={Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
  booktitle={Proceedings of the ACM on Web Conference 2025},
  pages={4442--4457},
  year={2025}
}

@article{zhao2025smart,
  title={A smart multimodal healthcare copilot with powerful llm reasoning},
  author={Zhao, Xuejiao and Liu, Siyan and Yang, Su-Yin and Miao, Chunyan},
  journal={arXiv preprint arXiv:2506.02470},
  year={2025}
}

@article{zhao2025gfriend,
  title={GFRIEND: Generative Few-shot Reward Inference through EfficieNt DPO},
  author={Zhao, Yiyang and Bai, Huiyu and Zhao, Xuejiao},
  journal={arXiv preprint arXiv:2506.08965},
  year={2025}
}

@article{rao2025survey,
  title={A survey of artificial intelligence in gait-based neurodegenerative disease diagnosis},
  author={Rao, Haocong and Zeng, Minlin and Zhao, Xuejiao and Miao, Chunyan},
  journal={Neurocomputing},
  volume={626},
  pages={129533},
  year={2025},
  publisher={Elsevier}
}

@inproceedings{hong2021learning,
  title={Learning from limited labels for long legal dialogue},
  author={Hong, Jenny and Chong, Derek and Manning, Christopher D},
  booktitle={Proceedings of the Natural Legal Language Processing Workshop 2021},
  pages={190--204},
  year={2021}
}

@article{yang2025ehrstruct,
  title={EHRStruct: A Comprehensive Benchmark Framework for Evaluating Large Language Models on Structured Electronic Health Record Tasks},
  author={Yang, Xiao and Zhao, Xuejiao and Shen, Zhiqi},
  journal={arXiv preprint arXiv:2511.08206},
  year={2025}
}

@article{zhou2025reagent,
  title={ReAgent-V: A Reward-Driven Multi-Agent Framework for Video Understanding},
  author={Zhou, Yiyang and He, Yangfan and Su, Yaofeng and Han, Siwei and Jang, Joel and Bertasius, Gedas and Bansal, Mohit and Yao, Huaxiu},
  journal={arXiv preprint arXiv:2506.01300},
  year={2025}
}

@article{zhou2024human,
  title={Human-centric reward optimization for reinforcement learning-based automated driving using large language models},
  author={Zhou, Ziqi and Zhang, Jingyue and Zhang, Jingyuan and He, Yangfan and Wang, Boyue and Shi, Tianyu and Khamis, Alaa},
  journal={arXiv preprint arXiv:2405.04135},
  year={2024}
}

@article{wang2024enhancing,
  title={Enhancing Code LLMs with Reinforcement Learning in Code Generation: A Survey},
  author={Wang, Junqiao and Zhang, Zeng and He, Yangfan and Zhang, Zihao and Song, Yuyang and Shi, Tianyu and Li, Yuchen and Xu, Hengyuan and Wu, Kunyu and Yi, Xin and others},
  journal={arXiv preprint arXiv:2412.20367},
  year={2024}
}

@article{xin2024parameter,
  title={Parameter-efficient fine-tuning for pre-trained vision models: A survey},
  author={Xin, Yi and Luo, Siqi and Zhou, Haodi and Du, Junlong and Liu, Xiaohong and Fan, Yue and Li, Qing and Du, Yuntao},
  journal={arXiv preprint arXiv:2402.02242},
  year={2024}
}

@article{chinta2024ai,
  title={Ai-driven healthcare: A survey on ensuring fairness and mitigating bias},
  author={Chinta, Sribala Vidyadhari and Wang, Zichong and Zhang, Xingyu and Viet, Thang Doan and Kashif, Ayesha and Smith, Monique Antoinette and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2407.19655},
  year={2024}
}

@article{tang2022attention,
  title={Attention Mechanism based Cognition-level Scene Understanding},
  author={Tang, Xuejiao and Quy, Tai Le and Ntoutsi, Eirini and Turner, Kea and Palade, Vasile and Haque, Israat and Xu, Peng and Brown, Chris and Zhang, Wenbin},
  journal={arXiv preprint arXiv:2204.08027},
  year={2022}
}

@inproceedings{tang2021interpretable,
  title={Interpretable visual understanding with cognitive attention network},
  author={Tang, Xuejiao and Zhang, Wenbin and Yu, Yi and Turner, Kea and Derr, Tyler and Wang, Mengyu and Ntoutsi, Eirini},
  booktitle={International Conference on Artificial Neural Networks},
  pages={555--568},
  year={2021},
  organization={Springer}
}

@inproceedings{li2025multi,
  title={Multi-Modal Large Language Model with RAG Strategies in Soccer Commentary Generation},
  author={Li, Xiang and He, Yangfan and Zu, Shuaishuai and Li, Zhengyang and Shi, Tianyu and Xie, Yiting and Zhang, Kevin},
  booktitle={2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)},
  pages={6197--6206},
  year={2025},
  organization={IEEE}
}


@article{wang2025twin,
  title={Twin Co-Adaptive Dialogue for Progressive Image Generation},
  author={Wang, Jianhui and He, Yangfan and Zhong, Yan and Song, Xinyuan and Su, Jiayi and Feng, Yuheng and He, Hongyang and Zhu, Wenyu and Yuan, Xinhang and Lu, Kuan and others},
  journal={arXiv preprint arXiv:2504.14868},
  year={2025}
}

