@article{liu2024lost,
  title={Lost in the Middle: How Language Models Use Long Contexts},
  author={Liu, Nelson F and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
  journal={Transactions of the Association for Computational Linguistics},
  volume={12},
  year={2024}
}

@article{bedi2024systematic,
  title={A systematic review of testing and evaluation of healthcare applications of large language models (LLMs)},
  author={Bedi, Suhana and Liu, Yutong and Orr-Ewing, Lucy and Dash, Dev and Koyejo, Sanmi and Callahan, Alison and Fries, Jason A and Wornow, Michael and Swaminathan, Akshay and Lehmann, Lisa Soleymani and others},
  journal={medRxiv},
  pages={2024--04},
  year={2024},
  publisher={Cold Spring Harbor Laboratory Press}
}

@inproceedings{liuagentbench,
  title={AgentBench: Evaluating LLMs as Agents},
  author={Liu, Xiao and Yu, Hao and Zhang, Hanchen and Xu, Yifan and Lei, Xuanyu and Lai, Hanyu and Gu, Yu and Ding, Hangliang and Men, Kaiwen and Yang, Kejuan and others},
  booktitle={The Twelfth International Conference on Learning Representations}
}

@article{pawar2024and,
  title={The What, Why, and How of Context Length Extension Techniques in Large Language Models--A Detailed Survey},
  author={Pawar, Saurav and Tonmoy, SM and Zaman, SM and Jain, Vinija and Chadha, Aman and Das, Amitava},
  journal={arXiv preprint arXiv:2401.07872},
  year={2024}
}

@article{wang2025automating,
  title={Automating a complete software test process using LLMs: An automotive case study},
  author={Wang, Shuai and Yu, Yinan and Feldt, Robert and Parthasarathy, Dhasarathy},
  journal={arXiv preprint arXiv:2502.04008},
  year={2025}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@misc{chiang2023vicuna,
  title        = {Vicuna: An open-source chatbot impressing GPT-4 with 90\%* ChatGPT quality},
  author       = {Wei-Lin Chiang and Zhuohan Li and Zi Lin and Ying Sheng and Zhanghao Wu and Hao Zhang and Lianmin Zheng and Siyuan Zhuang and Yonghao Zhuang and Joseph E. Gonzalez and Ion Stoica},
  year         = {2023},
  howpublished = {\url{https://vicuna.lmsys.org}},
  note         = {Accessed: 2023-04-14}
}


@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{hsieh2024ruler,
  title={RULER: What's the Real Context Size of Your Long-Context Language Models?},
  author={Hsieh, Cheng-Ping and Sun, Simeng and Kriman, Samuel and Acharya, Shantanu and Rekesh, Dima and Jia, Fei and Zhang, Yang and Ginsburg, Boris},
  journal={arXiv preprint arXiv:2404.06654},
  year={2024}
}

@article{bai2023longbench,
  title={Longbench: A bilingual, multitask benchmark for long context understanding},
  author={Bai, Yushi and Lv, Xin and Zhang, Jiajie and Lyu, Hongchang and Tang, Jiankai and Huang, Zhidian and Du, Zhengxiao and Liu, Xiao and Zeng, Aohan and Hou, Lei and others},
  journal={arXiv preprint arXiv:2308.14508},
  year={2023}
}

@article{shaham2022scrolls,
  title={Scrolls: Standardized comparison over long language sequences},
  author={Shaham, Uri and Segal, Elad and Ivgi, Maor and Efrat, Avia and Yoran, Ori and Haviv, Adi and Gupta, Ankit and Xiong, Wenhan and Geva, Mor and Berant, Jonathan and others},
  journal={arXiv preprint arXiv:2201.03533},
  year={2022}
}

@inproceedings{hsieh2024found,
  title={Found in the middle: Calibrating Positional Attention Bias Improves Long Context Utilization},
  author={Hsieh, Cheng-Yu and Chuang, Yung-Sung and Li, Chun-Liang and Wang, Zifeng and Le, Long and Kumar, Abhishek and Glass, James and Ratner, Alexander and Lee, Chen-Yu and Krishna, Ranjay and others},
  booktitle={Findings of the Association for Computational Linguistics ACL 2024},
  pages={14982--14995},
  year={2024}
}

@article{zhang2025lost,
  title={Lost-in-the-Middle in Long-Text Generation: Synthetic Dataset, Evaluation Framework, and Mitigation},
  author={Zhang, Junhao and Zhang, Richong and Kong, Fanshuang and Miao, Ziyang and Ye, Yanhan and Zheng, Yaowei},
  journal={arXiv preprint arXiv:2503.06868},
  year={2025}
}

@misc{kaiokendev_context_2023,
  author       = {kaiokendev},
  title        = {Extending Context is Hard â€¦ but not Impossible},
  howpublished = {\url{https://kaiokendev.github.io/context}},
  year         = {2023},
  note         = {Accessed: 2025-10-01}
}

@article{chen2023extending,
  title={Extending context window of large language models via positional interpolation},
  author={Chen, Shouyuan and Wong, Sherman and Chen, Liangjian and Tian, Yuandong},
  journal={arXiv preprint arXiv:2306.15595},
  year={2023}
}

@article{su2024roformer,
  title={RoFormer: Enhanced transformer with Rotary Position Embedding},
  author={Su, Jianlin and Ahmed, Murtadha and Lu, Yu and Pan, Shengfeng and Bo, Wen and Liu, Yunfeng},
  journal={Neurocomputing},
  volume={568},
  pages={127063},
  year={2024},
  publisher={Elsevier}
}

@article{peng2023yarn,
  title={Yarn: Efficient context window extension of large language models},
  author={Peng, Bowen and Quesnelle, Jeffrey and Fan, Honglu and Shippole, Enrico},
  journal={arXiv preprint arXiv:2309.00071},
  year={2023}
}

@article{zhang2024found,
  title={Found in the middle: How language models use long contexts better via plug-and-play positional encoding},
  author={Zhang, Zhenyu and Chen, Runjin and Liu, Shiwei and Yao, Zhewei and Ruwase, Olatunji and Chen, Beidi and Wu, Xiaoxia and Wang, Zhangyang and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={60755--60775},
  year={2024}
}

@article{yu2024mitigate,
  title={Mitigate position bias in large language models via scaling a single dimension},
  author={Yu, Yijiong and Jiang, Huiqiang and Luo, Xufang and Wu, Qianhui and Lin, Chin-Yew and Li, Dongsheng and Yang, Yuqing and Huang, Yongfeng and Qiu, Lili},
  journal={arXiv preprint arXiv:2406.02536},
  year={2024}
}

@article{an2024make,
  title={Make your llm fully utilize the context},
  author={An, Shengnan and Ma, Zexiong and Lin, Zeqi and Zheng, Nanning and Lou, Jian-Guang and Chen, Weizhu},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={62160--62188},
  year={2024}
}

@article{hu2022lora,
  title={Lora: Low-rank adaptation of large language models.},
  author={Hu, Edward J and Shen, Yelong and Wallis, Phillip and Allen-Zhu, Zeyuan and Li, Yuanzhi and Wang, Shean and Wang, Lu and Chen, Weizhu and others},
  journal={ICLR},
  volume={1},
  number={2},
  pages={3},
  year={2022}
}

@article{xiao2023efficient,
  title={Efficient streaming language models with attention sinks},
  author={Xiao, Guangxuan and Tian, Yuandong and Chen, Beidi and Han, Song and Lewis, Mike},
  journal={arXiv preprint arXiv:2309.17453},
  year={2023}
}

@inproceedings{kandpal2023large,
  title={Large language models struggle to learn long-tail knowledge},
  author={Kandpal, Nikhil and Deng, Haikang and Roberts, Adam and Wallace, Eric and Raffel, Colin},
  booktitle={International conference on machine learning},
  pages={15696--15707},
  year={2023},
  organization={PMLR}
}

@article{mallen2022not,
  title={When not to trust language models: Investigating effectiveness of parametric and non-parametric memories},
  author={Mallen, Alex and Asai, Akari and Zhong, Victor and Das, Rajarshi and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10511},
  year={2022}
}

@misc{kamradt2023needle,
  author       = {Gregory Kamradt},
  title        = {Needle In A Haystack - Pressure Testing LLMs},
  howpublished = {\url{https://github.com/gkamradt/LLMTestNeedleInAHaystack/tree/main}},
  year         = {2023}
}

@article{comanici2025gemini,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@article{rando2025longcodebench,
  title={LongCodeBench: Evaluating Coding LLMs at 1M Context Windows},
  author={Rando, Stefano and Romani, Luca and Sampieri, Alessio and Franco, Luca and Yang, John and Kyuragi, Yuta and Galasso, Fabio and Hashimoto, Tatsunori},
  journal={arXiv preprint arXiv:2505.07897},
  year={2025}
}

@article{peysakhovich2023attention,
  title={Attention sorting combats recency bias in long context language models},
  author={Peysakhovich, Alexander and Lerer, Adam},
  journal={arXiv preprint arXiv:2310.01427},
  year={2023}
}

@article{chen2023fortify,
  title={Fortify the shortest stave in attention: Enhancing context awareness of large language models for effective tool use},
  author={Chen, Yuhan and Lv, Ang and Lin, Ting-En and Chen, Changyu and Wu, Yuchuan and Huang, Fei and Li, Yongbin and Yan, Rui},
  journal={arXiv preprint arXiv:2312.04455},
  year={2023}
}

@article{chen2023clex,
  title={Clex: Continuous length extrapolation for large language models},
  author={Chen, Guanzheng and Li, Xin and Meng, Zaiqiao and Liang, Shangsong and Bing, Lidong},
  journal={arXiv preprint arXiv:2310.16450},
  year={2023}
}

@article{jin2024llm,
  title={Llm maybe longlm: Self-extend llm context window without tuning},
  author={Jin, Hongye and Han, Xiaotian and Yang, Jingfeng and Jiang, Zhimeng and Liu, Zirui and Chang, Chia-Yuan and Chen, Huiyuan and Hu, Xia},
  journal={arXiv preprint arXiv:2401.01325},
  year={2024}
}

@article{press2021train,
  title={Train short, test long: Attention with linear biases enables input length extrapolation},
  author={Press, Ofir and Smith, Noah A and Lewis, Mike},
  journal={arXiv preprint arXiv:2108.12409},
  year={2021}
}

@article{ding2024longrope,
  title={Longrope: Extending llm context window beyond 2 million tokens},
  author={Ding, Yiran and Zhang, Li Lyna and Zhang, Chengruidong and Xu, Yuanyuan and Shang, Ning and Xu, Jiahang and Yang, Fan and Yang, Mao},
  journal={arXiv preprint arXiv:2402.13753},
  year={2024}
}

@article{mohtashami2023random,
  title={Random-access infinite context length for transformers},
  author={Mohtashami, Amirkeivan and Jaggi, Martin},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={54567--54585},
  year={2023}
}