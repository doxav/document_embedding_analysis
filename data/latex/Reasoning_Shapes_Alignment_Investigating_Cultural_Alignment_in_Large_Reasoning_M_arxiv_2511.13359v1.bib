@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}


@inproceedings{kwon2023efficient,
  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},
  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},
  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},
  year={2023}
}
@inproceedings{huang-etal-2024-far,
    title = "How Far Can In-Context Alignment Go? Exploring the State of In-Context Alignment",
    author = "Huang, Heyan  and
      Li, Yinghao  and
      Sun, Huashan  and
      Bai, Yu  and
      Gao, Yang",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.504/",
    doi = "10.18653/v1/2024.findings-emnlp.504",
    pages = "8623--8644",
    abstract = "Recent studies have demonstrated that In-Context Learning (ICL), through the use of specific demonstrations, can align Large Language Models (LLMs) with human preferences known as In-Context Alignment (ICA), indicating that models can comprehend human instructions without requiring parameter adjustments. However, the exploration of the mechanism and applicability of ICA remains limited. In this paper, we begin by dividing the context text used in ICA into three categories: format, system prompt, and example. Through ablation experiments, we investigate the effectiveness of each part in enabling ICA to function effectively. We then examine how variants in these parts impact the model`s alignment performance. Our findings indicate that the example part is crucial for enhancing the model`s alignment capabilities, with changes in examples significantly affecting alignment performance. We also conduct a comprehensive evaluation of ICA`s zero-shot capabilities in various alignment tasks. The results indicate that compared to parameter fine-tuning methods, ICA demonstrates superior performance in knowledge-based tasks and tool-use tasks. However, it still exhibits certain limitations in areas such as multi-turn dialogues and instruction following. Source codes and scripts are available at https://github.com/li-aolong/how-far-can-ica-go."
}
@article{Hu2021LoRALA,
  title={LoRA: Low-Rank Adaptation of Large Language Models},
  author={J. Edward Hu and Yelong Shen and Phillip Wallis and Zeyuan Allen-Zhu and Yuanzhi Li and Shean Wang and Weizhu Chen},
  journal={ArXiv},
  year={2021},
  volume={abs/2106.09685},
  url={https://api.semanticscholar.org/CorpusID:235458009}
}
@article{DBLP:journals/corr/cs-CL-0108005,
  author    = {Joshua Goodman},
  title     = {A Bit of Progress in Language Modeling},
  journal   = {CoRR},
  volume    = {cs.CL/0108005v1},
  year      = {2001},
  url       = {http://arxiv.org/abs/cs.CL/0108005v1},
  timestamp = {Wed, 07 Jun 2017 14:40:38 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/cs-CL-0108005},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{GOODMAN2001403,
title = "A bit of progress in language modeling",
journal = "Computer Speech \& Language",
volume = "15",
number = "4",
pages = "403-434",
year = "2001",
issn = "0885-2308",
doi = "10.1006/csla.2001.0174",
OPTurl = "http://www.sciencedirect.com/science/article/pii/S0885230801901743",
author = "Joshua T. Goodman"
}

@article{DBLP:journals/corr/cs-CL-9905001,
  author    = {Rebecca Hwa},
  title     = {Supervised Grammar Induction Using Training Data with Limited Constituent Information},
  journal   = {CoRR},
  volume    = {cs.CL/9905001},
  note = {Version 1},
  year      = {1999},
  url       = {http://arxiv.org/abs/cs.CL/9905001},
  timestamp = {Wed, 07 Jun 2017 14:41:01 +0200},
  biburl    = {https://dblp.org/rec/bib/journals/corr/cs-CL-9905001},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@InProceedings{P99-1010,
  author =  "Hwa, Rebecca",
  title =   "Supervised Grammar Induction using Training Data with Limited Constituent Information",
  booktitle =   "Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics",
  year =    "1999",
  url =     "http://www.aclweb.org/anthology/P99-1010"
}

@book{Jurafsky+Martin:2009a,
  author    = {Jurafsky, Daniel and Martin, James H.},
  title     = {Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  publisher = {Pearson Prentice Hall},
  year      = 2009,
  edition   = {Second}
}




@misc{WVS,
  title={World values survey.},
  author={Christian Haerpfer and Ronald Inglehart and Alejandro Moreno and Christian Welzel and Kseniya Kizilova and Jaime Diez-Medrano and Marta Lagos and Pippa Norris and Eduard Ponarin and Bjorn Puranen},
  url="https://www.worldvaluessurvey.org/wvs.jsp",
  year={2022}
}

@article{christiano2017deep,
  title={Deep reinforcement learning from human preferences},
  author={Christiano, Paul F and Leike, Jan and Brown, Tom and Martic, Miljan and Legg, Shane and Amodei, Dario},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{rafailov2024direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}
@article{li2025system,
  title={From system 1 to system 2: A survey of reasoning large language models},
  author={Li, Zhong-Zhi and Zhang, Duzhen and Zhang, Ming-Liang and Zhang, Jiaxin and Liu, Zengyan and Yao, Yuxuan and Xu, Haotian and Zheng, Junhao and Wang, Pei-Jie and Chen, Xiuyi and others},
  journal={arXiv preprint arXiv:2502.17419},
  year={2025}
}

@article{guan2024deliberative,
  title={Deliberative alignment: Reasoning enables safer language models},
  author={Guan, Melody Y and Joglekar, Manas and Wallace, Eric and Jain, Saachi and Barak, Boaz and Heylar, Alec and Dias, Rachel and Vallone, Andrea and Ren, Hongyu and Wei, Jason and others},
  journal={arXiv preprint arXiv:2412.16339},
  year={2024}
}

@article{scherrer2024evaluating,
  title={Evaluating the moral beliefs encoded in llms},
  author={Scherrer, Nino and Shi, Claudia and Feder, Amir and Blei, David},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  year={2024}
}
@inproceedings{cao-etal-2023-assessing,
    title = "Assessing Cross-Cultural Alignment between {C}hat{GPT} and Human Societies: An Empirical Study",
    author = "Cao, Yong  and
      Zhou, Li  and
      Lee, Seolhwa  and
      Cabello, Laura  and
      Chen, Min  and
      Hershcovich, Daniel",
    editor = "Dev, Sunipa  and
      Prabhakaran, Vinodkumar  and
      Adelani, David  and
      Hovy, Dirk  and
      Benotti, Luciana",
    booktitle = "Proceedings of the First Workshop on Cross-Cultural Considerations in NLP (C3NLP)",
    month = may,
    year = "2023",
    address = "Dubrovnik, Croatia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.c3nlp-1.7",
    doi = "10.18653/v1/2023.c3nlp-1.7",
    pages = "53--67",
}

@inproceedings{alkhamissi-etal-2024-investigating,
    title = "Investigating Cultural Alignment of Large Language Models",
    author = "AlKhamissi, Badr  and
      ElNokrashy, Muhammad  and
      Alkhamissi, Mai  and
      Diab, Mona",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.671",
    doi = "10.18653/v1/2024.acl-long.671",
    pages = "12404--12422",
}

@inproceedings{wang-etal-2024-countries,
    title = "Not All Countries Celebrate Thanksgiving: On the Cultural Dominance in Large Language Models",
    author = "Wang, Wenxuan  and
      Jiao, Wenxiang  and
      Huang, Jingyuan  and
      Dai, Ruyi  and
      Huang, Jen-tse  and
      Tu, Zhaopeng  and
      Lyu, Michael",
    editor = "Ku, Lun-Wei  and
      Martins, Andre  and
      Srikumar, Vivek",
    booktitle = "Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.acl-long.345",
    doi = "10.18653/v1/2024.acl-long.345",
    pages = "6349--6384",
}

@inproceedings{wang-cdeval,
    title = "{CDE}val: A Benchmark for Measuring the Cultural Dimensions of Large Language Models",
    author = "Wang, Yuhang  and
      Zhu, Yanxu  and
      Kong, Chao  and
      Wei, Shuyu  and
      Yi, Xiaoyuan  and
      Xie, Xing  and
      Sang, Jitao",
    editor = "Prabhakaran, Vinodkumar  and
      Dev, Sunipa  and
      Benotti, Luciana  and
      Hershcovich, Daniel  and
      Cabello, Laura  and
      Cao, Yong  and
      Adebara, Ife  and
      Zhou, Li",
    booktitle = "Proceedings of the 2nd Workshop on Cross-Cultural Considerations in NLP",
    month = aug,
    year = "2024",
    address = "Bangkok, Thailand",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.c3nlp-1.1",
    doi = "10.18653/v1/2024.c3nlp-1.1",
    pages = "1--16",
}

@inproceedings{
li2024culturellm,
title={Culture{LLM}: Incorporating Cultural Differences into Large Language Models},
author={CHENG LI and Mengzhuo Chen and Jindong Wang and Sunayana Sitaram and Xing Xie},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=sIsbOkQmBL}
}
@inproceedings{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{kumar2024training,
  title={Training language models to self-correct via reinforcement learning},
  author={Kumar, Aviral and Zhuang, Vincent and Agarwal, Rishabh and Su, Yi and Co-Reyes, John D and Singh, Avi and Baumli, Kate and Iqbal, Shariq and Bishop, Colton and Roelofs, Rebecca and others},
  journal={arXiv preprint arXiv:2409.12917},
  year={2024}
}
@article{yao2023tree,
  title={Tree of thoughts: Deliberate problem solving with large language models},
  author={Yao, Shunyu and Yu, Dian and Zhao, Jeffrey and Shafran, Izhak and Griffiths, Tom and Cao, Yuan and Narasimhan, Karthik},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={11809--11822},
  year={2023}
}
@article{sun2024retrieved,
  title={Retrieved in-context principles from previous mistakes},
  author={Sun, Hao and Jiang, Yong and Wang, Bo and Hou, Yingyan and Zhang, Yan and Xie, Pengjun and Huang, Fei},
  journal={arXiv preprint arXiv:2407.05682},
  year={2024}
}
@article{yang2024qwen2,
  title={Qwen2. 5 technical report},
  author={Yang, An and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Li, Chengyuan and Liu, Dayiheng and Huang, Fei and Wei, Haoran and others},
  journal={arXiv preprint arXiv:2412.15115},
  year={2024}
}
@misc{qwq-32b-preview,
    title = {QwQ: Reflect Deeply on the Boundaries of the Unknown},
    url = {https://qwenlm.github.io/blog/qwq-32b-preview/},
    author = {Qwen Team},
    month = {November},
    year = {2024}
}

@misc{gemini_flash_thinking,
  author = {{Google DeepMind}},
  title = {Gemini 2.0 Flash Thinking Experimental Model 01-21},
  howpublished = {\url{https://deepmind.google/technologies/gemini/flash-thinking/}},
  year={2025}
}
@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}
@article{liu2024deepseek,
  title={Deepseek-v3 technical report},
  author={Liu, Aixin and Feng, Bei and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and others},
  journal={arXiv preprint arXiv:2412.19437},
  year={2024}
}

@article{wang2023not,
  title={Not all countries celebrate thanksgiving: On the cultural dominance in large language models},
  author={Wang, Wenxuan and Jiao, Wenxiang and Huang, Jingyuan and Dai, Ruyi and Huang, Jen-tse and Tu, Zhaopeng and Lyu, Michael R},
  journal={arXiv preprint arXiv:2310.12481},
  year={2023}
}
@article{masoud2023cultural,
  title={Cultural Alignment in Large Language Models: An Explanatory Analysis Based on Hofstede's Cultural Dimensions},
  author={Masoud, Reem I and Liu, Ziquan and Ferianc, Martin and Treleaven, Philip and Rodrigues, Miguel},
  journal={arXiv preprint arXiv:2309.12342},
  year={2023}
}
@article{chiu2024culturalbench,
  title={Culturalbench: a robust, diverse and challenging benchmark on measuring the (lack of) cultural knowledge of llms},
  author={Chiu, Yu Ying and Jiang, Liwei and Lin, Bill Yuchen and Park, Chan Young and Li, Shuyue Stella and Ravi, Sahithya and Bhatia, Mehar and Antoniak, Maria and Tsvetkov, Yulia and Shwartz, Vered and others},
  journal={arXiv preprint arXiv:2410.02677},
  year={2024}
}

@article{naous2023having,
  title={Having beer after prayer? measuring cultural bias in large language models},
  author={Naous, Tarek and Ryan, Michael J and Ritter, Alan and Xu, Wei},
  journal={arXiv preprint arXiv:2305.14456},
  year={2023}
}
@article{yang2024qwen2a,
  title={Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement},
  author={Yang, An and Zhang, Beichen and Hui, Binyuan and Gao, Bofei and Yu, Bowen and Li, Chengpeng and Liu, Dayiheng and Tu, Jianhong and Zhou, Jingren and Lin, Junyang and others},
  journal={arXiv preprint arXiv:2409.12122},
  year={2024}
}

@article{kenton2024scalable,
  title={On scalable oversight with weak llms judging strong llms},
  author={Kenton, Zachary and Siegel, Noah and Kram{\'a}r, J{\'a}nos and Brown-Cohen, Jonah and Albanie, Samuel and Bulian, Jannis and Agarwal, Rishabh and Lindner, David and Tang, Yunhao and Goodman, Noah and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={75229--75276},
  year={2024}
}

@article{grattafiori2024llama,
  title={The llama 3 herd of models},
  author={Grattafiori, Aaron and Dubey, Abhimanyu and Jauhri, Abhinav and Pandey, Abhinav and Kadian, Abhishek and Al-Dahle, Ahmad and Letman, Aiesha and Mathur, Akhil and Schelten, Alan and Vaughan, Alex and others},
  journal={arXiv preprint arXiv:2407.21783},
  year={2024}
}

@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}
@article{bai2022constitutional,
  title={Constitutional ai: Harmlessness from ai feedback},
  author={Bai, Yuntao and Kadavath, Saurav and Kundu, Sandipan and Askell, Amanda and Kernion, Jackson and Jones, Andy and Chen, Anna and Goldie, Anna and Mirhoseini, Azalia and McKinnon, Cameron and others},
  journal={arXiv preprint arXiv:2212.08073},
  year={2022}
}
@article{sun2023principle,
  title={Principle-driven self-alignment of language models from scratch with minimal human supervision},
  author={Sun, Zhiqing and Shen, Yikang and Zhou, Qinhong and Zhang, Hongxin and Chen, Zhenfang and Cox, David and Yang, Yiming and Gan, Chuang},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={2511--2565},
  year={2023}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@article{zhang2024context,
  title={In-context principle learning from mistakes},
  author={Zhang, Tianjun and Madaan, Aman and Gao, Luyu and Zheng, Steven and Mishra, Swaroop and Yang, Yiming and Tandon, Niket and Alon, Uri},
  journal={arXiv preprint arXiv:2402.05403},
  year={2024}
}
@inproceedings{
li2024culturepark,
title={CulturePark: Boosting Cross-cultural Understanding in Large Language Models},
author={CHENG LI and Damien Teney and Linyi Yang and Qingsong Wen and Xing Xie and Jindong Wang},
booktitle={The Thirty-eighth Annual Conference on Neural Information Processing Systems},
year={2024},
url={https://openreview.net/forum?id=bIFHHf2RoD}
}

@article{xu2024self,
  title={Self-Pluralising Culture Alignment for Large Language Models},
  author={Xu, Shaoyang and Leng, Yongqi and Yu, Linhao and Xiong, Deyi},
  journal={arXiv preprint arXiv:2410.12971},
  year={2024}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}
@article{wang2022self,
  title={Self-instruct: Aligning language models with self-generated instructions},
  author={Wang, Yizhong and Kordi, Yeganeh and Mishra, Swaroop and Liu, Alisa and Smith, Noah A and Khashabi, Daniel and Hajishirzi, Hannaneh},
  journal={arXiv preprint arXiv:2212.10560},
  year={2022}
}

@book{ abelson-et-al:scheme,
  author = "Harold Abelson and Gerald~Jay Sussman and Julie Sussman",
  title = "Structure and Interpretation of Computer Programs",
  publisher = "MIT Press",
  address = "Cambridge, Massachusetts",
  year = "1985"
}

@inproceedings{ bgf:Lixto,
  author = "Robert Baumgartner and Georg Gottlob and Sergio Flesca",
  title = "Visual Information Extraction with {Lixto}",
  booktitle = "Proceedings of the 27th International Conference on Very Large Databases",
  pages = "119--128",
  publisher = "Morgan Kaufmann",
  address = "Rome, Italy",
  month = "September",
  year = "2001"
}

@article{ brachman-schmolze:kl-one,
  author = "Ronald~J. Brachman and James~G. Schmolze",
  title = "An overview of the {KL-ONE} knowledge representation system",
  journal = "Cognitive Science",
  volume = "9",
  number = "2",
  pages = "171--216",
  month = "April--June",
  year = "1985"
}

@article{ gottlob:nonmon,
  author = "Georg Gottlob",
  title = "Complexity results for nonmonotonic logics",
  journal = "Journal of Logic and Computation",
  volume = "2",
  number = "3",
  pages = "397--425",
  month = "June",
  year = "1992"
}

@article{ gls:hypertrees,
  author = "Georg Gottlob and Nicola Leone and Francesco Scarcello",
  title = "Hypertree Decompositions and Tractable Queries",
  journal = "Journal of Computer and System Sciences",
  volume = "64",
  number = "3",
  pages = "579--627",
  month = "May",
  year = "2002"
}
@inproceedings{Rao2024NormAdAF,
  title={NormAd: A Framework for Measuring the Cultural Adaptability of Large Language Models},
  author={Abhinav Rao and Akhila Yerukola and Vishwa Shah and Katharina Reinecke and Maarten Sap},
  booktitle={North American Chapter of the Association for Computational Linguistics},
  year={2024},
  url={https://api.semanticscholar.org/CorpusID:269282746}
}
@article{ levesque:functional-foundations,
  author = "Hector~J. Levesque",
  title = "Foundations of a functional approach to knowledge representation",
  journal = "Artificial Intelligence",
  volume = "23",
  number = "2",
  pages = "155--212",
  month = "July",
  year = "1984"
}

@inproceedings{ levesque:belief,
  author = "Hector~J. Levesque",
  title = "A logic of implicit and explicit belief",
  booktitle = "Proceedings of the Fourth National Conference on Artificial Intelligence",
  publisher = "American Association for Artificial Intelligence",
  pages = "198--202",
  address = "Austin, Texas",
  month = "August",
  year = "1984"
}

@article{ nebel:jair-2000,
  author = "Bernhard Nebel",
  title = "On the compilability and expressive power of propositional planning formalisms",
  journal = "Journal of Artificial Intelligence Research",
  volume = "12",
  pages = "271--315",
  year = "2000"
}

 @misc{proceedings,
  author = {{IJCAI Proceedings}},
  title = {{IJCAI} Camera Ready Submission},
  howpublished = {\url{https://proceedings.ijcai.org/info}},
}
