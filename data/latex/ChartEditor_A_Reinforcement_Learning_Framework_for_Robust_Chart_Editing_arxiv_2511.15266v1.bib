% Citations of comparing models

@inproceedings{yan2024chartreformer,
  title={Chartreformer: Natural language-driven chart image editing},
  author={Yan, Pengyu and Bhosale, Mahesh and Lal, Jay and Adhikari, Bikhyat and Doermann, David},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={453--469},
  year={2024},
  organization={Springer}
}
@inproceedings{chen2025chart,
  title={Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts},
  author={Chen, Xiangnan and Fang, Yuancheng and Li, Juncheng and Xiao, Qian and Lin, Jun and Tang, Siliang and Zhuang, Yueting},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={13297--13303},
  year={2025}
}
@article{chartcoder,
  title={Chartcoder: Advancing multimodal large language model for chart-to-code generation},
  author={Zhao, Xuanle and Luo, Xianzhen and Shi, Qi and Chen, Chi and Wang, Shuo and Liu, Zhiyuan and Sun, Maosong},
  journal={arXiv preprint arXiv:2501.06598},
  year={2025}
}
@article{geminiprovision,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}
@inproceedings{ChartCraft,
  title={Chartreformer: Natural language-driven chart image editing},
  author={Yan, Pengyu and Bhosale, Mahesh and Lal, Jay and Adhikari, Bikhyat and Doermann, David},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={453--469},
  year={2024},
  organization={Springer}
}
@article{Claude3opus,
  title={Introducing the next generation of claude},
  year={2024},
  author={Anthropic, AI},
  journal={https://www. anthropic. com/news/claude-3-family},
}

@article{gpt4o,
  title={Gpt-4o system card},
  author={Hurst, Aaron and Lerer, Adam and Goucher, Adam P and Perelman, Adam and Ramesh, Aditya and Clark, Aidan and Ostrow, AJ and Welihinda, Akila and Hayes, Alan and Radford, Alec and others},
  journal={arXiv preprint arXiv:2410.21276},
  year={2024}
}
@inproceedings{internvl2series,
  title={Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks},
  author={Chen, Zhe and Wu, Jiannan and Wang, Wenhai and Su, Weijie and Chen, Guo and Xing, Sen and Zhong, Muyan and Zhang, Qinglong and Zhu, Xizhou and Lu, Lewei and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={24185--24198},
  year={2024}
}
@article{qwenvl2series,
  title={Qwen2-vl: Enhancing vision-language model's perception of the world at any resolution},
  author={Wang, Peng and Bai, Shuai and Tan, Sinan and Wang, Shijie and Fan, Zhihao and Bai, Jinze and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and others},
  journal={arXiv preprint arXiv:2409.12191},
  year={2024}
}
@misc{Qwen3-VL,
  title        = {Qwen3-VL},
  year         = {2025},
  url          = {https://github.com/QwenLM/Qwen3-VL},
  author={QwenLM},
}

@article{phi4,
  title={Phi-4 technical report},
  author={Abdin, Marah and Aneja, Jyoti and Behl, Harkirat and Bubeck, S{\'e}bastien and Eldan, Ronen and Gunasekar, Suriya and Harrison, Michael and Hewett, Russell J and Javaheripi, Mojan and Kauffmann, Piero and others},
  journal={arXiv preprint arXiv:2412.08905},
  year={2024}
}
@misc{phi3,
      title={Phi-3 Technical Report: A Highly Capable Language Model Locally on Your Phone}, 
      author={Marah Abdin and Jyoti Aneja and Hany Awadalla and Ahmed Awadallah and Ammar Ahmad Awan and Nguyen Bach and Amit Bahree and Arash Bakhtiari and Jianmin Bao and Harkirat Behl and Alon Benhaim and Misha Bilenko and Johan Bjorck and SÃ©bastien Bubeck and others},
      year={2024},
      eprint={2404.14219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2404.14219}, 
}
@article{deepseekvl,
  title={Deepseek-vl: towards real-world vision-language understanding},
  author={Lu, Haoyu and Liu, Wen and Zhang, Bo and Wang, Bingxuan and Dong, Kai and Liu, Bo and Sun, Jingxiang and Ren, Tongzheng and Li, Zhuoshu and Yang, Hao and others},
  journal={arXiv preprint arXiv:2403.05525},
  year={2024}
}
@misc{llavaNext,
    title={LLaVA-NeXT: Stronger LLMs Supercharge Multimodal Capabilities in the Wild},
    url={https://llava-vl.github.io/blog/2024-05-10-llava-next-stronger-llms/},
    author={Li, Bo and Zhang, Kaichen and Zhang, Hao and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Yuanhan and Liu, Ziwei and Li, Chunyuan},
    month={May},
    year={2024}
}
@article{IDEFICS2,
  title={What matters when building vision-language models?},
  author={Lauren{\c{c}}on, Hugo and Tronchon, L{\'e}o and Cord, Matthieu and Sanh, Victor},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={87874--87907},
  year={2024}
}
@inproceedings{MiniCPM,
  title={Llava-uhd: an lmm perceiving any aspect ratio and high-resolution images},
  author={Guo, Zonghao and Xu, Ruyi and Yao, Yuan and Cui, Junbo and Ni, Zanlin and Ge, Chunjiang and Chua, Tat-Seng and Liu, Zhiyuan and Huang, Gao},
  booktitle={European Conference on Computer Vision},
  pages={390--406},
  year={2024},
  organization={Springer}
}
@article{cogvlm2,
  title={Cogvlm: Visual expert for pretrained language models},
  author={Wang, Weihan and Lv, Qingsong and Yu, Wenmeng and Hong, Wenyi and Qi, Ji and Wang, Yan and Ji, Junhui and Yang, Zhuoyi and Zhao, Lei and XiXuan, Song and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={121475--121499},
  year={2024}
}
@article{chartllama,
  title={Chartllama: A multimodal llm for chart understanding and generation},
  author={Han, Yucheng and Zhang, Chi and Chen, Xin and Yang, Xu and Wang, Zhibin and Yu, Gang and Fu, Bin and Zhang, Hanwang},
  journal={arXiv preprint arXiv:2311.16483},
  year={2023}
}

@article{tinychart,
  title={Tinychart: Efficient chart understanding with visual token merging and program-of-thoughts learning},
  author={Zhang, Liang and Hu, Anwen and Xu, Haiyang and Yan, Ming and Xu, Yichen and Jin, Qin and Zhang, Ji and Huang, Fei},
  journal={arXiv preprint arXiv:2404.16635},
  year={2024}
}
@article{chartvlm_and_chartx,
  title={Chartx \& chartvlm: A versatile benchmark and foundation model for complicated chart reasoning},
  author={Xia, Renqiu and Zhang, Bo and Ye, Hancheng and Yan, Xiangchao and Liu, Qi and Zhou, Hongbin and Chen, Zijun and Ye, Peng and Dou, Min and Shi, Botian and others},
  journal={arXiv preprint arXiv:2402.12185},
  year={2024}
}
@article{chartmimic,
  title={Chartmimic: Evaluating lmm's cross-modal reasoning capability via chart-to-code generation},
  author={Yang, Cheng and Shi, Chufan and Liu, Yaxin and Shui, Bo and Wang, Junjie and Jing, Mohan and Xu, Linran and Zhu, Xinyu and Li, Siheng and Zhang, Yuxiang and others},
  journal={arXiv preprint arXiv:2406.09961},
  year={2024}
}


@inproceedings{AcademiaChart,
    title = "Is {GPT}-4{V} (ision) All You Need for Automating Academic Data Visualization? Exploring Vision-Language Models' Capability in Reproducing Academic Charts",
    author = "Zhang, Zhehao  and
      Ma, Weicheng  and
      Vosoughi, Soroush",
    editor = "Al-Onaizan, Yaser  and
      Bansal, Mohit  and
      Chen, Yun-Nung",
    booktitle = "Findings of the Association for Computational Linguistics: EMNLP 2024",
    month = nov,
    year = "2024",
    address = "Miami, Florida, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.findings-emnlp.485/",
    doi = "10.18653/v1/2024.findings-emnlp.485",
    pages = "8271--8288",
    abstract = "While effective data visualization is crucial to present complex information in academic research, its creation demands significant expertise in both data management and graphic design. We explore the potential of using Vision-Language Models (VLMs) in automating the creation of data visualizations by generating code templates from existing charts. As the first work to systematically investigate this task, we first introduce AcademiaChart, a dataset comprising 2525 high-resolution data visualization figures with captions from a variety of AI conferences, extracted directly from source codes. We then conduct large-scale experiments with six state-of-the-art (SOTA) VLMs, including both closed-source and open-source models. Our findings reveal that SOTA closed-source VLMs can indeed be helpful in reproducing charts. On the contrary, open-source ones are only effective at reproducing much simpler charts but struggle with more complex ones. Interestingly, the application of Chain-of-Thought (CoT) prompting significantly enhances the performance of the most advanced model, GPT-4-V, while it does not work as well for other models. These results underscore the potential of VLMs in data visualization while also highlighting critical areas that need improvement for broader application."
}
@article{chartmoe,
  title={ChartMoE: Mixture of diversely aligned expert connector for chart understanding},
  author={Xu, Zhengzhuo and Qu, Bowen and Qi, Yiyan and Du, Sinan and Xu, Chengjin and Yuan, Chun and Guo, Jian},
  journal={arXiv preprint arXiv:2409.03277},
  year={2024}
}

@article{chartqa,
  title={Chartqa: A benchmark for question answering about charts with visual and logical reasoning},
  author={Masry, Ahmed and Long, Do Xuan and Tan, Jia Qing and Joty, Shafiq and Hoque, Enamul},
  journal={arXiv preprint arXiv:2203.10244},
  year={2022}
}

@article{qwen2_5series,
  title={Qwen2. 5-vl technical report},
  author={Bai, Shuai and Chen, Keqin and Liu, Xuejing and Wang, Jialin and Ge, Wenbin and Song, Sibo and Dang, Kai and Wang, Peng and Wang, Shijie and Tang, Jun and others},
  journal={arXiv preprint arXiv:2502.13923},
  year={2025}
}

@article{chartedit,
  title={ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing},
  author={Zhao, Xuanle and Liu, Xuexin and Yang, Haoyue and Luo, Xianzhen and Zeng, Fanhu and Li, Jianling and Shi, Qi and Chen, Chi},
  journal={arXiv preprint arXiv:2505.11935},
  year={2025}
}
@article{chart_survey,
  title={From pixels to insights: A survey on automatic chart understanding in the era of large foundation models},
  author={Huang, Kung-Hsiang and Chan, Hou Pong and Fung, Yi R and Qiu, Haoyi and Zhou, Mingyang and Joty, Shafiq and Chang, Shih-Fu and Ji, Heng},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  year={2024},
  publisher={IEEE}
}

@article{Xcomposer2,
  title={Internlm-xcomposer2: Mastering free-form text-image composition and comprehension in vision-language large model},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Wei, Xilin and Zhang, Songyang and Duan, Haodong and Cao, Maosong and others},
  journal={arXiv preprint arXiv:2401.16420},
  year={2024}
}
@article{Xcomposer4k,
  title={Internlm-xcomposer2-4khd: A pioneering large vision-language model handling resolutions from 336 pixels to 4k hd},
  author={Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Cao, Yuhang and Wang, Bin and Ouyang, Linke and Zhang, Songyang and Duan, Haodong and Zhang, Wenwei and Li, Yining and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={42566--42592},
  year={2024}
}
@article{sphinx,
  title={Sphinx: The joint mixing of weights, tasks, and visual embeddings for multi-modal large language models},
  author={Lin, Ziyi and Liu, Chris and Zhang, Renrui and Gao, Peng and Qiu, Longtian and Xiao, Han and Qiu, Han and Lin, Chen and Shao, Wenqi and Chen, Keqin and others},
  journal={arXiv preprint arXiv:2311.07575},
  year={2023}
}
@inproceedings{Improved_baselines,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={26296--26306},
  year={2024}
}
@article{visual_instruction_tuning,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}
@article{gpt4,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}

@article{mplugowl,
  title={mplug-owl: Modularization empowers large language models with multimodality},
  author={Ye, Qinghao and Xu, Haiyang and Xu, Guohai and Ye, Jiabo and Yan, Ming and Zhou, Yiyang and Wang, Junyang and Hu, Anwen and Shi, Pengcheng and Shi, Yaya and others},
  journal={arXiv preprint arXiv:2304.14178},
  year={2023}
}
@inproceedings{mplug_oct,
  title={mPLUG-Octopus: The Versatile Assistant Empowered by A Modularized End-to-End Multimodal LLM},
  author={Ye, Qinghao and Xu, Haiyang and Yan, Ming and Zhao, Chenlin and Wang, Junyang and Yang, Xiaoshan and Zhang, Ji and Huang, Fei and Sang, Jitao and Xu, Changsheng},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={9365--9367},
  year={2023}
}
@inproceedings{mplugowl2,
  title={mplug-owl2: Revolutionizing multi-modal large language model with modality collaboration},
  author={Ye, Qinghao and Xu, Haiyang and Ye, Jiabo and Yan, Ming and Hu, Anwen and Liu, Haowei and Qian, Qi and Zhang, Ji and Huang, Fei},
  booktitle={Proceedings of the ieee/cvf conference on computer vision and pattern recognition},
  pages={13040--13051},
  year={2024}
}
@article{xcomposer,
  title={Internlm-xcomposer: A vision-language large model for advanced text-image comprehension and composition},
  author={Zhang, Pan and Dong, Xiaoyi and Wang, Bin and Cao, Yuhang and Xu, Chao and Ouyang, Linke and Zhao, Zhiyuan and Duan, Haodong and Zhang, Songyang and Ding, Shuangrui and others},
  journal={arXiv preprint arXiv:2309.15112},
  year={2023}
}
@article{grpo,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@misc{gpt4_1,
  author = {OpenAI},
  title = {GPT-4.1 Announcement},
  howpublished = {\url{https://openai.com/index/gpt-4-1/}},
  year = {2025},
  note = {Accessed: 2025-08-03}
}
@misc{claude37,
  title     = {Claude 3.7 Sonnet},
  author    = {{Anthropic}},
  year      = {2025},
  howpublished = {\url{https://www.anthropic.com/news/claude-3-7-sonnet}},

}

@article{grpo_also,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={URL https://arxiv. org/abs/2402.03300},
  volume={2},
  number={3},
  pages={5},
  year={2024}
}
@article{gemini2_5,
  title={Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities},
  author={Comanici, Gheorghe and Bieber, Eric and Schaekermann, Mike and Pasupat, Ice and Sachdeva, Noveen and Dhillon, Inderjit and Blistein, Marcel and Ram, Ori and Zhang, Dan and Rosen, Evan and others},
  journal={arXiv preprint arXiv:2507.06261},
  year={2025}
}

@article{internvl2_5,
  title={Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling},
  author={Chen, Zhe and Wang, Weiyun and Cao, Yue and Liu, Yangzhou and Gao, Zhangwei and Cui, Erfei and Zhu, Jinguo and Ye, Shenglong and Tian, Hao and Liu, Zhaoyang and others},
  journal={arXiv preprint arXiv:2412.05271},
  year={2024}
}

@article{llava,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}
@misc{dapo,
      title={DAPO: An Open-Source LLM Reinforcement Learning System at Scale}, 
      author={Qiying Yu and Zheng Zhang and Ruofei Zhu and Yufeng Yuan and Xiaochen Zuo and Yu Yue and Weinan Dai and Tiantian Fan and Gaohong Liu and Lingjun Liu and Xin Liu and Haibin Lin and Zhiqi Lin and Bole Ma and Guangming Sheng and Yuxuan Tong and Chi Zhang and Mofan Zhang and Wang Zhang and Hang Zhu and Jinhua Zhu and Jiaze Chen and Jiangjie Chen and Chengyi Wang and Hongli Yu and Yuxuan Song and Xiangpeng Wei and Hao Zhou and Jingjing Liu and Wei-Ying Ma and Ya-Qin Zhang and Lin Yan and Mu Qiao and Yonghui Wu and Mingxuan Wang},
      year={2025},
      eprint={2503.14476},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2503.14476}, 
}

@misc{code-rl,
      title={Improving LLM-Generated Code Quality with GRPO}, 
      author={Maxime Robeyns and Laurence Aitchison},
      year={2025},
      eprint={2506.02211},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2506.02211}, 
}