@article{li2025challenges,
  title={Challenges and Trends in Egocentric Vision: A Survey},
  author={Li, Xiang and Qiu, Heqian and Wang, Lanxiao and Zhang, Hanwen and Qi, Chenghao and Han, Linfeng and Xiong, Huiyu and Li, Hongliang},
  journal={arXiv preprint arXiv:2503.15275},
  year={2025}
}

@article{plizzari2024outlook,
  title={An outlook into the future of egocentric vision},
  author={Plizzari, Chiara and Goletto, Gabriele and Furnari, Antonino and Bansal, Siddhant and Ragusa, Francesco and Farinella, Giovanni Maria and Damen, Dima and Tommasi, Tatiana},
  journal={International Journal of Computer Vision},
  volume={132},
  number={11},
  pages={4880--4936},
  year={2024},
  publisher={Springer}
}

@article{thatipelli2025egocentric,
  title={Egocentric and exocentric methods: A short survey},
  author={Thatipelli, Anirudh and Lo, Shao-Yuan and Roy-Chowdhury, Amit K},
  journal={Computer Vision and Image Understanding},
  pages={104371},
  year={2025},
  publisher={Elsevier}
}

@article{zhang2019comprehensive,
  title={A comprehensive survey of vision-based human action recognition methods},
  author={Zhang, Hong-Bo and Zhang, Yi-Xiang and Zhong, Bineng and Lei, Qing and Yang, Lijie and Du, Ji-Xiang and Chen, Duan-Sheng},
  journal={Sensors},
  volume={19},
  number={5},
  pages={1005},
  year={2019},
  publisher={MDPI}
}

@article{kong2022human,
  title={Human action recognition and prediction: A survey},
  author={Kong, Yu and Fu, Yun},
  journal={International Journal of Computer Vision},
  volume={130},
  number={5},
  pages={1366--1401},
  year={2022},
  publisher={Springer}
}

@inproceedings{trong2017comprehensive,
  title={A comprehensive survey on human activity prediction},
  author={Trong, Nghia Pham and Nguyen, Hung and Kazunori, Kotani and Le Hoai, Bac},
  booktitle={International Conference on Computational Science and Its Applications},
  pages={411--425},
  year={2017},
  organization={Springer}
}

@article{rasouli2020deep,
  title={Deep learning for vision-based prediction: A survey},
  author={Rasouli, Amir},
  journal={arXiv preprint arXiv:2007.00095},
  year={2020}
}

@article{ding2023temporal,
  title={Temporal action segmentation: An analysis of modern techniques},
  author={Ding, Guodong and Sener, Fadime and Yao, Angela},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={46},
  number={2},
  pages={1011--1030},
  year={2023},
  publisher={IEEE}
}



@article{xiao2025videoqa,
  title={Videoqa in the era of llms: An empirical study},
  author={Xiao, Junbin and Huang, Nanxin and Qin, Hangyu and Li, Dongyang and Li, Yicong and Zhu, Fengbin and Tao, Zhulin and Yu, Jianxing and Lin, Liang and Chua, Tat-Seng and others},
  journal={International Journal of Computer Vision},
  pages={1--24},
  year={2025},
  publisher={Springer}
}

@inproceedings{grauman2022ego4d,
  title={Ego4d: Around the world in 3,000 hours of egocentric video},
  author={Grauman, Kristen and Westbury, Andrew and Byrne, Eugene and Chavis, Zachary and Furnari, Antonino and Girdhar, Rohit and Hamburger, Jackson and Jiang, Hao and Liu, Miao and Liu, Xingyu and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={18995--19012},
  year={2022}
}

@article{huang2024vinci,
  title={Vinci: A real-time embodied smart assistant based on egocentric vision-language model},
  author={Huang, Yifei and Xu, Jilan and Pei, Baoqi and He, Yuping and Chen, Guo and Yang, Lijin and Chen, Xinyuan and Wang, Yaohui and Nie, Zheng and Liu, Jinyao and others},
  journal={arXiv preprint arXiv:2412.21080},
  year={2024}
}

@article{pei2024egovideo,
  title={Egovideo: Exploring egocentric foundation model and downstream adaptation},
  author={Pei, Baoqi and Chen, Guo and Xu, Jilan and He, Yuping and Liu, Yicheng and Pan, Kanghua and Huang, Yifei and Wang, Yali and Lu, Tong and Wang, Limin and others},
  journal={arXiv preprint arXiv:2406.18070},
  year={2024}
}

@article{damen2020epic,
  title={The epic-kitchens dataset: Collection, challenges and baselines},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={43},
  number={11},
  pages={4125--4141},
  year={2020},
  publisher={IEEE}
}

@article{duan2022survey,
  title={A survey of embodied ai: From simulators to research tasks},
  author={Duan, Jiafei and Yu, Samson and Tan, Hui Li and Zhu, Hongyuan and Tan, Cheston},
  journal={IEEE Transactions on Emerging Topics in Computational Intelligence},
  volume={6},
  number={2},
  pages={230--244},
  year={2022},
  publisher={IEEE}
}

@inproceedings{das2018embodied,
  title={Embodied question answering},
  author={Das, Abhishek and Datta, Samyak and Gkioxari, Georgia and Lee, Stefan and Parikh, Devi and Batra, Dhruv},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1--10},
  year={2018}
}

@article{fan2024embodied,
  title={Embodied videoagent: Persistent memory from egocentric videos and embodied sensors enables dynamic scene understanding},
  author={Fan, Yue and Ma, Xiaojian and Su, Rongpeng and Guo, Jun and Wu, Rujie and Chen, Xi and Li, Qing},
  journal={arXiv preprint arXiv:2501.00358},
  year={2024}
}

@inproceedings{yu2019multi,
  title={Multi-target embodied question answering},
  author={Yu, Licheng and Chen, Xinlei and Gkioxari, Georgia and Bansal, Mohit and Berg, Tamara L and Batra, Dhruv},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6309--6318},
  year={2019}
}

@article{weihs2020allenact,
  title={Allenact: A framework for embodied ai research},
  author={Weihs, Luca and Salvador, Jordi and Kotar, Klemen and Jain, Unnat and Zeng, Kuo-Hao and Mottaghi, Roozbeh and Kembhavi, Aniruddha},
  journal={arXiv preprint arXiv:2008.12760},
  year={2020}
}

@article{bansal2024hoi,
  title={Hoi-ref: Hand-object interaction referral in egocentric vision},
  author={Bansal, Siddhant and Wray, Michael and Damen, Dima},
  journal={arXiv preprint arXiv:2404.09933},
  year={2024}
}

@inproceedings{liu2022joint,
  title={Joint hand motion and interaction hotspots prediction from egocentric videos},
  author={Liu, Shaowei and Tripathi, Subarna and Majumdar, Somdeb and Wang, Xiaolong},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3282--3292},
  year={2022}
}

@article{finocchiaro2025real,
  title={A Real-Time System for Egocentric Hand-Object Interaction Detection in Industrial Domains},
  author={Finocchiaro, Antonio and Catinello, Alessandro Sebastiano and Mazzamuto, Michele and Leonardi, Rosario and Furnari, Antonino and Farinella, Giovanni Maria},
  journal={arXiv preprint arXiv:2507.13326},
  year={2025}
}

@article{bandini2020analysis,
  title={Analysis of the hands in egocentric vision: A survey},
  author={Bandini, Andrea and Zariffa, Jos{\'e}},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={6},
  pages={6846--6866},
  year={2020},
  publisher={IEEE}
}

@inproceedings{prakash2025synthesizing,
  title={How do i do that? synthesizing 3d hand motion and contacts for everyday interactions},
  author={Prakash, Aditya and Lundell, Benjamin and Andreychuk, Dmitry and Forsyth, David and Gupta, Saurabh and Sawhney, Harpreet},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={7026--7036},
  year={2025}
}

@article{doosti2019hand,
  title={Hand pose estimation: A survey},
  author={Doosti, Bardia},
  journal={arXiv preprint arXiv:1903.01013},
  year={2019}
}

@article{oberweger2015hands,
  title={Hands deep in deep learning for hand pose estimation},
  author={Oberweger, Markus and Wohlhart, Paul and Lepetit, Vincent},
  journal={arXiv preprint arXiv:1502.06807},
  year={2015}
}

@inproceedings{mazzamuto2025gazing,
  title={Gazing into missteps: Leveraging eye-gaze for unsupervised mistake detection in egocentric videos of skilled human activities},
  author={Mazzamuto, Michele and Furnari, Antonino and Sato, Yoichi and Farinella, Giovanni Maria},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={8310--8320},
  year={2025}
}

@article{qian2024streaming,
  title={Streaming long video understanding with large language models},
  author={Qian, Rui and Dong, Xiaoyi and Zhang, Pan and Zang, Yuhang and Ding, Shuangrui and Lin, Dahua and Wang, Jiaqi},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={119336--119360},
  year={2024}
}

@inproceedings{rav2006making,
  title={Making a long video short: Dynamic video synopsis},
  author={Rav-Acha, Alex and Pritch, Yael and Peleg, Shmuel},
  booktitle={2006 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR'06)},
  volume={1},
  pages={435--441},
  year={2006},
  organization={IEEE}
}

@article{thakur2024anticipating,
  title={Anticipating next active objects for egocentric videos},
  author={Thakur, Sanket Kumar and Beyan, Cigdem and Morerio, Pietro and Murino, Vittorio and Del Bue, Alessio},
  journal={IEEE Access},
  volume={12},
  pages={61767--61779},
  year={2024},
  publisher={IEEE}
}

@inproceedings{ragusa2021meccano,
  title={The meccano dataset: Understanding human-object interactions from egocentric videos in an industrial-like domain},
  author={Ragusa, Francesco and Furnari, Antonino and Livatino, Salvatore and Farinella, Giovanni Maria},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1569--1578},
  year={2021}
}

@inproceedings{xu2023egopca,
  title={Egopca: A new framework for egocentric hand-object interaction understanding},
  author={Xu, Yue and Li, Yong-Lu and Huang, Zhemin and Liu, Michael Xu and Lu, Cewu and Tai, Yu-Wing and Tang, Chi-Keung},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5273--5284},
  year={2023}
}

@inproceedings{fan2019egovqa,
  title={Egovqa-an egocentric video question answering benchmark dataset},
  author={Fan, Chenyou},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@article{antoun2023human,
  title={Human object interaction detection: Design and survey},
  author={Antoun, Maya and Asmar, Daniel},
  journal={Image and Vision Computing},
  volume={130},
  pages={104617},
  year={2023},
  publisher={Elsevier}
}

@inproceedings{flaborea2024prego,
  title={PREGO: online mistake detection in PRocedural EGOcentric videos},
  author={Flaborea, Alessandro and Di Melendugno, Guido Maria D'Amely and Plini, Leonardo and Scofano, Luca and De Matteis, Edoardo and Furnari, Antonino and Farinella, Giovanni Maria and Galasso, Fabio},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18483--18492},
  year={2024}
}

@article{plini2024ti,
  title={TI-PREGO: Chain of Thought and In-Context Learning for Online Mistake Detection in PRocedural EGOcentric Videos},
  author={Plini, Leonardo and Scofano, Luca and De Matteis, Edoardo and di Melendugno, Guido Maria D'Amely and Flaborea, Alessandro and Sanchietti, Andrea and Farinella, Giovanni Maria and Galasso, Fabio and Furnari, Antonino},
  journal={arXiv preprint arXiv:2411.02570},
  year={2024}
}

@article{zhang2022automatic,
  title={Automatic chain of thought prompting in large language models},
  author={Zhang, Zhuosheng and Zhang, Aston and Li, Mu and Smola, Alex},
  journal={arXiv preprint arXiv:2210.03493},
  year={2022}
}

@inproceedings{huang2025modeling,
  title={Modeling Multiple Normal Action Representations for Error Detection in Procedural Tasks},
  author={Huang, Wei-Jin and Li, Yuan-Ming and Xia, Zhi-Wei and Tang, Yu-Ming and Lin, Kun-Yu and Hu, Jian-Fang and Zheng, Wei-Shi},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={27794--27804},
  year={2025}
}

@inproceedings{an2023miniroad,
  title={Miniroad: Minimal rnn framework for online action detection},
  author={An, Joungbin and Kang, Hyolim and Han, Su Ho and Yang, Ming-Hsuan and Kim, Seon Joo},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={10341--10350},
  year={2023}
}

@article{koubaa2023gpt,
  title={GPT-4 vs. GPT-3.5: A concise showdown},
  author={Koubaa, Anis},
  year={2023},
  publisher={Preprints}
}

@article{ye2023comprehensive,
  title={A comprehensive capability analysis of gpt-3 and gpt-3.5 series models},
  author={Ye, Junjie and Chen, Xuanting and Xu, Nuo and Zu, Can and Shao, Zekai and Liu, Shichun and Cui, Yuhan and Zhou, Zeyang and Gong, Chao and Shen, Yang and others},
  journal={arXiv preprint arXiv:2303.10420},
  year={2023}
}

@article{lai2022eye,
  title={In the eye of transformer: Global-local correlation for egocentric gaze estimation},
  author={Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M},
  journal={arXiv preprint arXiv:2208.04464},
  year={2022}
}

@inproceedings{peng2023i3d,
  title={I3D: Transformer architectures with input-dependent dynamic depth for speech recognition},
  author={Peng, Yifan and Lee, Jaesong and Watanabe, Shinji},
  booktitle={ICASSP 2023-2023 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{bellman1966dynamic,
  title={Dynamic programming},
  author={Bellman, Richard},
  journal={science},
  volume={153},
  number={3731},
  pages={34--37},
  year={1966},
  publisher={American Association for the Advancement of Science}
}

@inproceedings{bergroth2000survey,
  title={A survey of longest common subsequence algorithms},
  author={Bergroth, Lasse and Hakonen, Harri and Raita, Timo},
  booktitle={Proceedings Seventh International Symposium on String Processing and Information Retrieval. SPIRE 2000},
  pages={39--48},
  year={2000},
  organization={IEEE}
}

@inproceedings{paterson1994longest,
  title={Longest common subsequences},
  author={Paterson, Mike and Dan{\v{c}}{\'\i}k, Vlado},
  booktitle={International symposium on mathematical foundations of computer science},
  pages={127--142},
  year={1994},
  organization={Springer}
}

@article{ding2023every,
  title={Every mistake counts in assembly},
  author={Ding, Guodong and Sener, Fadime and Ma, Shugao and Yao, Angela},
  journal={arXiv preprint arXiv:2307.16453},
  year={2023}
}

@inproceedings{damen2018scaling,
  title={Scaling egocentric vision: The epic-kitchens dataset},
  author={Damen, Dima and Doughty, Hazel and Farinella, Giovanni Maria and Fidler, Sanja and Furnari, Antonino and Kazakos, Evangelos and Moltisanti, Davide and Munro, Jonathan and Perrett, Toby and Price, Will and others},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={720--736},
  year={2018}
}

@article{storks2024transparent,
  title={Transparent and Coherent Procedural Mistake Detection},
  author={Storks, Shane and Bar-Yossef, Itamar and Li, Yayuan and Zhang, Zheyuan and Corso, Jason J and Chai, Joyce},
  journal={arXiv preprint arXiv:2412.11927},
  year={2024}
}

@article{haneji2024egooops,
  title={Egooops: A dataset for mistake action detection from egocentric videos referring to procedural texts},
  author={Haneji, Yuto and Nishimura, Taichi and Kameko, Hirotaka and Shirai, Keisuke and Yoshida, Tomoya and Kajimura, Keiya and Yamamoto, Koki and Cui, Taiyu and Nishimoto, Tomohiro and Mori, Shinsuke},
  journal={arXiv preprint arXiv:2410.05343},
  year={2024}
}

@article{patsch2025technical,
  title={Technical Report for Egocentric Mistake Detection for the HoloAssist Challenge},
  author={Patsch, Constantin and Zakour, Marsil and Wu, Yuankai and Steinbach, Eckehard},
  journal={arXiv preprint arXiv:2506.06174},
  year={2025}
}

@inproceedings{lee2024error,
  title={Error detection in egocentric procedural task videos},
  author={Lee, Shih-Po and Lu, Zijia and Zhang, Zekun and Hoai, Minh and Elhamifar, Ehsan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18655--18666},
  year={2024}
}

@article{manuvinakurike2022human,
  title={Human in the loop approaches in multi-modal conversational task guidance system development},
  author={Manuvinakurike, Ramesh and Biswas, Sovan and Raffa, Giuseppe and Beckwith, Richard and Rhodes, Anthony and Shi, Meng and Mejia, Gesem Gudino and Sahay, Saurav and Nachman, Lama},
  journal={arXiv preprint arXiv:2211.01824},
  year={2022}
}

@article{lewis2019bart,
  title={BART: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{williams2017broad,
  title={A broad-coverage challenge corpus for sentence understanding through inference},
  author={Williams, Adina and Nangia, Nikita and Bowman, Samuel R},
  journal={arXiv preprint arXiv:1704.05426},
  year={2017}
}

@inproceedings{pramanick2023egovlpv2,
  title={Egovlpv2: Egocentric video-language pre-training with fusion in the backbone},
  author={Pramanick, Shraman and Song, Yale and Nag, Sayan and Lin, Kevin Qinghong and Shah, Hardik and Shou, Mike Zheng and Chellappa, Rama and Zhang, Pengchuan},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={5285--5297},
  year={2023}
}

@inproceedings{cui2019class,
  title={Class-balanced loss based on effective number of samples},
  author={Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9268--9277},
  year={2019}
}

@article{luo2020univl,
  title={Univl: A unified video and language pre-training model for multimodal understanding and generation},
  author={Luo, Huaishao and Ji, Lei and Shi, Botian and Huang, Haoyang and Duan, Nan and Li, Tianrui and Li, Jason and Bharti, Taroon and Zhou, Ming},
  journal={arXiv preprint arXiv:2002.06353},
  year={2020}
}

@incollection{kruse2022multi,
  title={Multi-layer perceptrons},
  author={Kruse, Rudolf and Mostaghim, Sanaz and Borgelt, Christian and Braune, Christian and Steinbrecher, Matthias},
  booktitle={Computational intelligence: a methodological introduction},
  pages={53--124},
  year={2022},
  publisher={Springer}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@inproceedings{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  booktitle={International conference on machine learning},
  pages={19730--19742},
  year={2023},
  organization={PMLR}
}

@article{touvron2023llama,
  title={Llama 2: Open foundation and fine-tuned chat models},
  author={Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and others},
  journal={arXiv preprint arXiv:2307.09288},
  year={2023}
}

@inproceedings{carreira2017quo,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@article{lin2022prototypical,
  title={Prototypical graph contrastive learning},
  author={Lin, Shuai and Liu, Chen and Zhou, Pan and Hu, Zi-Yuan and Wang, Shuojia and Zhao, Ruihui and Zheng, Yefeng and Lin, Liang and Xing, Eric and Liang, Xiaodan},
  journal={IEEE transactions on neural networks and learning systems},
  volume={35},
  number={2},
  pages={2747--2758},
  year={2022},
  publisher={IEEE}
}

@article{oord2018representation,
  title={Representation learning with contrastive predictive coding},
  author={Oord, Aaron van den and Li, Yazhe and Vinyals, Oriol},
  journal={arXiv preprint arXiv:1807.03748},
  year={2018}
}

@inproceedings{zhang2021ms,
  title={MS-TCN: A multiscale temporal convolutional network for fault diagnosis in industrial processes},
  author={Zhang, Jiyang and Wang, Yuxuan and Tang, Jianxiong and Zou, Jianxiao and Fan, Shicai},
  booktitle={2021 American Control Conference (ACC)},
  pages={1601--1606},
  year={2021},
  organization={IEEE}
}

@article{li2023ms,
  title={Ms-tcn++: Multi-stage temporal convolutional network for action segmentation},
  author={Li, Shijie and Farha, Yazan Abu and Liu, Yun and Cheng, Ming-Ming and Gall, Juergen},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={6},
  pages={6647--6658},
  year={2023}
}

@inproceedings{radford2021learning,
  title={Learning transferable visual models from natural language supervision},
  author={Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle={International conference on machine learning},
  pages={8748--8763},
  year={2021},
  organization={PmLR}
}

@article{reimers2019sentence,
  title={Sentence-bert: Sentence embeddings using siamese bert-networks},
  author={Reimers, Nils and Gurevych, Iryna},
  journal={arXiv preprint arXiv:1908.10084},
  year={2019}
}

@article{shi2019simple,
  title={Simple bert models for relation extraction and semantic role labeling},
  author={Shi, Peng and Lin, Jimmy},
  journal={arXiv preprint arXiv:1904.05255},
  year={2019}
}

@inproceedings{sener2022assembly101,
  title={Assembly101: A large-scale multi-view video dataset for understanding procedural activities},
  author={Sener, Fadime and Chatterjee, Dibyadip and Shelepov, Daniel and He, Kun and Singhania, Dipika and Wang, Robert and Yao, Angela},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21096--21106},
  year={2022}
}

@inproceedings{grauman2024ego,
  title={Ego-exo4d: Understanding skilled human activity from first-and third-person perspectives},
  author={Grauman, Kristen and Westbury, Andrew and Torresani, Lorenzo and Kitani, Kris and Malik, Jitendra and Afouras, Triantafyllos and Ashutosh, Kumar and Baiyya, Vijay and Bansal, Siddhant and Boote, Bikram and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={19383--19400},
  year={2024}
}

@inproceedings{wang2023holoassist,
  title={Holoassist: an egocentric human interaction dataset for interactive ai assistants in the real world},
  author={Wang, Xin and Kwon, Taein and Rad, Mahdi and Pan, Bowen and Chakraborty, Ishani and Andrist, Sean and Bohus, Dan and Feniello, Ashley and Tekin, Bugra and Frujeri, Felipe Vieira and others},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={20270--20281},
  year={2023}
}

@article{liu2024ikea,
  title={Ikea manuals at work: 4d grounding of assembly instructions on internet videos},
  author={Liu, Yunong and Eyzaguirre, Cristobal and Li, Manling and Khanna, Shubh and Niebles, Juan Carlos and Ravi, Vineeth and Mishra, Saumitra and Liu, Weiyu and Wu, Jiajun},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={80536--80566},
  year={2024}
}

@article{peddi2024captaincook4d,
  title={CaptainCook4D: A dataset for understanding errors in procedural activities},
  author={Peddi, Rohith and Arya, Shivvrat and Challa, Bharath and Pallapothula, Likhitha and Vyas, Akshay and Gouripeddi, Bhavya and Zhang, Qifan and Wang, Jikai and Komaragiri, Vasundhara and Ragan, Eric and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={135626--135679},
  year={2024}
}

@inproceedings{jang2019epic,
  title={Epic-tent: An egocentric video dataset for camping tent assembly},
  author={Jang, Youngkyoon and Sullivan, Brian and Ludwig, Casimir and Gilchrist, Iain and Damen, Dima and Mayol-Cuevas, Walterio},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops},
  pages={0--0},
  year={2019}
}

@article{northcutt2020egocom,
  title={Egocom: A multi-person multi-modal egocentric communications dataset},
  author={Northcutt, Curtis G and Zha, Shengxin and Lovegrove, Steven and Newcombe, Richard},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={6},
  pages={6783--6793},
  year={2020},
  publisher={IEEE}
}

@inproceedings{schoonbeek2024industreal,
  title={Industreal: A dataset for procedure step recognition handling execution errors in egocentric videos in an industrial-like setting},
  author={Schoonbeek, Tim J and Houben, Tim and Onvlee, Hans and Van der Sommen, Fons and others},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={4365--4374},
  year={2024}
}

@article{ungureanu2020hololens,
  title={Hololens 2 research mode as a tool for computer vision research},
  author={Ungureanu, Dorin and Bogo, Federica and Galliani, Silvano and Sama, Pooja and Duan, Xin and Meekhof, Casey and St{\"u}hmer, Jan and Cashman, Thomas J and Tekin, Bugra and Sch{\"o}nberger, Johannes L and others},
  journal={arXiv preprint arXiv:2008.11239},
  year={2020}
}


@inproceedings{subetha2016survey,
  title={A survey on human activity recognition from videos},
  author={Subetha, T and Chitrakala, S},
  booktitle={2016 international conference on information communication and embedded systems (ICICES)},
  pages={1--7},
  year={2016},
  organization={IEEE}
}

@article{song2013united,
  title={United we stand, divided we fall: a meta-analysis of experiments on clonal integration and its relationship to invasiveness},
  author={Song, Yao-Bin and Yu, Fei-Hai and Keser, Lidewij H and Dawson, Wayne and Fischer, Markus and Dong, Ming and van Kleunen, Mark},
  journal={Oecologia},
  volume={171},
  number={2},
  pages={317--327},
  year={2013},
  publisher={Springer}
}

@inproceedings{he2016deep,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}

@inproceedings{alayrac2016unsupervised,
  title={Unsupervised learning from narrated instruction videos},
  author={Alayrac, Jean-Baptiste and Bojanowski, Piotr and Agrawal, Nishant and Sivic, Josef and Laptev, Ivan and Lacoste-Julien, Simon},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={4575--4583},
  year={2016}
}

@inproceedings{elhamifar2019unsupervised,
  title={Unsupervised procedure learning via joint dynamic summarization},
  author={Elhamifar, Ehsan and Naing, Zwe},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={6341--6350},
  year={2019}
}

@inproceedings{kukleva2019unsupervised,
  title={Unsupervised learning of action classes with continuous temporal embedding},
  author={Kukleva, Anna and Kuehne, Hilde and Sener, Fadime and Gall, Jurgen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12066--12074},
  year={2019}
}

@article{chowdhury2024opel,
  title={Opel: Optimal transport guided procedure learning},
  author={Chowdhury, Sayeed Shafayet and Chandra, Soumyadeep and Roy, Kaushik},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={59984--60011},
  year={2024}
}

@book{villani2008optimal,
  title={Optimal transport: old and new},
  author={Villani, C{\'e}dric and others},
  volume={338},
  year={2008},
  publisher={Springer}
}

@inproceedings{sener2015unsupervised,
  title={Unsupervised semantic parsing of video collections},
  author={Sener, Ozan and Zamir, Amir R and Savarese, Silvio and Saxena, Ashutosh},
  booktitle={Proceedings of the IEEE International conference on Computer Vision},
  pages={4480--4488},
  year={2015}
}

@inproceedings{grover2016node2vec,
  title={node2vec: Scalable feature learning for networks},
  author={Grover, Aditya and Leskovec, Jure},
  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on Knowledge discovery and data mining},
  pages={855--864},
  year={2016}
}

@inproceedings{jaggi2013revisiting,
  title={Revisiting Frank-Wolfe: Projection-free sparse convex optimization},
  author={Jaggi, Martin},
  booktitle={International conference on machine learning},
  pages={427--435},
  year={2013},
  organization={PMLR}
}

@misc{kilgarriff2000wordnet,
  title={Wordnet: An electronic lexical database},
  author={Kilgarriff, Adam},
  year={2000},
  publisher={JSTOR}
}

@article{miller1995wordnet,
  title={WordNet: a lexical database for English},
  author={Miller, George A},
  journal={Communications of the ACM},
  volume={38},
  number={11},
  pages={39--41},
  year={1995},
  publisher={ACM New York, NY, USA}
}

@inproceedings{bojanowski2014weakly,
  title={Weakly supervised action labeling in videos under ordering constraints},
  author={Bojanowski, Piotr and Lajugie, R{\'e}mi and Bach, Francis and Laptev, Ivan and Ponce, Jean and Schmid, Cordelia and Sivic, Josef},
  booktitle={European conference on computer vision},
  pages={628--643},
  year={2014},
  organization={Springer}
}

@inproceedings{bojanowski2015weakly,
  title={Weakly-supervised alignment of video with text},
  author={Bojanowski, Piotr and Lajugie, R{\'e}mi and Grave, Edouard and Bach, Francis and Laptev, Ivan and Ponce, Jean and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={4462--4470},
  year={2015}
}


@inproceedings{richard2018neuralnetwork,
  title={Neuralnetwork-viterbi: A framework for weakly supervised video learning},
  author={Richard, Alexander and Kuehne, Hilde and Iqbal, Ahsan and Gall, Juergen},
  booktitle={Proceedings of the IEEE conference on Computer Vision and Pattern Recognition},
  pages={7386--7395},
  year={2018}
}

@inproceedings{yi2012image,
  title={Image segmentation: A survey of graph-cut methods},
  author={Yi, Faliu and Moon, Inkyu},
  booktitle={2012 international conference on systems and informatics (ICSAI2012)},
  pages={1936--1941},
  year={2012},
  organization={IEEE}
}

@article{fox2014joint,
  title={Joint modeling of multiple time series via the beta process with application to motion capture segmentation},
  author={Fox, Emily B and Hughes, Michael C and Sudderth, Erik B and Jordan, Michael I},
  year={2014}
}

@article{shannon1948mathematical,
  title={A mathematical theory of communication},
  author={Shannon, Claude E},
  journal={The Bell system technical journal},
  volume={27},
  number={3},
  pages={379--423},
  year={1948},
  publisher={Nokia Bell Labs}
}

@inproceedings{lin2022learning,
  title={Learning to recognize procedural activities with distant supervision},
  author={Lin, Xudong and Petroni, Fabio and Bertasius, Gedas and Rohrbach, Marcus and Chang, Shih-Fu and Torresani, Lorenzo},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13853--13863},
  year={2022}
}

@inproceedings{anthonio2020wikihowtoimprove,
  title={wikiHowToImprove: A resource and analyses on edits in instructional texts},
  author={Anthonio, Talita and Bhat, Irshad and Roth, Michael},
  booktitle={Proceedings of the Twelfth Language Resources and Evaluation Conference},
  pages={5721--5729},
  year={2020}
}

@inproceedings{miech2020end,
  title={End-to-end learning of visual representations from uncurated instructional videos},
  author={Miech, Antoine and Alayrac, Jean-Baptiste and Smaira, Lucas and Laptev, Ivan and Sivic, Josef and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={9879--9889},
  year={2020}
}

@inproceedings{zhou2018towards,
  title={Towards automatic learning of procedures from web instructional videos},
  author={Zhou, Luowei and Xu, Chenliang and Corso, Jason},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={32},
  number={1},
  year={2018}
}

@inproceedings{zhukov2019cross,
  title={Cross-task weakly supervised learning from instructional videos},
  author={Zhukov, Dimitri and Alayrac, Jean-Baptiste and Cinbis, Ramazan Gokberk and Fouhey, David and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={3537--3545},
  year={2019}
}

@inproceedings{naing2020procedure,
  title={Procedure completion by learning from partial summaries},
  author={Naing, Zwe and Elhamifar, Ehsan},
  booktitle={British Machine Vision Conference},
  year={2020}
}

@inproceedings{miech2019howto100m,
  title={Howto100m: Learning a text-video embedding by watching hundred million narrated video clips},
  author={Miech, Antoine and Zhukov, Dimitri and Alayrac, Jean-Baptiste and Tapaswi, Makarand and Laptev, Ivan and Sivic, Josef},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={2630--2640},
  year={2019}
}

@inproceedings{bertasius2021space,
  title={Is space-time attention all you need for video understanding?},
  author={Bertasius, Gedas and Wang, Heng and Torresani, Lorenzo},
  booktitle={Icml},
  volume={2},
  number={3},
  pages={4},
  year={2021}
}

@inproceedings{deng2009imagenet,
  title={Imagenet: A large-scale hierarchical image database},
  author={Deng, Jia and Dong, Wei and Socher, Richard and Li, Li-Jia and Li, Kai and Fei-Fei, Li},
  booktitle={2009 IEEE conference on computer vision and pattern recognition},
  pages={248--255},
  year={2009},
  organization={Ieee}
}

@inproceedings{lin2014microsoft,
  title={Microsoft coco: Common objects in context},
  author={Lin, Tsung-Yi and Maire, Michael and Belongie, Serge and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{\'a}r, Piotr and Zitnick, C Lawrence},
  booktitle={European conference on computer vision},
  pages={740--755},
  year={2014},
  organization={Springer}
}

@article{graves2005framewise,
  title={Framewise phoneme classification with bidirectional LSTM and other neural network architectures},
  author={Graves, Alex and Schmidhuber, J{\"u}rgen},
  journal={Neural networks},
  volume={18},
  number={5-6},
  pages={602--610},
  year={2005},
  publisher={Elsevier}
}

@inproceedings{elhamifar2019sequential,
  title={Sequential facility location: Approximate submodularity and greedy algorithm},
  author={Elhamifar, Ehsan},
  booktitle={International Conference on Machine Learning},
  pages={1784--1793},
  year={2019},
  organization={PMLR}
}

@article{krause2014submodular,
  title={Submodular function maximization.},
  author={Krause, Andreas and Golovin, Daniel},
  journal={Tractability},
  volume={3},
  number={71-104},
  pages={3},
  year={2014}
}

@inproceedings{kuehne2014language,
  title={The language of actions: Recovering the syntax and semantics of goal-directed human activities},
  author={Kuehne, Hilde and Arslan, Ali and Serre, Thomas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={780--787},
  year={2014}
}

@inproceedings{bansal2022my,
  title={My view is the best view: Procedure learning from egocentric videos},
  author={Bansal, Siddhant and Arora, Chetan and Jawahar, CV},
  booktitle={European Conference on Computer Vision},
  pages={657--675},
  year={2022},
  organization={Springer}
}

@inproceedings{dwibedi2019temporal,
  title={Temporal cycle-consistency learning},
  author={Dwibedi, Debidatta and Aytar, Yusuf and Tompson, Jonathan and Sermanet, Pierre and Zisserman, Andrew},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={1801--1810},
  year={2019}
}

@article{mahmood2025procedure,
  title={Procedure Learning via Regularized Gromov-Wasserstein Optimal Transport},
  author={Mahmood, Syed Ahmed and Ali, Ali Shah and Ahmed, Umer and Fateh, Fawad Javed and Zia, M Zeeshan and Tran, Quoc-Huy},
  journal={arXiv preprint arXiv:2507.15540},
  year={2025}
}

@article{shah2023steps,
  title={Steps: Self-supervised key step extraction from unlabeled procedural videos},
  author={Shah, Anshul and Lundell, Benjamin and Sawhney, Harpreet and Chellappa, Rama},
  journal={arXiv preprint arXiv:2301.00794},
  volume={2},
  year={2023}
}

@inproceedings{haresh2021learning,
  title={Learning by aligning videos in time},
  author={Haresh, Sanjay and Kumar, Sateesh and Coskun, Huseyin and Syed, Shahram N and Konin, Andrey and Zia, Zeeshan and Tran, Quoc-Huy},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={5548--5558},
  year={2021}
}

@article{wilson2024fused,
  title={Fused Gromov-Wasserstein Variance Decomposition with Linear Optimal Transport},
  author={Wilson, Michael and Needham, Tom and Srivastava, Anuj},
  journal={arXiv preprint arXiv:2411.10204},
  year={2024}
}

@article{boykov2002fast,
  title={Fast approximate energy minimization via graph cuts},
  author={Boykov, Yuri and Veksler, Olga and Zabih, Ramin},
  journal={IEEE Transactions on pattern analysis and machine intelligence},
  volume={23},
  number={11},
  pages={1222--1239},
  year={2002},
  publisher={IEEE}
}

@inproceedings{teed2020raft,
  title={Raft: Recurrent all-pairs field transforms for optical flow},
  author={Teed, Zachary and Deng, Jia},
  booktitle={European conference on computer vision},
  pages={402--419},
  year={2020},
  organization={Springer}
}

@article{martinez2019openpose,
  title={Openpose: Whole-body pose estimation},
  author={Mart{\i}nez, Gin{\'e}s Hidalgo},
  journal={Ph. D. dissertation},
  year={2019},
  publisher={Carnegie Mellon University}
}

@inproceedings{laptev2008learning,
  title={Learning realistic human actions from movies},
  author={Laptev, Ivan and Marszalek, Marcin and Schmid, Cordelia and Rozenfeld, Benjamin},
  booktitle={2008 IEEE conference on computer vision and pattern recognition},
  pages={1--8},
  year={2008},
  organization={IEEE}
}

@inproceedings{matikainen2010representing,
  title={Representing pairwise spatial and temporal relations for action recognition},
  author={Matikainen, Pyry and Hebert, Martial and Sukthankar, Rahul},
  booktitle={European Conference on Computer Vision},
  pages={508--521},
  year={2010},
  organization={Springer}
}

@inproceedings{wang2009evaluation,
  title={Evaluation of local spatio-temporal features for action recognition},
  author={Wang, Heng and Ullah, Muhammad Muneeb and Klaser, Alexander and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Bmvc 2009-british machine vision conference},
  pages={124--1},
  year={2009},
  organization={BMVA Press}
}

@article{eddy1996hidden,
  title={Hidden markov models},
  author={Eddy, Sean R},
  journal={Current opinion in structural biology},
  volume={6},
  number={3},
  pages={361--365},
  year={1996},
  publisher={Elsevier}
}

@article{young2002htk,
  title={The HTK book},
  author={Young, Steve and Evermann, Gunnar and Gales, Mark and Hain, Thomas and Kershaw, Dan and Liu, Xunying and Moore, Gareth and Odell, Julian and Ollason, Dave and Povey, Dan and others},
  journal={Cambridge university engineering department},
  volume={3},
  number={175},
  pages={12},
  year={2002}
}

@inproceedings{tang2019coin,
  title={Coin: A large-scale dataset for comprehensive instructional video analysis},
  author={Tang, Yansong and Ding, Dajun and Rao, Yongming and Zheng, Yu and Zhang, Danyang and Zhao, Lili and Lu, Jiwen and Zhou, Jie},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={1207--1216},
  year={2019}
}



@article{de2009guide,
  title={Guide to the carnegie mellon university multimodal activity (cmu-mmac) database},
  author={De la Torre, Fernando and Hodgins, Jessica and Bargteil, Adam and Martin, Xavier and Macey, Justin and Collado, Alex and Beltran, Pep},
  year={2009},
  publisher={Carnegie Mellon University}
}

@article{li2021eye,
  title={In the eye of the beholder: Gaze and actions in first person video},
  author={Li, Yin and Liu, Miao and Rehg, James M},
  journal={IEEE transactions on pattern analysis and machine intelligence},
  volume={45},
  number={6},
  pages={6731--6747},
  year={2021},
  publisher={IEEE}
}

@inproceedings{shen2021learning,
  title={Learning to segment actions from visual and language instructions via differentiable weak sequence alignment},
  author={Shen, Yuhan and Wang, Lu and Elhamifar, Ehsan},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10156--10165},
  year={2021}
}

@article{zeng2025multimodal,
  title={Multimodal knowledge retrieval of layout image text based on CLIP and ViT},
  author={Zeng, Bowen and Lu, Rong and Mao, Guanghu},
  journal={Signal, Image and Video Processing},
  volume={19},
  number={12},
  pages={1001},
  year={2025},
  publisher={Springer}
}

@inproceedings{yang2024clip,
  title={CLIP-ViT Detector: Side Adapter with Prompt for Vision Transformer Object Detection},
  author={Yang, Han and Xu, Minxia and Sun, Zhiyong and Song, Bo and Cheng, Erkang},
  booktitle={2024 7th International Conference on Algorithms, Computing and Artificial Intelligence (ACAI)},
  pages={1--8},
  year={2024},
  organization={IEEE}
}

@inproceedings{mehta2025efficiently,
  title={Efficiently Fine-Tuned Flan T5 LLM for Happiness Oriented Dialogue Summarization},
  author={Mehta, Vivek and Agarwal, Mohit and Yadav, Sanskriti and Sahni, Jahnvi},
  booktitle={2025 International Conference on Innovation in Computing and Engineering (ICE)},
  pages={1--6},
  year={2025},
  organization={IEEE}
}

@article{yang2023dawn,
  title={The dawn of lmms: Preliminary explorations with gpt-4v (ision)},
  author={Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  journal={arXiv preprint arXiv:2309.17421},
  year={2023}
}

@article{wu2023early,
  title={An early evaluation of gpt-4v (ision)},
  author={Wu, Yang and Wang, Shilong and Yang, Hao and Zheng, Tian and Zhang, Hongbo and Zhao, Yanyan and Qin, Bing},
  journal={arXiv preprint arXiv:2310.16534},
  year={2023}
}

@inproceedings{islam2025gpt,
  title={Gpt-4o: The cutting-edge advancement in multimodal llm},
  author={Islam, Raisa and Moushi, Owana Marzia},
  booktitle={Intelligent Computing-Proceedings of the Computing Conference},
  pages={47--60},
  year={2025},
  organization={Springer}
}

@article{team2023gemini,
  title={Gemini: a family of highly capable multimodal models},
  author={Team, Gemini and Anil, Rohan and Borgeaud, Sebastian and Alayrac, Jean-Baptiste and Yu, Jiahui and Soricut, Radu and Schalkwyk, Johan and Dai, Andrew M and Hauth, Anja and Millican, Katie and others},
  journal={arXiv preprint arXiv:2312.11805},
  year={2023}
}

@article{he2024think,
  title={Think-Program-reCtify: 3D Situated Reasoning with Large Language Models},
  author={He, Qingrong and Lin, Kejun and Chen, Shizhe and Hu, Anwen and Jin, Qin},
  journal={arXiv preprint arXiv:2404.14705},
  year={2024}
}

@article{wang2023lifelongmemory,
  title={Lifelongmemory: Leveraging llms for answering queries in long-form egocentric videos},
  author={Wang, Ying and Yang, Yanlai and Ren, Mengye},
  journal={arXiv preprint arXiv:2312.05269},
  year={2023}
}

@article{liu2023visual,
  title={Visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Wu, Qingyang and Lee, Yong Jae},
  journal={Advances in neural information processing systems},
  volume={36},
  pages={34892--34916},
  year={2023}
}

@inproceedings{liu2024improved,
  title={Improved baselines with visual instruction tuning},
  author={Liu, Haotian and Li, Chunyuan and Li, Yuheng and Lee, Yong Jae},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={26296--26306},
  year={2024}
}

@inproceedings{zhao2023learning,
  title={Learning video representations from large language models},
  author={Zhao, Yue and Misra, Ishan and Kr{\"a}henb{\"u}hl, Philipp and Girdhar, Rohit},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6586--6597},
  year={2023}
}

@article{baktash2023gpt,
  title={Gpt-4: A review on advancements and opportunities in natural language processing},
  author={Baktash, Jawid Ahmad and Dawodi, Mursal},
  journal={arXiv preprint arXiv:2305.03195},
  year={2023}
}

@inproceedings{ramakrishnan2023naq,
  title={Naq: Leveraging narrations as queries to supervise episodic memory},
  author={Ramakrishnan, Santhosh Kumar and Al-Halah, Ziad and Grauman, Kristen},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={6694--6703},
  year={2023}
}

@article{hou2023groundnlq,
  title={Groundnlq@ ego4d natural language queries challenge 2023},
  author={Hou, Zhijian and Ji, Lei and Gao, Difei and Zhong, Wanjun and Yan, Kun and Li, Chao and Chan, Wing-Kwong and Ngo, Chong-Wah and Duan, Nan and Shou, Mike Zheng},
  journal={arXiv preprint arXiv:2306.15255},
  year={2023}
}

@article{zhang2024hcqa,
  title={Hcqa@ ego4d egoschema challenge 2024},
  author={Zhang, Haoyu and Xie, Yuquan and Feng, Yisen and Li, Zaijing and Liu, Meng and Nie, Liqiang},
  journal={arXiv preprint arXiv:2406.15771},
  year={2024}
}

@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}

@article{zhang2025hcqa,
  title={Hcqa-1.5@ ego4d egoschema challenge 2025},
  author={Zhang, Haoyu and Feng, Yisen and Chu, Qiaohui and Liu, Meng and Guan, Weili and Wang, Yaowei and Nie, Liqiang},
  journal={arXiv preprint arXiv:2505.20644},
  year={2025}
}

@article{team2024gemini,
  title={Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context},
  author={Team, Gemini and Georgiev, Petko and Lei, Ving Ian and Burnell, Ryan and Bai, Libin and Gulati, Anmol and Tanzer, Garrett and Vincent, Damien and Pan, Zhufeng and Wang, Shibo and others},
  journal={arXiv preprint arXiv:2403.05530},
  year={2024}
}

@article{sonoda2024diagnostic,
  title={Diagnostic performances of gpt-4o, claude 3 opus, and gemini 1.5 pro in “diagnosis please” cases},
  author={Sonoda, Yuki and Kurokawa, Ryo and Nakamura, Yuta and Kanzawa, Jun and Kurokawa, Mariko and Ohizumi, Yuji and Gonoi, Wataru and Abe, Osamu},
  journal={Japanese journal of radiology},
  volume={42},
  number={11},
  pages={1231--1235},
  year={2024},
  publisher={Springer}
}

@article{fachada2025gpt,
  title={Gpt-4.1 sets the standard in automated experiment design using novel python libraries},
  author={Fachada, Nuno and Fernandes, Daniel and Fernandes, Carlos M and Ferreira-Saraiva, Bruno D and Matos-Carvalho, Jo{\~a}o P},
  journal={arXiv preprint arXiv:2508.00033},
  year={2025}
}

@article{hui2024qwen2,
  title={Qwen2. 5-coder technical report},
  author={Hui, Binyuan and Yang, Jian and Cui, Zeyu and Yang, Jiaxi and Liu, Dayiheng and Zhang, Lei and Liu, Tianyu and Zhang, Jiajun and Yu, Bowen and Lu, Keming and others},
  journal={arXiv preprint arXiv:2409.12186},
  year={2024}
}

@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}

@article{taluzzi2025pixels,
  title={From Pixels to Graphs: using Scene and Knowledge Graphs for HD-EPIC VQA Challenge},
  author={Taluzzi, Agnese and Gesualdi, Davide and Santambrogio, Riccardo and Plizzari, Chiara and Palermo, Francesca and Mentasti, Simone and Matteucci, Matteo},
  journal={arXiv preprint arXiv:2506.08553},
  year={2025}
}

@article{narzary2025comparative,
  title={Comparative Study of Zero-Shot Cross-Lingual Transfer for Bodo POS and NER Tagging Using Gemini 2.0 Flash Thinking Experimental Model},
  author={Narzary, Sanjib and Brahma, Bihung and Mahilary, Haradip and Brahma, Mahananda and Som, Bidisha and Nandi, Sukumar},
  journal={arXiv preprint arXiv:2503.04405},
  year={2025}
}

@inproceedings{speer2017conceptnet,
  title={Conceptnet 5.5: An open multilingual graph of general knowledge},
  author={Speer, Robyn and Chin, Joshua and Havasi, Catherine},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={31},
  number={1},
  year={2017}
}

@inproceedings{yang2025egolife,
  title={Egolife: Towards egocentric life assistant},
  author={Yang, Jingkang and Liu, Shuai and Guo, Hongming and Dong, Yuhao and Zhang, Xiamengwei and Zhang, Sicheng and Wang, Pengyun and Zhou, Zitang and Xie, Binzhu and Wang, Ziyue and others},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={28885--28900},
  year={2025}
}

@inproceedings{di2024grounded,
  title={Grounded question-answering in long egocentric videos},
  author={Di, Shangzhe and Xie, Weidi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={12934--12943},
  year={2024}
}

@article{chung2024scaling,
  title={Scaling instruction-finetuned language models},
  author={Chung, Hyung Won and Hou, Le and Longpre, Shayne and Zoph, Barret and Tay, Yi and Fedus, William and Li, Yunxuan and Wang, Xuezhi and Dehghani, Mostafa and Brahma, Siddhartha and others},
  journal={Journal of Machine Learning Research},
  volume={25},
  number={70},
  pages={1--53},
  year={2024}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lin2022egocentric,
  title={Egocentric video-language pretraining},
  author={Lin, Kevin Qinghong and Wang, Jinpeng and Soldan, Mattia and Wray, Michael and Yan, Rui and Xu, Eric Z and Gao, Difei and Tu, Rong-Cheng and Zhao, Wenzhe and Kong, Weijie and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7575--7586},
  year={2022}
}

@article{chen2022internvideo,
  title={Internvideo-ego4d: A pack of champion solutions to ego4d challenges},
  author={Chen, Guo and Xing, Sen and Chen, Zhe and Wang, Yi and Li, Kunchang and Li, Yizhuo and Liu, Yi and Wang, Jiahao and Zheng, Yin-Dong and Huang, Bingkun and others},
  journal={arXiv preprint arXiv:2211.09529},
  year={2022}
}

@article{huang2025egocentric,
  title={An egocentric vision-language model based portable real-time smart assistant},
  author={Huang, Yifei and Xu, Jilan and Pei, Baoqi and He, Yuping and Chen, Guo and Zhang, Mingfang and Yang, Lijin and Nie, Zheng and Liu, Jinyao and Fan, Guoshun and others},
  journal={arXiv preprint arXiv:2503.04250},
  year={2025}
}


@article{cai2024internlm2,
  title={Internlm2 technical report},
  author={Cai, Zheng and Cao, Maosong and Chen, Haojiong and Chen, Kai and Chen, Keyu and Chen, Xin and Chen, Xun and Chen, Zehui and Chen, Zhi and Chu, Pei and others},
  journal={arXiv preprint arXiv:2403.17297},
  year={2024}
}

@article{song2023ego4d,
  title={Ego4d goal-step: Toward hierarchical understanding of procedural activities},
  author={Song, Yale and Byrne, Eugene and Nagarajan, Tushar and Wang, Huiyu and Martin, Miguel and Torresani, Lorenzo},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={38863--38886},
  year={2023}
}

@article{suglia2024alanavlm,
  title={Alanavlm: A multimodal embodied ai foundation model for egocentric video understanding},
  author={Suglia, Alessandro and Greco, Claudio and Baker, Katie and Part, Jose L and Papaioannou, Ioannis and Eshghi, Arash and Konstas, Ioannis and Lemon, Oliver},
  journal={arXiv preprint arXiv:2406.13807},
  year={2024}
}

@article{mo2022simple,
  title={A simple transformer-based model for ego4d natural language queries challenge},
  author={Mo, Sicheng and Mu, Fangzhou and Li, Yin},
  journal={arXiv preprint arXiv:2211.08704},
  year={2022}
}

@article{liu2023visual,
  title={Visual spatial reasoning},
  author={Liu, Fangyu and Emerson, Guy and Collier, Nigel},
  journal={Transactions of the Association for Computational Linguistics},
  volume={11},
  pages={635--651},
  year={2023},
  publisher={MIT Press One Broadway, 12th Floor, Cambridge, Massachusetts 02142, USA~…}
}

@inproceedings{devalal2018lora,
  title={LoRa technology-an overview},
  author={Devalal, Shilpa and Karthikeyan, A},
  booktitle={2018 second international conference on electronics, communication and aerospace technology (ICECA)},
  pages={284--290},
  year={2018},
  organization={IEEE}
}

@article{kirkpatrick2017overcoming,
  title={Overcoming catastrophic forgetting in neural networks},
  author={Kirkpatrick, James and Pascanu, Razvan and Rabinowitz, Neil and Veness, Joel and Desjardins, Guillaume and Rusu, Andrei A and Milan, Kieran and Quan, John and Ramalho, Tiago and Grabska-Barwinska, Agnieszka and others},
  journal={Proceedings of the national academy of sciences},
  volume={114},
  number={13},
  pages={3521--3526},
  year={2017},
  publisher={National Academy of Sciences}
}

@article{robins1995catastrophic,
  title={Catastrophic forgetting, rehearsal and pseudorehearsal},
  author={Robins, Anthony},
  journal={Connection Science},
  volume={7},
  number={2},
  pages={123--146},
  year={1995},
  publisher={Taylor \& Francis}
}

@inproceedings{jin2024chat,
  title={Chat-univi: Unified visual representation empowers large language models with image and video understanding},
  author={Jin, Peng and Takanobu, Ryuichi and Zhang, Wancai and Cao, Xiaochun and Yuan, Li},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13700--13710},
  year={2024}
}

@inproceedings{chen2025grounded,
  title={Grounded multi-hop videoqa in long-form egocentric videos},
  author={Chen, Qirui and Di, Shangzhe and Xie, Weidi},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={2},
  pages={2159--2167},
  year={2025}
}


@article{wang2022internvideo,
  title={Internvideo: General video foundation models via generative and discriminative learning},
  author={Wang, Yi and Li, Kunchang and Li, Yizhuo and He, Yinan and Huang, Bingkun and Zhao, Zhiyu and Zhang, Hongjie and Xu, Jilan and Liu, Yi and Wang, Zun and others},
  journal={arXiv preprint arXiv:2212.03191},
  year={2022}
}

@article{chiang2023vicuna,
  title={Vicuna: An open-source chatbot impressing gpt-4 with 90\%* chatgpt quality},
  author={Chiang, Wei-Lin and Li, Zhuohan and Lin, Ziqing and Sheng, Ying and Wu, Zhanghao and Zhang, Hao and Zheng, Lianmin and Zhuang, Siyuan and Zhuang, Yonghao and Gonzalez, Joseph E and others},
  journal={See https://vicuna. lmsys. org (accessed 14 April 2023)},
  volume={2},
  number={3},
  pages={6},
  year={2023}
}

@article{biswas2025raven,
  title={RAVEN: Query-Guided Representation Alignment for Question Answering over Audio, Video, Embedded Sensors, and Natural Language},
  author={Biswas, Subrata and Khan, Mohammad Nur Hossain and Islam, Bashima},
  journal={arXiv preprint arXiv:2505.17114},
  year={2025}
}

@article{ye2024mm,
  title={MM-Ego: Towards Building Egocentric Multimodal LLMs for Video QA},
  author={Ye, Hanrong and Zhang, Haotian and Daxberger, Erik and Chen, Lin and Lin, Zongyu and Li, Yanghao and Zhang, Bowen and You, Haoxuan and Xu, Dan and Gan, Zhe and others},
  journal={arXiv preprint arXiv:2410.07177},
  year={2024}
}

@inproceedings{liu2024optimizing,
  title={Optimizing few-shot learning: From static to adaptive in qwen2-7b},
  author={Liu, Wenhao and Bu, Tianxing and Yu, Erchen and Li, Dailin and Ai, Ding and Lu, Zhenyi and Luo, Haoran},
  booktitle={Amazon KDD Cup 2024 Workshop},
  year={2024}
}

@article{wu2025st,
  title={St-think: How multimodal large language models reason about 4d worlds from ego-centric videos},
  author={Wu, Peiran and Liu, Yunze and Liu, Miao and Shen, Junxiao},
  journal={arXiv preprint arXiv:2503.12542},
  year={2025}
}

@article{mangalam2023egoschema,
  title={Egoschema: A diagnostic benchmark for very long-form video language understanding},
  author={Mangalam, Karttikeya and Akshulakov, Raiymbek and Malik, Jitendra},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={46212--46244},
  year={2023}
}



@inproceedings{majumdar2024openeqa,
  title={Openeqa: Embodied question answering in the era of foundation models},
  author={Majumdar, Arjun and Ajay, Anurag and Zhang, Xiaohan and Putta, Pranav and Yenamandra, Sriram and Henaff, Mikael and Silwal, Sneha and Mcvay, Paul and Maksymets, Oleksandr and Arnaud, Sergio and others},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={16488--16498},
  year={2024}
}

@inproceedings{plizzari2025omnia,
  title={Omnia de egotempo: Benchmarking temporal understanding of multi-modal llms in egocentric videos},
  author={Plizzari, Chiara and Tonioni, Alessio and Xian, Yongqin and Kulshrestha, Achin and Tombari, Federico},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={24129--24138},
  year={2025}
}

@article{jia2022egotaskqa,
  title={Egotaskqa: Understanding human tasks in egocentric videos},
  author={Jia, Baoxiong and Lei, Ting and Zhu, Song-Chun and Huang, Siyuan},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={3343--3360},
  year={2022}
}

@article{xiao2025egoblind,
  title={EgoBlind: Towards Egocentric Visual Assistance for the Blind People},
  author={Xiao, Junbin and Huang, Nanxin and Qiu, Hao and Tao, Zhulin and Yang, Xun and Hong, Richang and Wang, Meng and Yao, Angela},
  journal={arXiv preprint arXiv:2503.08221},
  year={2025}
}

@inproceedings{gurari2018vizwiz,
  title={Vizwiz grand challenge: Answering visual questions from blind people},
  author={Gurari, Danna and Li, Qing and Stangl, Abigale J and Guo, Anhong and Lin, Chi and Grauman, Kristen and Luo, Jiebo and Bigham, Jeffrey P},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={3608--3617},
  year={2018}
}

@article{song2024video,
  title={Video question answering for people with visual impairments using an egocentric 360-degree camera},
  author={Song, Inpyo and Joo, Minjun and Kwon, Joonhyung and Lee, Jangwon},
  journal={arXiv preprint arXiv:2405.19794},
  year={2024}
}

@inproceedings{wong2022assistq,
  title={Assistq: Affordance-centric question-driven task completion for egocentric assistant},
  author={Wong, Benita and Chen, Joya and Wu, You and Lei, Stan Weixian and Mao, Dongxing and Gao, Difei and Shou, Mike Zheng},
  booktitle={European Conference on Computer Vision},
  pages={485--501},
  year={2022},
  organization={Springer}
}

@inproceedings{song2017semantic,
  title={Semantic scene completion from a single depth image},
  author={Song, Shuran and Yu, Fisher and Zeng, Andy and Chang, Angel X and Savva, Manolis and Funkhouser, Thomas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={1746--1754},
  year={2017}
}

@inproceedings{sultani2018real,
  title={Real-world anomaly detection in surveillance videos},
  author={Sultani, Waqas and Chen, Chen and Shah, Mubarak},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={6479--6488},
  year={2018}
}

@inproceedings{epstein2020oops,
  title={Oops! predicting unintentional action in video},
  author={Epstein, Dave and Chen, Boyuan and Vondrick, Carl},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={919--929},
  year={2020}
}

@article{kundu2024algo,
  title={ALGO: Object-Grounded Visual Commonsense Reasoning for Open-World Egocentric Action Recognition},
  author={Kundu, Sanjoy and Trehan, Shubham and Aakur, Sathyanarayanan N},
  journal={arXiv preprint arXiv:2406.05722},
  year={2024}
}

@inproceedings{ma2025drvideo,
  title={Drvideo: Document retrieval based long video understanding},
  author={Ma, Ziyu and Gou, Chenhui and Shi, Hengcan and Sun, Bin and Li, Shutao and Rezatofighi, Hamid and Cai, Jianfei},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={18936--18946},
  year={2025}
}

@article{park2024too,
  title={Too many frames, not all useful: Efficient strategies for long-form video qa},
  author={Park, Jongwoo and Ranasinghe, Kanchana and Kahatapitiya, Kumara and Ryu, Wonjeong and Kim, Donghyun and Ryoo, Michael S},
  journal={arXiv preprint arXiv:2406.09396},
  year={2024}
}

@inproceedings{wang2024videoagent,
  title={Videoagent: Long-form video understanding with large language model as agent},
  author={Wang, Xiaohan and Zhang, Yuhui and Zohar, Orr and Yeung-Levy, Serena},
  booktitle={European Conference on Computer Vision},
  pages={58--76},
  year={2024},
  organization={Springer}
}

@article{li2024llms,
  title={Llms meet long video: Advancing long video comprehension with an interactive visual adapter in llms},
  author={Li, Yunxin and Chen, Xinyu and Hu, Baotain and Zhang, Min},
  journal={arXiv preprint arXiv:2402.13546},
  volume={3},
  number={7},
  year={2024}
}



@inproceedings{ren2024timechat,
  title={Timechat: A time-sensitive multimodal large language model for long video understanding},
  author={Ren, Shuhuai and Yao, Linli and Li, Shicheng and Sun, Xu and Hou, Lu},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14313--14323},
  year={2024}
}

@inproceedings{song2024moviechat,
  title={Moviechat: From dense token to sparse memory for long video understanding},
  author={Song, Enxin and Chai, Wenhao and Wang, Guanhong and Zhang, Yucheng and Zhou, Haoyang and Wu, Feiyang and Chi, Haozhe and Guo, Xun and Ye, Tian and Zhang, Yanting and others},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18221--18232},
  year={2024}
}

@inproceedings{kitani2012activity,
  title={Activity forecasting},
  author={Kitani, Kris M and Ziebart, Brian D and Bagnell, James Andrew and Hebert, Martial},
  booktitle={European conference on computer vision},
  pages={201--214},
  year={2012},
  organization={Springer}
}

@inproceedings{lan2014hierarchical,
  title={A hierarchical representation for future action prediction},
  author={Lan, Tian and Chen, Tsung-Chuan and Savarese, Silvio},
  booktitle={European conference on computer vision},
  pages={689--704},
  year={2014},
  organization={Springer}
}

@inproceedings{huang2014action,
  title={Action-reaction: Forecasting the dynamics of human interaction},
  author={Huang, De-An and Kitani, Kris M},
  booktitle={European Conference on Computer Vision},
  pages={489--504},
  year={2014},
  organization={Springer}
}

@inproceedings{jain2015car,
  title={Car that knows before you do: Anticipating maneuvers via learning temporal driving models},
  author={Jain, Ashesh and Koppula, Hema S and Raghavan, Bharad and Soh, Shane and Saxena, Ashutosh},
  booktitle={Proceedings of the IEEE International Conference on Computer Vision},
  pages={3182--3190},
  year={2015}
}

@inproceedings{felsen2017will,
  title={What will happen next? forecasting player moves in sports videos},
  author={Felsen, Panna and Agrawal, Pulkit and Malik, Jitendra},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3342--3351},
  year={2017}
}

@article{gao2017red,
  title={Red: Reinforced encoder-decoder networks for action anticipation},
  author={Gao, Jiyang and Yang, Zhenheng and Nevatia, Ram},
  journal={arXiv preprint arXiv:1707.04818},
  year={2017}
}

@inproceedings{abu2018will,
  title={When will you do what?-anticipating temporal occurrences of activities},
  author={Abu Farha, Yazan and Richard, Alexander and Gall, Juergen},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={5343--5352},
  year={2018}
}

@inproceedings{vondrick2016anticipating,
  title={Anticipating visual representations from unlabeled video},
  author={Vondrick, Carl and Pirsiavash, Hamed and Torralba, Antonio},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={98--106},
  year={2016}
}

@inproceedings{liu2020forecasting,
  title={Forecasting human-object interaction: joint prediction of motor attention and actions in first person video},
  author={Liu, Miao and Tang, Siyu and Li, Yin and Rehg, James M},
  booktitle={European conference on computer vision},
  pages={704--721},
  year={2020},
  organization={Springer}
}

@article{dessalene2021forecasting,
  title={Forecasting action through contact representations from first person video},
  author={Dessalene, Eadom and Devaraj, Chinmaya and Maynord, Michael and Ferm{\"u}ller, Cornelia and Aloimonos, Yiannis},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume={45},
  number={6},
  pages={6703--6714},
  year={2021},
  publisher={IEEE}
}

@article{huang2015using,
  title={Using gaze patterns to predict task intent in collaboration},
  author={Huang, Chien-Ming and Andrist, Sean and Saupp{\'e}, Allison and Mutlu, Bilge},
  journal={Frontiers in psychology},
  volume={6},
  pages={1049},
  year={2015},
  publisher={Frontiers Media SA}
}

@inproceedings{zhang2022can,
  title={Can gaze inform egocentric action recognition?},
  author={Zhang, Zehua and Crandall, David and Proulx, Michael and Talathi, Sachin and Sharma, Abhishek},
  booktitle={2022 Symposium on Eye Tracking Research and Applications},
  pages={1--7},
  year={2022}
}

@inproceedings{huang2018predicting,
  title={Predicting gaze in egocentric video by learning task-dependent attention transition},
  author={Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Sato, Yoichi},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={754--769},
  year={2018}
}

@article{huang2020mutual,
  title={Mutual context network for jointly estimating egocentric gaze and action},
  author={Huang, Yifei and Cai, Minjie and Li, Zhenqiang and Lu, Feng and Sato, Yoichi},
  journal={IEEE Transactions on Image Processing},
  volume={29},
  pages={7795--7806},
  year={2020},
  publisher={IEEE}
}

@article{huang2020ego,
  title={An ego-vision system for discovering human joint attention},
  author={Huang, Yifei and Cai, Minjie and Sato, Yoichi},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={50},
  number={4},
  pages={306--316},
  year={2020},
  publisher={IEEE}
}

@article{lai2024eye,
  title={In the eye of transformer: Global--local correlation for egocentric gaze estimation and beyond},
  author={Lai, Bolin and Liu, Miao and Ryan, Fiona and Rehg, James M},
  journal={International Journal of Computer Vision},
  volume={132},
  number={3},
  pages={854--871},
  year={2024},
  publisher={Springer}
}

@inproceedings{li2013learning,
  title={Learning to predict gaze in egocentric video},
  author={Li, Yin and Fathi, Alireza and Rehg, James M},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={3216--3223},
  year={2013}
}

@inproceedings{li2018eye,
  title={In the eye of beholder: Joint learning of gaze and actions in first person video},
  author={Li, Yin and Liu, Miao and Rehg, James M},
  booktitle={Proceedings of the European conference on computer vision (ECCV)},
  pages={619--635},
  year={2018}
}

@inproceedings{min2021integrating,
  title={Integrating human gaze into attention for egocentric activity recognition},
  author={Min, Kyle and Corso, Jason J},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={1069--1078},
  year={2021}
}

@inproceedings{xu2015gaze,
  title={Gaze-enabled egocentric video summarization via constrained submodular maximization},
  author={Xu, Jia and Mukherjee, Lopamudra and Li, Yin and Warner, Jamieson and Rehg, James M and Singh, Vikas},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2235--2244},
  year={2015}
}

@inproceedings{zheng2022gimo,
  title={Gimo: Gaze-informed human motion prediction in context},
  author={Zheng, Yang and Yang, Yanchao and Mo, Kaichun and Li, Jiaman and Yu, Tao and Liu, Yebin and Liu, C Karen and Guibas, Leonidas J},
  booktitle={European Conference on Computer Vision},
  pages={676--694},
  year={2022},
  organization={Springer}
}

@inproceedings{chang2020procedure,
  title={Procedure planning in instructional videos},
  author={Chang, Chien-Yi and Huang, De-An and Xu, Danfei and Adeli, Ehsan and Fei-Fei, Li and Niebles, Juan Carlos},
  booktitle={European Conference on Computer Vision},
  pages={334--350},
  year={2020},
  organization={Springer}
}

@article{niu2024schema,
  title={Schema: State changes matter for procedure planning in instructional videos},
  author={Niu, Yulei and Guo, Wenliang and Chen, Long and Lin, Xudong and Chang, Shih-Fu},
  journal={arXiv preprint arXiv:2403.01599},
  year={2024}
}

@inproceedings{wang2023event,
  title={Event-guided procedure planning from instructional videos with text supervision},
  author={Wang, An-Lan and Lin, Kun-Yu and Du, Jia-Run and Meng, Jingke and Zheng, Wei-Shi},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={13565--13575},
  year={2023}
}

@inproceedings{zhao2022p3iv,
  title={P3iv: Probabilistic procedure planning from instructional videos with weak supervision},
  author={Zhao, He and Hadji, Isma and Dvornik, Nikita and Derpanis, Konstantinos G and Wildes, Richard P and Jepson, Allan D},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={2938--2948},
  year={2022}
}

@inproceedings{islam2024propose,
  title={Propose, assess, search: Harnessing llms for goal-oriented planning in instructional videos},
  author={Islam, Md Mohaiminul and Nagarajan, Tushar and Wang, Huiyu and Chu, Fu-Jen and Kitani, Kris and Bertasius, Gedas and Yang, Xitong},
  booktitle={European Conference on Computer Vision},
  pages={436--452},
  year={2024},
  organization={Springer}
}

@inproceedings{liu2023language,
  title={A language-first approach for procedure planning},
  author={Liu, Jiateng and Li, Sha and Wang, Zhenhailong and Li, Manling and Ji, Heng},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={1941--1954},
  year={2023}
}

@inproceedings{nagasinghe2024not,
  title={Why not use your textbook? knowledge-enhanced procedure planning of instructional videos},
  author={Nagasinghe, Kumaranage Ravindu Yasas and Zhou, Honglu and Gunawardhana, Malitha and Min, Martin Renqiang and Harari, Daniel and Khan, Muhammad Haris},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={18816--18826},
  year={2024}
}

@inproceedings{wang2023pdpp,
  title={Pdpp: Projected diffusion for procedure planning in instructional videos},
  author={Wang, Hanlin and Wu, Yilu and Guo, Sheng and Wang, Limin},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14836--14845},
  year={2023}
}

@inproceedings{zare2024rap,
  title={RAP: Retrieval-Augmented planner for adaptive procedure planning in instructional videos},
  author={Zare, Ali and Niu, Yulei and Ayyubi, Hammad and Chang, Shih-fu},
  booktitle={European Conference on Computer Vision},
  pages={410--426},
  year={2024},
  organization={Springer}
}

@inproceedings{patel2023pretrained,
  title={Pretrained language models as visual planners for human assistance},
  author={Patel, Dhruvesh and Eghbalzadeh, Hamid and Kamra, Nitin and Iuzzolino, Michael Louis and Jain, Unnat and Desai, Ruta},
  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
  pages={15302--15314},
  year={2023}
}

@inproceedings{tom2023reading,
  title={Reading between the lanes: Text videoqa on the road},
  author={Tom, George and Mathew, Minesh and Garcia-Bordils, Sergi and Karatzas, Dimosthenis and Jawahar, CV},
  booktitle={International Conference on Document Analysis and Recognition},
  pages={137--154},
  year={2023},
  organization={Springer}
}

@article{li2022pp,
  title={PP-OCRv3: More attempts for the improvement of ultra lightweight OCR system},
  author={Li, Chenxia and Liu, Weiwei and Guo, Ruoyu and Yin, Xiaoting and Jiang, Kaitao and Du, Yongkun and Du, Yuning and Zhu, Lingfeng and Lai, Baohua and Hu, Xiaoguang and others},
  journal={arXiv preprint arXiv:2206.03001},
  year={2022}
}

@inproceedings{zhou2025egotextvqa,
  title={Egotextvqa: Towards egocentric scene-text aware video question answering},
  author={Zhou, Sheng and Xiao, Junbin and Li, Qingyun and Li, Yicong and Yang, Xun and Guo, Dan and Wang, Meng and Chua, Tat-Seng and Yao, Angela},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={3363--3373},
  year={2025}
}


@article{li2025clivis,
  title={CLiViS: Unleashing Cognitive Map through Linguistic-Visual Synergy for Embodied Visual Reasoning},
  author={Li, Kailing and Xu, Qi'ao and Qian, Tianwen and Fu, Yuqian and Jiao, Yang and Wang, Xiaoling},
  journal={arXiv preprint arXiv:2506.17629},
  year={2025}
}

@article{tian2025ego,
  title={Ego-R1: Chain-of-Tool-Thought for Ultra-Long Egocentric Video Reasoning},
  author={Tian, Shulin and Wang, Ruiqi and Guo, Hongming and Wu, Penghao and Dong, Yuhao and Wang, Xiuying and Yang, Jingkang and Zhang, Hao and Zhu, Hongyuan and Liu, Ziwei},
  journal={arXiv preprint arXiv:2506.13654},
  year={2025}
}

@article{vinod2025egovlm,
  title={EgoVLM: Policy Optimization for Egocentric Video Understanding},
  author={Vinod, Ashwin and Pandit, Shrey and Vavre, Aditya and Liu, Linshen},
  journal={arXiv preprint arXiv:2506.03097},
  year={2025}
}

@article{lee2025towards,
  title={Towards Comprehensive Scene Understanding: Integrating First and Third-Person Views for LVLMs},
  author={Lee, Insu and Park, Wooje and Jang, Jaeyun and Noh, Minyoung and Shim, Kyuhong and Shim, Byonghyo},
  journal={arXiv preprint arXiv:2505.21955},
  year={2025}
}

@article{peirone2025hier,
  title={Hier-EgoPack: Hierarchical Egocentric Video Understanding with Diverse Task Perspectives},
  author={Peirone, Simone Alberto and Pistilli, Francesca and Alliegro, Antonio and Tommasi, Tatiana and Averta, Giuseppe},
  journal={arXiv preprint arXiv:2502.02487},
  year={2025}
}


@article{liu2023egocentric,
  title={Egocentric planning for scalable embodied task achievement},
  author={Liu, Xiatoian and Palacios, Hector and Muise, Christian},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={54586--54613},
  year={2023}
}

@article{chen2025planning,
  title={Planning with Reasoning using Vision Language World Model},
  author={Chen, Delong and Moutakanni, Theo and Chung, Willy and Bang, Yejin and Ji, Ziwei and Bolourchi, Allen and Fung, Pascale},
  journal={arXiv preprint arXiv:2509.02722},
  year={2025}
}

@article{cheng2024videollama,
  title={Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@article{lin2023video,
  title={Video-llava: Learning united visual representation by alignment before projection},
  author={Lin, Bin and Ye, Yang and Zhu, Bin and Cui, Jiaxi and Ning, Munan and Jin, Peng and Yuan, Li},
  journal={arXiv preprint arXiv:2311.10122},
  year={2023}
}

@article{li2024llava,
  title={Llava-onevision: Easy visual task transfer},
  author={Li, Bo and Zhang, Yuanhan and Guo, Dong and Zhang, Renrui and Li, Feng and Zhang, Hao and Zhang, Kaichen and Zhang, Peiyuan and Li, Yanwei and Liu, Ziwei and others},
  journal={arXiv preprint arXiv:2408.03326},
  year={2024}
}

@inproceedings{yang2021just,
  title={Just ask: Learning to answer questions from millions of narrated videos},
  author={Yang, Antoine and Miech, Antoine and Sivic, Josef and Laptev, Ivan and Schmid, Cordelia},
  booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
  pages={1686--1697},
  year={2021}
}
@article{chen2023egoplan,
  title={Egoplan-bench: Benchmarking multimodal large language models for human-level planning},
  author={Chen, Yi and Ge, Yuying and Ge, Yixiao and Ding, Mingyu and Li, Bohao and Wang, Rui and Xu, Ruifeng and Shan, Ying and Liu, Xihui},
  journal={arXiv preprint arXiv:2312.06722},
  year={2023}
}

@inproceedings{cheng2024egothink,
  title={Egothink: Evaluating first-person perspective thinking capability of vision-language models},
  author={Cheng, Sijie and Guo, Zhicheng and Wu, Jingwen and Fang, Kechen and Li, Peng and Liu, Huaping and Liu, Yang},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14291--14302},
  year={2024}
}

@inproceedings{bansal2024united,
  title={United we stand, divided we fall: Unitygraph for unsupervised procedure learning from videos},
  author={Bansal, Siddhant and Arora, Chetan and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision},
  pages={6509--6519},
  year={2024}
}

@article{bambach2015survey,
  title={A survey on recent advances of computer vision algorithms for egocentric video},
  author={Bambach, Sven},
  journal={arXiv preprint arXiv:1501.02825},
  year={2015}
}
@article{nguyen2016recognition,
  title={Recognition of activities of daily living with egocentric vision: A review},
  author={Nguyen, Thi-Hoa-Cuc and Nebel, Jean-Christophe and Florez-Revuelta, Francisco},
  journal={Sensors},
  volume={16},
  number={1},
  pages={72},
  year={2016},
  publisher={MDPI}
}
@article{del2016summarization,
  title={Summarization of egocentric videos: A comprehensive survey},
  author={Del Molino, Ana Garcia and Tan, Cheston and Lim, Joo-Hwee and Tan, Ah-Hwee},
  journal={IEEE Transactions on Human-Machine Systems},
  volume={47},
  number={1},
  pages={65--76},
  year={2016},
  publisher={IEEE}
}

@inproceedings{hamid2017survey,
  title={A survey of activity recognition in egocentric lifelogging datasets},
  author={Hamid, Aksasse and Brahim, Aksasse and Mohammed, Ouanan and others},
  booktitle={2017 International Conference on Wireless Technologies, Embedded and Intelligent Systems (WITS)},
  pages={1--8},
  year={2017},
  organization={IEEE}
}

@article{rodin2021predicting,
  title={Predicting the future from first person (egocentric) vision: A survey},
  author={Rodin, Ivan and Furnari, Antonino and Mavroeidis, Dimitrios and Farinella, Giovanni Maria},
  journal={Computer Vision and Image Understanding},
  volume={211},
  pages={103252},
  year={2021},
  publisher={Elsevier}
}

@inproceedings{chen2021survey,
  title={A survey on deep-learning methods for pedestrian behavior prediction from the egocentric view},
  author={Chen, Tina and Tian, Renran},
  booktitle={2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
  pages={1898--1905},
  year={2021},
  organization={IEEE}
}

@article{nunez2022egocentric,
  title={Egocentric vision-based action recognition: A survey},
  author={N{\'u}{\~n}ez-Marcos, Adri{\'a}n and Azkune, Gorka and Arganda-Carreras, Ignacio},
  journal={Neurocomputing},
  volume={472},
  pages={175--197},
  year={2022},
  publisher={Elsevier}
}

@article{su2024care,
  title={CaRe-Ego: Contact-aware Relationship Modeling for Egocentric Interactive Hand-object Segmentation},
  author={Su, Yuejiao and Wang, Yi and Chau, Lap-Pui},
  journal={arXiv preprint arXiv:2407.05576},
  year={2024}
}

@inproceedings{su2025annexe,
  title={ANNEXE: Unified Analyzing, Answering, and Pixel Grounding for Egocentric Interaction},
  author={Su, Yuejiao and Wang, Yi and Hu, Qiongyang and Yang, Chuang and Chau, Lap-Pui},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={9027--9038},
  year={2025}
}
@article{deng2025egocentric,
  title={Egocentric Human-Object Interaction Detection: A New Benchmark and Method},
  author={Deng, Kunyuan and Wang, Yi and Chau, Lap-Pui},
  journal={arXiv preprint arXiv:2506.14189},
  year={2025}
}
