@article{wu2025more,
  title={When More is Less: Understanding Chain-of-Thought Length in LLMs},
  author={Wu, Yuyang and Wang, Yifei and Du, Tianqi and Jegelka, Stefanie and Wang, Yisen},
  journal={arXiv preprint arXiv:2502.07266},
  year={2025}
}
@article{chen2025towards,
  title={Towards reasoning era: A survey of long chain-of-thought for reasoning large language models},
  author={Chen, Qiguang and Qin, Libo and Liu, Jinhao and Peng, Dengyun and Guan, Jiannan and Wang, Peng and Hu, Mengkang and Zhou, Yuhang and Gao, Te and Che, Wangxiang},
  journal={arXiv preprint arXiv:2503.09567},
  year={2025}
}
@article{sui2025stop,
  title={Stop overthinking: A survey on efficient reasoning for large language models},
  author={Sui, Yang and Chuang, Yu-Neng and Wang, Guanchu and Zhang, Jiamu and Zhang, Tianyi and Yuan, Jiayi and Liu, Hongyi and Wen, Andrew and Chen, Hanjie and Hu, Xia and others},
  journal={arXiv preprint arXiv:2503.16419},
  year={2025}
}
@article{hendrycksmath2021,
  title={Measuring Mathematical Problem Solving With the MATH Dataset},
  author={Dan Hendrycks and Collin Burns and Saurav Kadavath and Akul Arora and Steven Basart and Eric Tang and Dawn Song and Jacob Steinhardt},
  journal={NeurIPS},
  year={2021}
}
@article{rafailov2023direct,
  title={Direct preference optimization: Your language model is secretly a reward model},
  author={Rafailov, Rafael and Sharma, Archit and Mitchell, Eric and Manning, Christopher D and Ermon, Stefano and Finn, Chelsea},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={53728--53741},
  year={2023}
}
@article{gao2024omni,
  title={Omni-math: A universal olympiad level mathematic benchmark for large language models},
  author={Gao, Bofei and Song, Feifan and Yang, Zhe and Cai, Zefan and Miao, Yibo and Dong, Qingxiu and Li, Lei and Ma, Chenghao and Chen, Liang and Xu, Runxin and others},
  journal={arXiv preprint arXiv:2410.07985},
  year={2024}
}
@article{guo2025deepseek,
  title={Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning},
  author={Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and others},
  journal={arXiv preprint arXiv:2501.12948},
  year={2025}
}
@article{team2025kimi,
  title={Kimi k1. 5: Scaling reinforcement learning with llms},
  author={Team, Kimi and Du, Angang and Gao, Bofei and Xing, Bowei and Jiang, Changjiu and Chen, Cheng and Li, Cheng and Xiao, Chenjun and Du, Chenzhuang and Liao, Chonghua and others},
  journal={arXiv preprint arXiv:2501.12599},
  year={2025}
}
@misc{reduce_overthinking_2025,
  author       = {NovaSky Team},
  title        = {Think Less, Achieve More: Cut Reasoning Costs by 50% Without Sacrificing Accuracy},
  howpublished = {https://novasky-ai.github.io/posts/reduce-overthinking},
  note         = {Accessed: 2025-01-23},
  year         = {2025}
}
@article{chen2024not,
  title={Do not think that much for 2+ 3=? on the overthinking of o1-like llms},
  author={Chen, Xingyu and Xu, Jiahao and Liang, Tian and He, Zhiwei and Pang, Jianhui and Yu, Dian and Song, Linfeng and Liu, Qiuzhi and Zhou, Mengfei and Zhang, Zhuosheng and others},
  journal={arXiv preprint arXiv:2412.21187},
  year={2024}
}

@article{xia2025tokenskip,
  title={Tokenskip: Controllable chain-of-thought compression in llms},
  author={Xia, Heming and Li, Yongqi and Leong, Chak Tou and Wang, Wenjie and Li, Wenjie},
  journal={arXiv preprint arXiv:2502.12067},
  year={2025}
}
@inproceedings{kang2025c3ot,
  title={C3ot: Generating shorter chain-of-thought without compromising effectiveness},
  author={Kang, Yu and Sun, Xianghui and Chen, Liangyu and Zou, Wei},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={23},
  pages={24312--24320},
  year={2025}
}
@article{jaech2024openai,
  title={Openai o1 system card},
  author={Jaech, Aaron and Kalai, Adam and Lerer, Adam and Richardson, Adam and El-Kishky, Ahmed and Low, Aiden and Helyar, Alec and Madry, Aleksander and Beutel, Alex and Carney, Alex and others},
  journal={arXiv preprint arXiv:2412.16720},
  year={2024}
}
@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}
@article{shao2024deepseekmath,
  title={Deepseekmath: Pushing the limits of mathematical reasoning in open language models},
  author={Shao, Zhihong and Wang, Peiyi and Zhu, Qihao and Xu, Runxin and Song, Junxiao and Bi, Xiao and Zhang, Haowei and Zhang, Mingchuan and Li, YK and Wu, Y and others},
  journal={arXiv preprint arXiv:2402.03300},
  year={2024}
}
@article{aggarwal2025l1,
  title={L1: Controlling how long a reasoning model thinks with reinforcement learning},
  author={Aggarwal, Pranjal and Welleck, Sean},
  journal={arXiv preprint arXiv:2503.04697},
  year={2025}
}
@article{luo2025o1,
  title={O1-Pruner: Length-Harmonizing Fine-Tuning for O1-Like Reasoning Pruning},
  author={Luo, Haotian and Shen, Li and He, Haiying and Wang, Yibo and Liu, Shiwei and Li, Wei and Tan, Naiqiang and Cao, Xiaochun and Tao, Dacheng},
  journal={arXiv preprint arXiv:2501.12570},
  year={2025}
}
@article{munkhbat2025self,
  title={Self-training elicits concise reasoning in large language models},
  author={Munkhbat, Tergel and Ho, Namgyu and Kim, Seo Hyun and Yang, Yongjin and Kim, Yujin and Yun, Se-Young},
  journal={arXiv preprint arXiv:2502.20122},
  year={2025}
}
@article{han2024token,
  title={Token-budget-aware llm reasoning},
  author={Han, Tingxu and Wang, Zhenting and Fang, Chunrong and Zhao, Shiyu and Ma, Shiqing and Chen, Zhenyu},
  journal={arXiv preprint arXiv:2412.18547},
  year={2024}
}
@article{shen2025dast,
  title={Dast: Difficulty-adaptive slow-thinking for large reasoning models},
  author={Shen, Yi and Zhang, Jian and Huang, Jieyun and Shi, Shuming and Zhang, Wenjing and Yan, Jiangze and Wang, Ning and Wang, Kai and Lian, Shiguo},
  journal={arXiv preprint arXiv:2503.04472},
  year={2025}
}


@article{achiam2023gpt,
  title={Gpt-4 technical report},
  author={Achiam, Josh and Adler, Steven and Agarwal, Sandhini and Ahmad, Lama and Akkaya, Ilge and Aleman, Florencia Leoni and Almeida, Diogo and Altenschmidt, Janko and Altman, Sam and Anadkat, Shyamal and others},
  journal={arXiv preprint arXiv:2303.08774},
  year={2023}
}
@article{liu2024can,
  title={Can language models learn to skip steps?},
  author={Liu, Tengxiao and Guo, Qipeng and Hu, Xiangkun and Jiayang, Cheng and Zhang, Yue and Qiu, Xipeng and Zhang, Zheng},
  journal={arXiv preprint arXiv:2411.01855},
  year={2024}
}
@article{gandhi2025cognitive,
  title={Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars},
  author={Gandhi, Kanishk and Chakravarthy, Ayush and Singh, Anikait and Lile, Nathan and Goodman, Noah D},
  journal={arXiv preprint arXiv:2503.01307},
  year={2025}
}
@article{cuadron2025danger,
  title={The Danger of Overthinking: Examining the Reasoning-Action Dilemma in Agentic Tasks},
  author={Cuadron, Alejandro and Li, Dacheng and Ma, Wenjie and Wang, Xingyao and Wang, Yichuan and Zhuang, Siyuan and Liu, Shu and Schroeder, Luis Gaspar and Xia, Tian and Mao, Huanzhi and others},
  journal={arXiv preprint arXiv:2502.08235},
  year={2025}
}
@article{zhao2023survey,
  title={A survey of large language models},
  author={Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and others},
  journal={arXiv preprint arXiv:2303.18223},
  volume={1},
  number={2},
  year={2023}
}
@article{hadi2023survey,
  title={A survey on large language models: Applications, challenges, limitations, and practical usage},
  author={Hadi, Muhammad Usman and Qureshi, Rizwan and Shah, Abbas and Irfan, Muhammad and Zafar, Anas and Shaikh, Muhammad Bilal and Akhtar, Naveed and Wu, Jia and Mirjalili, Seyedali and others},
  journal={Authorea Preprints},
  volume={3},
  year={2023}
}
@article{jiang2024survey,
  title={A survey on large language models for code generation},
  author={Jiang, Juyong and Wang, Fan and Shen, Jiasi and Kim, Sungju and Kim, Sunghun},
  journal={arXiv preprint arXiv:2406.00515},
  year={2024}
}
@article{ahn2024large,
  title={Large language models for mathematical reasoning: Progresses and challenges},
  author={Ahn, Janice and Verma, Rishu and Lou, Renze and Liu, Di and Zhang, Rui and Yin, Wenpeng},
  journal={arXiv preprint arXiv:2402.00157},
  year={2024}
}
@article{arora2025training,
  title={Training Language Models to Reason Efficiently},
  author={Arora, Daman and Zanette, Andrea},
  journal={arXiv preprint arXiv:2502.04463},
  year={2025}
}
@article{liu2025efficient,
  title={Efficient Inference for Large Reasoning Models: A Survey},
  author={Liu, Yue and Wu, Jiaying and He, Yufei and Gao, Hongcheng and Chen, Hongyu and Bi, Baolong and Zhang, Jiaheng and Huang, Zhiqi and Hooi, Bryan},
  journal={arXiv preprint arXiv:2503.23077},
  year={2025}
}
@article{ding2023efficiency,
  title={The efficiency spectrum of large language models: An algorithmic survey},
  author={Ding, Tianyu and Chen, Tianyi and Zhu, Haidong and Jiang, Jiachen and Zhong, Yiqi and Zhou, Jinxin and Wang, Guangzhi and Zhu, Zhihui and Zharkov, Ilya and Liang, Luming},
  journal={arXiv preprint arXiv:2312.00678},
  year={2023}
}
@inproceedings{
xiang2024badchain,
title={BadChain: Backdoor Chain-of-Thought Prompting for Large Language Models},
author={Zhen Xiang and Fengqing Jiang and Zidi Xiong and Bhaskar Ramasubramanian and Radha Poovendran and Bo Li},
booktitle={NeurIPS 2023 Workshop on Backdoors in Deep Learning - The Good, the Bad, and the Ugly},
year={2024},
url={https://openreview.net/forum?id=S4cYxINzjp}
}
@article{hu2025reinforce++,
  title={REINFORCE++: A Simple and Efficient Approach for Aligning Large Language Models},
  author={Hu, Jian},
  journal={arXiv preprint arXiv:2501.03262},
  year={2025}
}
@article{wei2022chain,
  title={Chain-of-thought prompting elicits reasoning in large language models},
  author={Wei, Jason and Wang, Xuezhi and Schuurmans, Dale and Bosma, Maarten and Xia, Fei and Chi, Ed and Le, Quoc V and Zhou, Denny and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={24824--24837},
  year={2022}
}
@inproceedings{gao2023pal,
  title={Pal: Program-aided language models},
  author={Gao, Luyu and Madaan, Aman and Zhou, Shuyan and Alon, Uri and Liu, Pengfei and Yang, Yiming and Callan, Jamie and Neubig, Graham},
  booktitle={International Conference on Machine Learning},
  pages={10764--10799},
  year={2023},
  organization={PMLR}
}
@article{zhou2022least,
  title={Least-to-most prompting enables complex reasoning in large language models},
  author={Zhou, Denny and Sch{\"a}rli, Nathanael and Hou, Le and Wei, Jason and Scales, Nathan and Wang, Xuezhi and Schuurmans, Dale and Cui, Claire and Bousquet, Olivier and Le, Quoc and others},
  journal={arXiv preprint arXiv:2205.10625},
  year={2022}
}
@inproceedings{lightman2023let,
  title={Let's verify step by step},
  author={Lightman, Hunter and Kosaraju, Vineet and Burda, Yuri and Edwards, Harrison and Baker, Bowen and Lee, Teddy and Leike, Jan and Schulman, John and Sutskever, Ilya and Cobbe, Karl},
  booktitle={The Twelfth International Conference on Learning Representations},
  year={2023}
}
@article{aytes2025sketch,
  title={Sketch-of-thought: Efficient llm reasoning with adaptive cognitive-inspired sketching},
  author={Aytes, Simon A and Baek, Jinheon and Hwang, Sung Ju},
  journal={arXiv preprint arXiv:2503.05179},
  year={2025}
}
@article{xu2025chain,
  title={Chain of draft: Thinking faster by writing less},
  author={Xu, Silei and Xie, Wenhao and Zhao, Lingxiao and He, Pengcheng},
  journal={arXiv preprint arXiv:2502.18600},
  year={2025}
}
@article{yang2025think,
  title={Think When You Need: Self-Adaptive Chain-of-Thought Learning},
  author={Yang, Junjie and Lin, Ke and Yu, Xing},
  journal={arXiv preprint arXiv:2504.03234},
  year={2025}
}
@article{lee2025well,
  title={How Well do LLMs Compress Their Own Chain-of-Thought? A Token Complexity Approach},
  author={Lee, Ayeong and Che, Ethan and Peng, Tianyi},
  journal={arXiv preprint arXiv:2503.01141},
  year={2025}
}
@article{yu2025dapo,
  title={Dapo: An open-source llm reinforcement learning system at scale},
  author={Yu, Qiying and Zhang, Zheng and Zhu, Ruofei and Yuan, Yufeng and Zuo, Xiaochen and Yue, Yu and Fan, Tiantian and Liu, Gaohong and Liu, Lingjun and Liu, Xin and others},
  journal={arXiv preprint arXiv:2503.14476},
  year={2025}
}
@article{jain2024livecodebench,
  author = {Naman Jain and King Han and Alex Gu and Wen-Ding Li and Fanjia Yan and Tianjun Zhang and Sida Wang and Armando Solar-Lezama and Koushik Sen and Ion Stoica},
  title     = {LiveCodeBench: Holistic and Contamination Free Evaluation of Large Language Models for Code},
  year      = {2024},
  journal   = {arXiv preprint},
}
@inproceedings{
zhang2025flexcad,
title={Flex{CAD}: Unified and Versatile Controllable {CAD} Generation with Fine-tuned Large Language Models},
author={Zhanwei Zhang and Shizhao Sun and Wenxiao Wang and Deng Cai and Jiang Bian},
booktitle={The Thirteenth International Conference on Learning Representations},
year={2025},
}

@article{zhang2025geocad,
  title={GeoCAD: Local Geometry-Controllable CAD Generation},
  author={Zhang, Zhanwei and Liu, Kaiyuan and Liu, Junjie and Wang, Wenxiao and Lin, Binbin and Xie, Liang and Shen, Chen and Cai, Deng},
  journal={arXiv preprint arXiv:2506.10337},
  year={2025}
}