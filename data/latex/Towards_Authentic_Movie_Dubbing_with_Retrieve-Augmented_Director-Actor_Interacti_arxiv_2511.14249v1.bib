@book{em:86,
  editor  = "Engelmore, Robert and Morgan, Anthony",
  title   = "Blackboard Systems",
  year    = 1986,
  address = "Reading, Mass.",
  publisher = "Addison-Wesley",
}

@inproceedings{c:83,
  author  = "Clancey, William J.",
  year    = 1983,
  title   = "{Communication, Simulation, and Intelligent
Agents: Implications of Personal Intelligent Machines
for Medical Education}",
  booktitle="Proceedings of the Eighth International Joint Conference on Artificial Intelligence {(IJCAI-83)}", 
  pages   = "556-560",
  address = "Menlo Park, Calif",
  publisher = "{IJCAI Organization}",
}
@inproceedings{c:84,
  author  = "Clancey, William J.",
  year    = 1984,
  title   = "{Classification Problem Solving}",
  booktitle = "Proceedings of the Fourth National 
              Conference on Artificial Intelligence",
  pages   = "45-54",
  address = "Menlo Park, Calif.",
  publisher="AAAI Press",
}
@article{r:80,
  author = {Robinson, Arthur L.},
  title = {New Ways to Make Microcircuits Smaller},
  volume = {208},
  number = {4447},
  pages = {1019--1022},
  year = {1980},
  doi = {10.1126/science.208.4447.1019},
  publisher = {American Association for the Advancement of Science},
  issn = {0036-8075},
  URL = {https://science.sciencemag.org/content/208/4447/1019},
  eprint = {https://science.sciencemag.org/content/208/4447/1019.full.pdf},
  journal = {Science},
}
@article{r:80x,
  author  = "Robinson, Arthur L.",
  year    = 1980,
  title   = "{New Ways to Make Microcircuits Smaller---Duplicate Entry}",
  journal = "Science",
  volume  =  208,
  pages   = "1019-1026",
}
@article{hcr:83,
title = {Strategic explanations for a diagnostic consultation system},
journal = {International Journal of Man-Machine Studies},
volume = {20},
number = {1},
pages = {3-19},
year = {1984},
issn = {0020-7373},
doi = {https://doi.org/10.1016/S0020-7373(84)80003-6},
url = {https://www.sciencedirect.com/science/article/pii/S0020737384800036},
author = {Diane Warner Hasling and William J. Clancey and Glenn Rennels},
abstract = {This article examines the problem of automatte explanation of reasoning, especially as it relates to expert systems. By explanation we mean the ability of a program to discuss what it is doing in some understandable way. We first present a general framework in which to view explanation and review some of the research done in this area. We then focus on the explanation system for NEOMYCIN, a medical consultation program. A consultation program interactively helps a user to solve a problem. Our goal is to have NEOMYCIN explain its problem-solving strategies. An explanation of strategy describes the plan the program is using to reach a solution. Such an explanation is usually concrete, referring to aspects of the current problem situation. Abstract explanations articulate a general principle, which can be applied in different situations; such explanations are useful in teaching and in explaining by analogy. We describe the aspects of NEOMYCIN that make abstract strategic explanations possible—the representation of strategic knowledge explicitly and separately from domain knowledge— and demonstrate how this representation can be used to generate explanations.}
}
@article{hcrt:83,
  author  = "Hasling, Diane Warner and Clancey, William J. and Rennels, Glenn R. and Test, Thomas",
  year    = 1983,
  title   = "{Strategic Explanations in Consultation---Duplicate}",
  journal = "The International Journal of Man-Machine Studies",
  volume  = 20,
  number  = 1,
  pages   = "3-19",
}
@techreport{r:86,
  author  = "Rice, James",
  year    = 1986,
  title   = "{Poligon: A System for Parallel Problem Solving}",
  type    = "Technical Report", 
  number  = "KSL-86-19", 
  institution = "Dept.\ of Computer Science, Stanford Univ.",
}
@phdthesis{c:79,
  author  = "Clancey, William J.",
  year    = 1979,
  title   = "{Transfer of Rule-Based Expertise
through a Tutorial Dialogue}",
  type    = "{Ph.D.} diss.",
  school  = "Dept.\ of Computer Science, Stanford Univ.",
  address = "Stanford, Calif.",
}
@unpublished{c:21,
  author  = "Clancey, William J.",
  title   = "{The Engineering of Qualitative Models}",
  year    = 2021,
  note    = "Forthcoming",
}
@misc{c:22,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2017},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}
@misc{c:23,
  title        = "Pluto: The 'Other' Red Planet",
  author       = "{NASA}",
  howpublished = "\url{https://www.nasa.gov/nh/pluto-the-other-red-planet}",
  year         = 2015,
  note         = "Accessed: 2018-12-06"
}

@inproceedings{V2CNET,
  title={V2C: Visual voice cloning},
  author={Chen, Qi and Tan, Mingkui and Qi, Yuankai and Zhou, Jiaqiu and Li, Yuanqing and Wu, Qi},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={21242--21251},
  year={2022}
}

@article{fs2,
  title={Fastspeech 2: Fast and high-quality end-to-end text to speech},
  author={Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2006.04558},
  year={2020}
}

@article{maskgct,
  title={Maskgct: Zero-shot text-to-speech with masked generative codec transformer},
  author={Wang, Yuancheng and Zhan, Haoyue and Liu, Liwei and Zeng, Ruihong and Guo, Haotian and Zheng, Jiachen and Zhang, Qiang and Zhang, Xueyao and Zhang, Shunsi and Wu, Zhizheng},
  journal={arXiv preprint arXiv:2409.00750},
  year={2024}
}

@inproceedings{speaker2dubber,
  title={From speaker to dubber: movie dubbing with prosody and duration consistency learning},
  author={Zhang, Zhedong and Li, Liang and Cong, Gaoxiang and Yin, Haibing and Gao, Yuhan and Yan, Chenggang and Hengel, Anton van den and Qi, Yuankai},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={7523--7532},
  year={2024}
}

@inproceedings{zhang2024speaker,
  title={From Speaker to Dubber: Movie Dubbing with Prosody and Duration Consistency Learning},
  author={Zhang, Zhedong and Li, Liang and Cong, Gaoxiang and Haibing, YIN and Gao, Yuhan and Yan, Chenggang and van den Hengel, Anton and Qi, Yuankai},
  booktitle={ACM Multimedia 2024}
}
@article{cong2024styledubber,
  title={StyleDubber: Towards Multi-Scale Style Learning for Movie Dubbing},
  author={Cong, Gaoxiang and Qi, Yuankai and Li, Liang and Beheshti, Amin and Zhang, Zhedong and Hengel, Anton van den and Yang, Ming-Hsuan and Yan, Chenggang and Huang, Qingming},
  journal={arXiv preprint arXiv:2402.12636},
  year={2024}
}
@article{quasi_phoneme_level_2,
  title={Content-dependent fine-grained speaker embedding for zero-shot speaker adaptation in text-to-speech synthesis},
  author={Zhou, Yixuan and Song, Changhe and Li, Xiang and Zhang, Luwen and Wu, Zhiyong and Bian, Yanyao and Su, Dan and Meng, Helen},
  journal={arXiv preprint arXiv:2204.00990},
  year={2022}
}

@article{quasi_phoneme_level,
  title={Towards multi-scale style control for expressive speech synthesis},
  author={Li, Xiang and Song, Changhe and Li, Jingbei and Wu, Zhiyong and Jia, Jia and Meng, Helen},
  journal={arXiv preprint arXiv:2104.03521},
  year={2021}
}

@article{GAT,
  title={Graph attention networks},
  author={Veli{\v{c}}kovi{\'c}, Petar and Cucurull, Guillem and Casanova, Arantxa and Romero, Adriana and Lio, Pietro and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1710.10903},
  year={2017}
}

@inproceedings{whisper,
  title={Robust speech recognition via large-scale weak supervision},
  author={Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  booktitle={International conference on machine learning},
  pages={28492--28518},
  year={2023},
  organization={PMLR}
}
@article{zhao2024mcdubber,
  title={MCDubber: Multimodal Context-Aware Expressive Video Dubbing},
  author={Zhao, Yuan and Jia, Zhenqi and Liu, Rui and Hu, De and Bao, Feilong and Gao, Guanglai},
  journal={arXiv preprint arXiv:2408.11593},
  year={2024}
}
@article{GAT_MERC,
  title={Bi-stream graph learning based multimodal fusion for emotion recognition in conversation},
  author={Lu, Nannan and Han, Zhiyuan and Han, Min and Qian, Jiansheng},
  journal={Information Fusion},
  volume={106},
  pages={102272},
  year={2024},
  publisher={Elsevier}
}

@article{liu2024ma_emotion,
  title={Emotion-Aware Speech Self-Supervised Representation Learning with Intensity Knowledge},
  author={Liu, Rui and Ma, Zening},
  journal={arXiv preprint arXiv:2406.06646},
  year={2024}
}

@article{busso2008iemocap,
  title={IEMOCAP: Interactive emotional dyadic motion capture database},
  author={Busso, Carlos and Bulut, Murtaza and Lee, Chi-Chun and Kazemzadeh, Abe and Mower, Emily and Kim, Samuel and Chang, Jeannette N and Lee, Sungbok and Narayanan, Shrikanth S},
  journal={Language resources and evaluation},
  volume={42},
  pages={335--359},
  year={2008},
  publisher={Springer}
}
@article{baevski2020wav2vec,
  title={wav2vec 2.0: A framework for self-supervised learning of speech representations},
  author={Baevski, Alexei and Zhou, Yuhao and Mohamed, Abdelrahman and Auli, Michael},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={12449--12460},
  year={2020}
}
@misc{EERL,
  author={Hartmann, Jochen},
  title={Emotion-English-Roberta-large},
  year={2021},
  howpublished = {\url{https://huggingface.co/j-hartmann/emotion-english-roberta-large}},
}
@inproceedings{DFEW,
  title={Dfew: A large-scale database for recognizing dynamic facial expressions in the wild},
  author={Jiang, Xingxun and Zong, Yuan and Zheng, Wenming and Tang, Chuangao and Xia, Wanchuang and Lu, Cheng and Liu, Jiateng},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={2881--2889},
  year={2020}
}

@inproceedings{MAE-DFER,
  title={Mae-dfer: Efficient masked autoencoder for self-supervised dynamic facial expression recognition},
  author={Sun, Licai and Lian, Zheng and Liu, Bin and Tao, Jianhua},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6110--6121},
  year={2023}
}

@article{lei2023msstyletts,
  title={MSStyleTTS: Multi-scale style modeling with hierarchical context information for expressive speech synthesis},
  author={Lei, Shun and Zhou, Yixuan and Chen, Liyang and Wu, Zhiyong and Wu, Xixin and Kang, Shiyin and Meng, Helen},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2023},
  publisher={IEEE}
}

@inproceedings{xue2023m,
  title={M 2-ctts: End-to-end multi-scale multi-modal conversational text-to-speech synthesis},
  author={Xue, Jinlong and Deng, Yayue and Wang, Fengping and Li, Ya and Gao, Yingming and Tao, Jianhua and Sun, Jianqing and Liang, Jiaen},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}


@inproceedings{lei22c_interspeech,
  author={Shun Lei and Yixuan Zhou and Liyang Chen and Jiankun Hu and Zhiyong Wu and Shiyin Kang and Helen Meng},
  title={{Towards Multi-Scale Speaking Style Modelling with Hierarchical Context Information for Mandarin Speech Synthesis}},
  year=2022,
  booktitle={Proc. Interspeech 2022},
  pages={5523--5527},
  doi={10.21437/Interspeech.2022-11171},
  issn={2958-1796}
}

@inproceedings{li2022inferring,
  title={Inferring speaking styles from multi-modal conversational context by multi-scale relational graph convolutional networks},
  author={Li, Jingbei and Meng, Yi and Wu, Xixin and Wu, Zhiyong and Jia, Jia and Meng, Helen and Tian, Qiao and Wang, Yuping and Wang, Yuxuan},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={5811--5820},
  year={2022}
}

@inproceedings{chen2022unsupervised,
  title={Unsupervised multi-scale expressive speaking style modeling with hierarchical context information for audiobook speech synthesis},
  author={Chen, Xueyuan and Lei, Shun and Wu, Zhiyong and Xu, Dong and Zhao, Weifeng and Meng, Helen},
  booktitle={Proceedings of the 29th International Conference on Computational Linguistics},
  pages={7193--7202},
  year={2022}
}

@article{sahipjohn2024dubwise,
  title={DubWise: Video-Guided Speech Duration Control in Multimodal LLM-based Text-to-Speech for Dubbing},
  author={Sahipjohn, Neha and Gudmalwar, Ashishkumar and Shah, Nirmesh and Wasnik, Pankaj and Shah, Rajiv Ratn},
  journal={arXiv preprint arXiv:2406.08802},
  year={2024}
}

@article{MERC_infor,
  title={Adversarial alignment and graph fusion via information bottleneck for multimodal emotion recognition in conversations},
  author={Shou, Yuntao and Meng, Tao and Ai, Wei and Zhang, Fuchen and Yin, Nan and Li, Keqin},
  journal={Information Fusion},
  volume={112},
  pages={102590},
  year={2024},
  publisher={Elsevier}
}

@article{lei2022msemotts,
  title={Msemotts: Multi-scale emotion transfer, prediction, and control for emotional speech synthesis},
  author={Lei, Yi and Yang, Shan and Wang, Xinsheng and Xie, Lei},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={853--864},
  year={2022},
  publisher={IEEE}
}

@inproceedings{inoue2024hierarchical,
  title={Hierarchical Emotion Prediction and Control in Text-to-Speech Synthesis},
  author={Inoue, Sho and Zhou, Kun and Wang, Shuai and Li, Haizhou},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={10601--10605},
  year={2024},
  organization={IEEE}
}

@article{hu2021neural,
  title={Neural dubber: Dubbing for videos according to scripts},
  author={Hu, Chenxu and Tian, Qiao and Li, Tingle and Yuping, Wang and Wang, Yuxuan and Zhao, Hang},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={16582--16595},
  year={2021}
}

@inproceedings{3D-VD,
  author={Zhihan Yang and Shansong Liu and Xu Li and Haozhe Wu and Zhiyong Wu and Ying Shan and Jia Jia},
  title={{Prosody Modeling with 3D Visual Information for Expressive Video Dubbing}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={4863--4867},
  doi={10.21437/Interspeech.2023-289},
  issn={2308-457X}
}

@article{tacotron1,
  title={Tacotron: Towards end-to-end speech synthesis},
  author={Wang, Yuxuan and Skerry-Ryan, RJ and Stanton, Daisy and Wu, Yonghui and Weiss, Ron J and Jaitly, Navdeep and Yang, Zongheng and Xiao, Ying and Chen, Zhifeng and Bengio, Samy and others},
  journal={arXiv preprint arXiv:1703.10135},
  year={2017}
}
@inproceedings{tacotron2,
  title={Natural tts synthesis by conditioning wavenet on mel spectrogram predictions},
  author={Shen, Jonathan and Pang, Ruoming and Weiss, Ron J and Schuster, Mike and Jaitly, Navdeep and Yang, Zongheng and Chen, Zhifeng and Zhang, Yu and Wang, Yuxuan and Skerrv-Ryan, Rj and others},
  booktitle={2018 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={4779--4783},
  year={2018},
  organization={IEEE}
}
@article{fastspeech1,
  title={Fastspeech: Fast, robust and controllable text to speech},
  author={Ren, Yi and Ruan, Yangjun and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  journal={Advances in neural information processing systems},
  volume={32},
  year={2019}
}

@article{fastspeech2,
  title={Fastspeech 2: Fast and high-quality end-to-end text to speech},
  author={Ren, Yi and Hu, Chenxu and Tan, Xu and Qin, Tao and Zhao, Sheng and Zhao, Zhou and Liu, Tie-Yan},
  journal={arXiv preprint arXiv:2006.04558},
  year={2020}
}

@inproceedings{visualtts,
  title={Visualtts: Tts with accurate lip-speech synchronization for automatic voice over},
  author={Lu, Junchen and Sisman, Berrak and Liu, Rui and Zhang, Mingyang and Li, Haizhou},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8032--8036},
  year={2022},
  organization={IEEE}
}

@article{DSU-AVO,
  title={High-Quality Automatic Voice Over with Accurate Alignment: Supervision through Self-Supervised Discrete Speech Units},
  author={Lu, Junchen and Sisman, Berrak and Zhang, Mingyang and Li, Haizhou},
  journal={arXiv preprint arXiv:2306.17005},
  year={2023}
}

@inproceedings{VDTTS,
  title={More than words: In-the-wild visually-driven prosody for text-to-speech},
  author={Hassid, Michael and Ramanovich, Michelle Tadmor and Shillingford, Brendan and Wang, Miaosen and Jia, Ye and Remez, Tal},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={10587--10597},
  year={2022}
}

@inproceedings{HPMDubbing,
  title={Learning to dub movies via hierarchical prosody models},
  author={Cong, Gaoxiang and Li, Liang and Qi, Yuankai and Zha, Zheng-Jun and Wu, Qi and Wang, Wenyu and Jiang, Bin and Yang, Ming-Hsuan and Huang, Qingming},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={14687--14697},
  year={2023}
}

@inproceedings{masked_speech,
  author={Ya-Jie Zhang and Wei Song and Yanghao Yue and Zhengchen Zhang and Youzheng Wu and Xiaodong He},
  title={{MaskedSpeech: Context-aware Speech Synthesis with Masking Strategy}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={4803--4807},
  doi={10.21437/Interspeech.2023-782},
  issn={2308-457X}
}

@inproceedings{context_speech,
  title={Improving speech prosody of audiobook text-to-speech synthesis with acoustic and textual contexts},
  author={Xin, Detai and Adavanne, Sharath and Ang, Federico and Kulkarni, Ashish and Takamichi, Shinnosuke and Saruwatari, Hiroshi},
  booktitle={ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@article{hifigan,
  title={Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis},
  author={Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17022--17033},
  year={2020}
}

@inproceedings{flow_tts,
  title={Flow-TTS: A non-autoregressive network for text to speech based on flow},
  author={Miao, Chenfeng and Liang, Shuang and Chen, Minchuan and Ma, Jun and Wang, Shaojun and Xiao, Jing},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7209--7213},
  year={2020},
  organization={IEEE}
}

@article{glow_tts,
  title={Glow-tts: A generative flow for text-to-speech via monotonic alignment search},
  author={Kim, Jaehyeon and Kim, Sungwon and Kong, Jungil and Yoon, Sungroh},
  journal={Advances in Neural Information Processing Systems},
  volume={33},
  pages={8067--8077},
  year={2020}
}

@inproceedings{vits,
  title={Conditional variational autoencoder with adversarial learning for end-to-end text-to-speech},
  author={Kim, Jaehyeon and Kong, Jungil and Son, Juhee},
  booktitle={International Conference on Machine Learning},
  pages={5530--5540},
  year={2021},
  organization={PMLR}
}
@article{natural_speech,
  title={Naturalspeech: End-to-end text-to-speech synthesis with human-level quality},
  author={Tan, Xu and Chen, Jiawei and Liu, Haohe and Cong, Jian and Zhang, Chen and Liu, Yanqing and Wang, Xi and Leng, Yichong and Yi, Yuanhao and He, Lei and others},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2024},
  publisher={IEEE}
}

@article{natural_speech2,
  title={Naturalspeech 2: Latent diffusion models are natural and zero-shot speech and singing synthesizers},
  author={Shen, Kai and Ju, Zeqian and Tan, Xu and Liu, Yanqing and Leng, Yichong and He, Lei and Qin, Tao and Zhao, Sheng and Bian, Jiang},
  journal={arXiv preprint arXiv:2304.09116},
  year={2023}
}

@article{natural_speech3,
  title={Naturalspeech 3: Zero-shot speech synthesis with factorized codec and diffusion models},
  author={Ju, Zeqian and Wang, Yuancheng and Shen, Kai and Tan, Xu and Xin, Detai and Yang, Dongchao and Liu, Yanqing and Leng, Yichong and Song, Kaitao and Tang, Siliang and others},
  journal={arXiv preprint arXiv:2403.03100},
  year={2024}
}

@article{Low-Resource_tts,
  title={Text-to-speech for low-resource agglutinative language with morphology-aware language model pre-training},
  author={Liu, Rui and Hu, Yifan and Zuo, Haolin and Luo, Zhaojie and Wang, Longbiao and Gao, Guanglai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}

@article{valle,
  title={Neural codec language models are zero-shot text to speech synthesizers},
  author={Wang, Chengyi and Chen, Sanyuan and Wu, Yu and Zhang, Ziqiang and Zhou, Long and Liu, Shujie and Chen, Zhuo and Liu, Yanqing and Wang, Huaming and Li, Jinyu and others},
  journal={arXiv preprint arXiv:2301.02111},
  year={2023}
}

@inproceedings{prosody_tts,
  title={Prosody-tts: Improving prosody with masked autoencoder and conditional diffusion model for expressive text-to-speech},
  author={Huang, Rongjie and Zhang, Chunlei and Ren, Yi and Zhao, Zhou and Yu, Dong},
  booktitle={Findings of the Association for Computational Linguistics: ACL 2023},
  pages={8018--8034},
  year={2023}
}

@inproceedings{prodiff,
  title={Prodiff: Progressive fast diffusion model for high-quality text-to-speech},
  author={Huang, Rongjie and Zhao, Zhou and Liu, Huadai and Liu, Jinglin and Cui, Chenye and Ren, Yi},
  booktitle={Proceedings of the 30th ACM International Conference on Multimedia},
  pages={2595--2605},
  year={2022}
}

@article{paratts,
  title={Paratts: Learning linguistic and prosodic cross-sentence information in paragraph-based tts},
  author={Xue, Liumeng and Soong, Frank K and Zhang, Shaofei and Xie, Lei},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  volume={30},
  pages={2854--2864},
  year={2022},
  publisher={IEEE}
}

@article{oplustil2020using,
  title={Using previous acoustic context to improve text-to-speech synthesis},
  author={Oplustil-Gallegos, Pilar and King, Simon},
  journal={arXiv preprint arXiv:2012.03763},
  year={2020}
}

@inproceedings{liu2024emotion,
  title={Emotion rendering for conversational speech synthesis with heterogeneous graph-based context modeling},
  author={Liu, Rui and Hu, Yifan and Ren, Yi and Yin, Xiang and Li, Haizhou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={38},
  number={17},
  pages={18698--18706},
  year={2024}
}

@inproceedings{xu2021improving,
  title={Improving prosody modelling with cross-utterance bert embeddings for end-to-end speech synthesis},
  author={Xu, Guanghui and Song, Wei and Zhang, Zhengchen and Zhang, Chao and He, Xiaodong and Zhou, Bowen},
  booktitle={ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6079--6083},
  year={2021},
  organization={IEEE}
}
@inproceedings{cong21b_interspeech,
  author={Jian Cong and Shan Yang and Na Hu and Guangzhi Li and Lei Xie and Dan Su},
  title={{Controllable Context-Aware Conversational Speech Synthesis}},
  year=2021,
  booktitle={Proc. Interspeech 2021},
  pages={4658--4662},
  doi={10.21437/Interspeech.2021-412},
  issn={2308-457X}
}

@inproceedings{xiao23_interspeech,
  author={Yujia Xiao and Shaofei Zhang and Xi Wang and Xu Tan and Lei He and Sheng Zhao and Frank K. Soong and Tan Lee},
  title={{ContextSpeech: Expressive and Efficient Text-to-Speech for Paragraph Reading}},
  year=2023,
  booktitle={Proc. INTERSPEECH 2023},
  pages={4883--4887},
  doi={10.21437/Interspeech.2023-122},
  issn={2308-457X}
}

@inproceedings{MFA,
  author={Michael McAuliffe and Michaela Socolof and Sarah Mihuc and Michael Wagner and Morgan Sonderegger},
  title={{Montreal Forced Aligner: Trainable Text-Speech Alignment Using Kaldi}},
  year=2017,
  booktitle={Proc. Interspeech 2017},
  pages={498--502},
  doi={10.21437/Interspeech.2017-1386},
  issn={2308-457X}
}

@inproceedings{martinez2020lipreading,
  title={Lipreading using temporal convolutional networks},
  author={Martinez, Brais and Ma, Pingchuan and Petridis, Stavros and Pantic, Maja},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6319--6323},
  year={2020},
  organization={IEEE}
}

@inproceedings{s3fd,
  title={S3fd: Single shot scale-invariant face detector},
  author={Zhang, Shifeng and Zhu, Xiangyu and Lei, Zhen and Shi, Hailin and Wang, Xiaobo and Li, Stan Z},
  booktitle={Proceedings of the IEEE international conference on computer vision},
  pages={192--201},
  year={2017}
}

@article{emofan,
  title={Estimation of continuous valence and arousal levels from faces in naturalistic conditions},
  author={Toisoul, Antoine and Kossaifi, Jean and Bulat, Adrian and Tzimiropoulos, Georgios and Pantic, Maja},
  journal={Nature Machine Intelligence},
  volume={3},
  number={1},
  pages={42--50},
  year={2021},
  publisher={Nature Publishing Group UK London}
}


@article{attentionstitch,
  title={AttentionStitch: How Attention Solves the Speech Editing Problem},
  author={Alexos, Antonios and Baldi, Pierre},
  journal={arXiv preprint arXiv:2403.04804},
  year={2024}
}

@article{double_attention_block,
  title={A\^{} 2-nets: Double attention networks},
  author={Chen, Yunpeng and Kalantidis, Yannis and Li, Jianshu and Yan, Shuicheng and Feng, Jiashi},
  journal={Advances in neural information processing systems},
  volume={31},
  year={2018}
}

@article{hifihan,
  title={Hifi-gan: Generative adversarial networks for efficient and high fidelity speech synthesis},
  author={Kong, Jungil and Kim, Jaehyeon and Bae, Jaekyoung},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={17022--17033},
  year={2020}
}

@inproceedings{chemdataset,
  title={Learning individual speaking styles for accurate lip to speech synthesis},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={13796--13805},
  year={2020}
}

@article{GPE,
  title={A method for fundamental frequency estimation and voicing decision: Application to infant utterances recorded in real acoustical environments},
  author={Nakatani, Tomohiro and Amano, Shigeaki and Irino, Toshio and Ishizuka, Kentaro and Kondo, Tadahisa},
  journal={Speech Communication},
  volume={50},
  number={3},
  pages={203--214},
  year={2008},
  publisher={Elsevier}
}


@inproceedings{FFE,
  title={Reducing f0 frame error of f0 tracking algorithms under noisy conditions with an unvoiced/voiced classification frontend},
  author={Chu, Wei and Alwan, Abeer},
  booktitle={2009 IEEE International Conference on Acoustics, Speech and Signal Processing},
  pages={3969--3972},
  year={2009},
  organization={IEEE}
}

@inproceedings{syncnet,
  title={Out of time: automated lip sync in the wild},
  author={Chung, Joon Son and Zisserman, Andrew},
  booktitle={Computer Vision--ACCV 2016 Workshops: ACCV 2016 International Workshops, Taipei, Taiwan, November 20-24, 2016, Revised Selected Papers, Part II 13},
  pages={251--263},
  year={2017},
  organization={Springer}
}

@inproceedings{lipexpert,
  title={A lip sync expert is all you need for speech to lip generation in the wild},
  author={Prajwal, KR and Mukhopadhyay, Rudrabha and Namboodiri, Vinay P and Jawahar, CV},
  booktitle={Proceedings of the 28th ACM international conference on multimedia},
  pages={484--492},
  year={2020}
}


@inproceedings{mcd,
  title={Mel-cepstral distance measure for objective speech quality assessment},
  author={Kubichek, Robert},
  booktitle={Proceedings of IEEE pacific rim conference on communications computers and signal processing},
  volume={1},
  pages={125--128},
  year={1993},
  organization={IEEE}
}

@inproceedings{stoi,
  title={A short-time objective intelligibility measure for time-frequency weighted noisy speech},
  author={Taal, Cees H and Hendriks, Richard C and Heusdens, Richard and Jensen, Jesper},
  booktitle={2010 IEEE international conference on acoustics, speech and signal processing},
  pages={4214--4217},
  year={2010},
  organization={IEEE}
}

@inproceedings{PESQ,
  title={Perceptual evaluation of speech quality (PESQ)-a new method for speech quality assessment of telephone networks and codecs},
  author={Rix, Antony W and Beerends, John G and Hollier, Michael P and Hekstra, Andries P},
  booktitle={2001 IEEE international conference on acoustics, speech, and signal processing. Proceedings (Cat. No. 01CH37221)},
  volume={2},
  pages={749--752},
  year={2001},
  organization={IEEE}
}

@article{he2024multi,
  title={Multi-source spatial knowledge understanding for immersive visual text-to-speech},
  author={He, Shuwei and Liu, Rui and Li, Haizhou},
  journal={arXiv preprint arXiv:2410.14101},
  year={2024}
}

@article{liu2024multi,
  title={Multi-modal and Multi-scale Spatial Environment Understanding for Immersive Visual Text-to-Speech},
  author={Liu, Rui and He, Shuwei and Hu, Yifan and Li, Haizhou},
  journal={arXiv preprint arXiv:2412.11409},
  year={2024}
}

@article{liu2024emphasis,
  title={Emphasis Rendering for Conversational Text-to-Speech with Multi-modal Multi-scale Context Modeling},
  author={Liu, Rui and Jia, Zhenqi and Yang, Jie and Hu, Yifan and Li, Haizhou},
  journal={arXiv preprint arXiv:2410.09524},
  year={2024}
}





@inproceedings{cong2025emodubber,
  title={Emodubber: Towards high quality and emotion controllable movie dubbing},
  author={Cong, Gaoxiang and Pan, Jiadong and Li, Liang and Qi, Yuankai and Peng, Yuxin and van den Hengel, Anton and Yang, Jian and Huang, Qingming},
  booktitle={Proceedings of the Computer Vision and Pattern Recognition Conference},
  pages={15863--15873},
  year={2025}
}

@inproceedings{liu2024generative,
  title={Generative Expressive Conversational Speech Synthesis},
  author={Liu, Rui and Hu, Yifan and Ren, Yi and Yin, Xiang and Li, Haizhou},
  booktitle={Proceedings of the 32nd ACM International Conference on Multimedia},
  pages={4187--4196},
  year={2024}
}



@inproceedings{hu2024fctalker,
  title={Fctalker: Fine and coarse grained context modeling for expressive conversational speech synthesis},
  author={Hu, Yifan and Liu, Rui and Gao, Guanglai and Li, Haizhou},
  booktitle={2024 IEEE 14th International Symposium on Chinese Spoken Language Processing (ISCSLP)},
  pages={299--303},
  year={2024},
  organization={IEEE}
}

@article{liu2024text,
  title={Text-to-speech for low-resource agglutinative language with morphology-aware language model pre-training},
  author={Liu, Rui and Hu, Yifan and Zuo, Haolin and Luo, Zhaojie and Wang, Longbiao and Gao, Guanglai},
  journal={IEEE/ACM Transactions on Audio, Speech, and Language Processing},
  year={2024},
  publisher={IEEE}
}

@misc{zhang2025produbber,
      title={Prosody-Enhanced Acoustic Pre-training and Acoustic-Disentangled Prosody Adapting for Movie Dubbing}, 
      author={Zhedong Zhang and Liang Li and Chenggang Yan and Chunshan Liu and Anton van den Hengel and Yuankai Qi},
      year={2025},
      eprint={2503.12042},
      archivePrefix={arXiv},
      url={https://arxiv.org/abs/2503.12042}, 
}

@inproceedings{li2025fccondubber,
  title={FCConDubber: Fine And Coarse Grained Prosody Alignment For Expressive Video Dubbing via Contrastive Audio-Motion Pretraining},
  author={Li, Qiulin and Wu, Zhichao and Li, Hanwei and Dong, Xin and Yang, Qun},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}

@inproceedings{zhao2025m2ci_dubber,
  title={Towards Expressive Video Dubbing with Multiscale Multimodal Context Interaction},
  author={Zhao, Yuan and Liu, Rui and Cong, Gaoxiang},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}

@article{lewis2020retrieval,
  title={Retrieval-augmented generation for knowledge-intensive nlp tasks},
  author={Lewis, Patrick and Perez, Ethan and Piktus, Aleksandra and Petroni, Fabio and Karpukhin, Vladimir and Goyal, Naman and K{\"u}ttler, Heinrich and Lewis, Mike and Yih, Wen-tau and Rockt{\"a}schel, Tim and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={9459--9474},
  year={2020}
}

@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Haofen and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  volume={2},
  year={2023}
}

@inproceedings{guu2020retrieval,
  title={Retrieval augmented language model pre-training},
  author={Guu, Kelvin and Lee, Kenton and Tung, Zora and Pasupat, Panupong and Chang, Mingwei},
  booktitle={International conference on machine learning},
  pages={3929--3938},
  year={2020},
  organization={PMLR}
}

@inproceedings{kang2024retrieval,
  title={Retrieval-augmented audio deepfake detection},
  author={Kang, Zuheng and He, Yayun and Zhao, Botao and Qu, Xiaoyang and Peng, Junqing and Xiao, Jing and Wang, Jianzong},
  booktitle={Proceedings of the 2024 International Conference on Multimedia Retrieval},
  pages={376--384},
  year={2024}
}

@inproceedings{yuan2024retrieval,
  title={Retrieval-augmented text-to-audio generation},
  author={Yuan, Yi and Liu, Haohe and Liu, Xubo and Huang, Qiushi and Plumbley, Mark D and Wang, Wenwu},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={581--585},
  year={2024},
  organization={IEEE}
}

@inproceedings{yang2024rag,
  title={Im-rag: Multi-round retrieval-augmented generation through learning inner monologues},
  author={Yang, Diji and Rao, Jinmeng and Chen, Kezhen and Guo, Xiaoyuan and Zhang, Yawen and Yang, Jie and Zhang, Yi},
  booktitle={Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval},
  pages={730--740},
  year={2024}
}

@inproceedings{ghosh2024recap,
  title={Recap: Retrieval-augmented audio captioning},
  author={Ghosh, Sreyan and Kumar, Sonal and Evuru, Chandra Kiran Reddy and Duraiswami, Ramani and Manocha, Dinesh},
  booktitle={ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1161--1165},
  year={2024},
  organization={IEEE}
}

@article{zen2019libritts,
  title={Libritts: A corpus derived from librispeech for text-to-speech},
  author={Zen, Heiga and Dang, Viet and Clark, Rob and Zhang, Yu and Weiss, Ron J and Jia, Ye and Chen, Zhifeng and Wu, Yonghui},
  journal={arXiv preprint arXiv:1904.02882},
  year={2019}
}

@article{chen2022RE_imagen,
  title={Re-imagen: Retrieval-augmented text-to-image generator},
  author={Chen, Wenhu and Hu, Hexiang and Saharia, Chitwan and Cohen, William W},
  journal={arXiv preprint arXiv:2209.14491},
  year={2022}
}

@inproceedings{kim2024you,
  title={Do you remember? dense video captioning with cross-modal memory retrieval},
  author={Kim, Minkuk and Kim, Hyeon Bae and Moon, Jinyoung and Choi, Jinwoo and Kim, Seong Tae},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={13894--13904},
  year={2024}
}

@article{sanchez2021gentle,
  title={A gentle introduction to graph neural networks},
  author={Sanchez-Lengeling, Benjamin and Reif, Emily and Pearce, Adam and Wiltschko, Alexander B},
  journal={Distill},
  volume={6},
  number={9},
  pages={e33},
  year={2021}
}

@article{hamilton2017representation,
  title={Representation learning on graphs: Methods and applications},
  author={Hamilton, William L and Ying, Rex and Leskovec, Jure},
  journal={arXiv preprint arXiv:1709.05584},
  year={2017}
}

@book{hamilton2020graph,
  title={Graph representation learning},
  author={Hamilton, William L},
  year={2020},
  publisher={Morgan \& Claypool Publishers}
}

@article{peng2022control,
  title={Control globally, understand locally: A global-to-local hierarchical graph network for emotional support conversation},
  author={Peng, Wei and Hu, Yue and Xing, Luxi and Xie, Yuqiang and Sun, Yajing and Li, Yunpeng},
  journal={arXiv preprint arXiv:2204.12749},
  year={2022}
}

@inproceedings{lijingbei_dialoguegnn_gcn,
  title={Enhancing speaking styles in conversational text-to-speech synthesis with graph-based multi-modal context modeling},
  author={Li, Jingbei and Meng, Yi and Li, Chenyi and Wu, Zhiyong and Meng, Helen and Weng, Chao and Su, Dan},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={7917--7921},
  year={2022},
  organization={IEEE}
}

@article{cheng2024videollama2,
  title={Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms},
  author={Cheng, Zesen and Leng, Sicong and Zhang, Hang and Xin, Yifei and Li, Xin and Chen, Guanzheng and Zhu, Yongxin and Zhang, Wenqi and Luo, Ziyang and Zhao, Deli and others},
  journal={arXiv preprint arXiv:2406.07476},
  year={2024}
}

@inproceedings{deng2023cmcu,
  title={Cmcu-css: Enhancing naturalness via commonsense-based multi-modal context understanding in conversational speech synthesis},
  author={Deng, Yayue and Xue, Jinlong and Wang, Fengping and Gao, Yingming and Li, Ya},
  booktitle={Proceedings of the 31st ACM International Conference on Multimedia},
  pages={6081--6089},
  year={2023}
}

@article{ma2023emotion2vec,
  title={emotion2vec: Self-supervised pre-training for speech emotion representation},
  author={Ma, Ziyang and Zheng, Zhisheng and Ye, Jiaxin and Li, Jinchao and Gao, Zhifu and Zhang, Shiliang and Chen, Xie},
  journal={arXiv preprint arXiv:2312.15185},
  year={2023}
}

@inproceedings{TIM_NER_SER,
  title={Temporal modeling matters: A novel temporal emotional modeling approach for speech emotion recognition},
  author={Ye, Jiaxin and Wen, Xin-Cheng and Wei, Yujie and Xu, Yong and Liu, Kunhong and Shan, Hongming},
  booktitle={ICASSP 2023-2023 IEEE international conference on acoustics, speech and signal processing (ICASSP)},
  pages={1--5},
  year={2023},
  organization={IEEE}
}

@inproceedings{I3D_model,
  title={Quo vadis, action recognition? a new model and the kinetics dataset},
  author={Carreira, Joao and Zisserman, Andrew},
  booktitle={proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
  pages={6299--6308},
  year={2017}
}

@article{flowdubber,
  title={FlowDubber: Movie Dubbing with LLM-based Semantic-aware Learning and Flow Matching based Voice Enhancing},
  author={Cong, Gaoxiang and Li, Liang and Pan, Jiadong and Zhang, Zhedong and Beheshti, Amin and Hengel, Anton van den and Qi, Yuankai and Huang, Qingming},
  journal={arXiv preprint arXiv:2505.01263},
  year={2025}
}

@article{MM-MovieDubber,
  title={MM-MovieDubber: Towards Multi-Modal Learning for Multi-Modal Movie Dubbing},
  author={Zheng, Junjie and Chen, Zihao and Ding, Chaofan and Liang, Yunming and Fan, Yihan and Yang, Huan and Xie, Lei and Di, Xinhan},
  journal={arXiv preprint arXiv:2505.16279},
  year={2025}
}

@article{voicecraft_dubber,
  title={VoiceCraft-Dub: Automated Video Dubbing with Neural Codec Language Models},
  author={Sung-Bin, Kim and Choi, Jeongsoo and Peng, Puyuan and Chung, Joon Son and Oh, Tae-Hyun and Harwath, David},
  journal={arXiv preprint arXiv:2504.02386},
  year={2025}
}

@inproceedings{MCD_DTW,
  title={Location-relative attention mechanisms for robust long-form speech synthesis},
  author={Battenberg, Eric and Skerry-Ryan, RJ and Mariooryad, Soroosh and Stanton, Daisy and Kao, David and Shannon, Matt and Bagby, Tom},
  booktitle={ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={6194--6198},
  year={2020},
  organization={IEEE}
}

@article{RADKA,
  title={Retrieval-Augmented Dialogue Knowledge Aggregation for expressive conversational speech synthesis},
  author={Liu, Rui and Jia, Zhenqi and Bao, Feilong and Li, Haizhou},
  journal={Information Fusion},
  volume={118},
  pages={102948},
  year={2025},
  publisher={Elsevier}
}

@article{AIGC,
  title={A survey of ai-generated content (aigc)},
  author={Cao, Yihan and Li, Siyu and Liu, Yixin and Yan, Zhiling and Dai, Yutong and Yu, Philip and Sun, Lichao},
  journal={ACM Computing Surveys},
  volume={57},
  number={5},
  pages={1--38},
  year={2025},
  publisher={ACM New York, NY}
}
@article{alidit,
  title={AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation},
  author={Choi, Jeongsoo and Kim, Ji-Hoon and Sung-Bin, Kim and Oh, Tae-Hyun and Chung, Joon Son},
  journal={arXiv preprint arXiv:2504.20629},
  year={2025}
}

@inproceedings{Multimodal-jia,
  title={Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis},
  author={Jia, Zhenqi and Liu, Rui and Sisman, Berrak and Li, Haizhou},
  booktitle={Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing},
  pages={8863--8869},
  year={2025}
}

@inproceedings{hu2025unitalker,
  title={UniTalker: Conversational Speech-Visual Synthesis},
  author={Hu, Yifan and Liu, Rui and Ren, Yi and Yin, Xiang and Li, Haizhou},
  booktitle={Proceedings of the 33rd ACM International Conference on Multimedia},
  pages={10248--10257},
  year={2025}
}

@inproceedings{hu-etal-2025-chain,
    title = "Chain-Talker: Chain Understanding and Rendering for Empathetic Conversational Speech Synthesis",
    author = "Hu, Yifan  and
      Liu, Rui  and
      Ren, Yi  and
      Yin, Xiang  and
      Li, Haizhou",
    editor = "Che, Wanxiang  and
      Nabende, Joyce  and
      Shutova, Ekaterina  and
      Pilehvar, Mohammad Taher",
    booktitle = "Findings of the Association for Computational Linguistics: ACL 2025",
    month = jul,
    year = "2025",
    address = "Vienna, Austria",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2025.findings-acl.101/",
    doi = "10.18653/v1/2025.findings-acl.101",
    pages = "1988--2003",
    ISBN = "979-8-89176-256-5"
}

@inproceedings{liu2025multi,
  title={Multi-modal and multi-scale spatial environment understanding for immersive visual text-to-speech},
  author={Liu, Rui and He, Shuwei and Hu, Yifan and Li, Haizhou},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={39},
  number={23},
  pages={24632--24640},
  year={2025}
}

@inproceedings{he2025multi-icassp,
  title={Multi-source spatial knowledge understanding for immersive visual text-to-speech},
  author={He, Shuwei and Liu, Rui},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}

@inproceedings{jia2025intra,
  title={Intra-and inter-modal context interaction modeling for conversational speech synthesis},
  author={Jia, Zhenqi and Liu, Rui},
  booktitle={ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={1--5},
  year={2025},
  organization={IEEE}
}

@article{li2025dubbing,
  title={Dubbing Movies via Hierarchical Phoneme Modeling and Acoustic Diffusion Denoising},
  author={Li, Liang and Cong, Gaoxiang and Qi, Yuankai and Zha, Zheng-Jun and Wu, Qi and Sheng, Quan Z and Huang, Qingming and Yang, Ming-Hsuan},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
  year={2025},
  publisher={IEEE}
}

@inproceedings{lu2022visualtts,
  title={Visualtts: Tts with accurate lip-speech synchronization for automatic voice over},
  author={Lu, Junchen and Sisman, Berrak and Liu, Rui and Zhang, Mingyang and Li, Haizhou},
  booktitle={ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages={8032--8036},
  year={2022},
  organization={IEEE}
}

