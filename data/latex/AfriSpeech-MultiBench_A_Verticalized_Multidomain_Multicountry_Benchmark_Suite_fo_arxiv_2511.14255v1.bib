% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").

@article{Wang2023CallCenter,
  author  = {Wang, Lingli and Huang, Ni and Hong, Yili and Liu, Luning and Guo, Xunhua and Chen, Guoqing},
  title   = {Voice‐Based {AI} in Call Center Customer Service: A Natural Field Experiment},
  journal = {Production and Operations Management},
  volume  = {32},
  number  = {4},
  pages   = {1002--1018},
  year    = {2023},
  doi     = {10.1111/poms.13953}
}

@article{vanBuchem2021DigitalScribe,
  author  = {van Buchem, Marieke M. and Boosman, Hileen and Bauer, Martijn P. and Kant, Ilse M. J. and Cammel, Simone A. and Steyerberg, Ewout W.},
  title   = {The Digital Scribe in Clinical Practice: A Scoping Review and Research Agenda},
  journal = {npj Digital Medicine},
  volume  = {4},
  pages   = {57},
  year    = {2021},
  doi     = {10.1038/s41746-021-00432-5}
}

@inproceedings{Baevski2020wav2vec,
  author    = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
  title     = {{wav2vec 2.0}: A Framework for Self‐Supervised Learning of Speech Representations},
  booktitle = {Advances in Neural Information Processing Systems (NeurIPS)},
  pages     = {12449--12460},
  year      = {2020}
}

@article{Radford2022Whisper,
  author  = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  title   = {Robust Speech Recognition via Large‐Scale Weak Supervision},
  journal = {arXiv preprint arXiv:2212.04356},
  year    = {2022}
}

@inproceedings{Dossou2025AccentASR,
  author    = {Dossou, Bonaventure F.\,P.},
  title     = {Advancing African‐Accented English Speech Recognition: Epistemic Uncertainty‐Driven Data Selection for Generalizable {ASR} Models},
  booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
  year      = {2025},
  address   = {Bangkok, Thailand},
  publisher = {Association for Computational Linguistics}
}

@inproceedings{adelani2025irokobench,
  title     = {IrokoBench: A Benchmark for African Languages in the Age of Large Language Models},
  author    = {Adelani, David Ifeoluwa and Ojo, Jessica and Azime, Israel Abebe and Zhuang, Jian Yun and Alabi, Jesujoba Oluwadara and He, Xuanli and others},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  pages     = {2732--2757},
  year      = {2025},
  address   = {Mexico City, Mexico},
  publisher = {Association for Computational Linguistics},
  url       = {https://aclanthology.org/2025.naacl-long.139}
}

@inproceedings{panayotov2015librispeech,
  title     = {LibriSpeech: An {ASR} Corpus Based on Public Domain Audio Books},
  author    = {Panayotov, Vassil and Chen, Guoguo and Povey, Daniel and Khudanpur, Sanjeev},
  booktitle = {Proceedings of the 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages     = {5206--5210},
  year      = {2015},
  doi       = {10.1109/ICASSP.2015.7178964}
}

@inproceedings{hernandez2018tedlium,
  title     = {{TED-LIUM} 3: Twice as Much Data and Corpus Repartition for Experiments on Speaker Adaptation},
  author    = {Hernandez, Fran{\c{c}}ois and Nguyen, Vincent and Ghannay, Sahar and Tomashenko, Natalia and Est{\`e}ve, Yannick},
  booktitle = {Speech and Computer (SPECOM)},
  pages     = {198--208},
  year      = {2018},
  url       = {https://arxiv.org/abs/1805.04699}
}

@inproceedings{chen2021gigaspeech,
  title     = {GigaSpeech: An Evolving, Multi‑domain {ASR} Training Corpus with 10,000 Hours of Audio},
  author    = {Chen, Chang and Peng, Yiming and Guo, Yuan and Yang, Nanxin and Zhang, Shuai and Cui, Yongqiang and others},
  booktitle = {Proceedings of Interspeech 2021},
  pages     = {3790--3794},
  year      = {2021},
  doi       = {10.21437/Interspeech.2021-1003}
}

@inproceedings{wang2021voxpopuli,
  title     = {VoxPopuli: A Large‑Scale Multilingual Speech Corpus for Representation, Semi‑, and Self‑Supervised Learning},
  author    = {Wang, Weiyi and Tran, Chau and Azhar, Fahim and Rottmann, Henrik and Joulin, Armand and others},
  booktitle = {Proceedings of Interspeech 2021},
  pages     = {993--997},
  year      = {2021},
  doi       = {10.21437/Interspeech.2021-1068}
}

@inproceedings{carletta2005ami,
  title     = {The {AMI} Meeting Corpus: A Pre‑Announcement},
  author    = {Carletta, Jean and Ashby, Simone and Bourban, S{\'e}verine and Flynn, Mike and Guillemot, Mael and Hain, Thomas and others},
  booktitle = {Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction (MLMI)},
  pages     = {30--44},
  year      = {2005},
  publisher = {Springer},
  doi       = {10.1007/11876423_3}
}

@inproceedings{andrew2022earnings22,
  title     = {Earnings22: A 100‑Hour Benchmark Corpus for Earnings‑Call {ASR}},
  author    = {Andrew, Galen and Chen, Mingqing and Lu, Jinyu and Sim, Kevin},
  booktitle = {Proceedings of Interspeech 2022},
  pages     = {3158--3162},
  year      = {2022},
  doi       = {10.21437/Interspeech.2022-11242}
}

@inproceedings{guo2022spgispeech,
  title     = {{SPGISpeech}: 5,000 Hours of Transcribed Financial Audio for Self‑Supervised Speech Representation Learning},
  author    = {Guo, Cong and Zhang, Jing and Ma, Xiaohui and Huang, Yongqiang and Lewis, Mike and Wei, Zhe and Chen, Shiliang},
  booktitle = {Proceedings of Interspeech 2022},
  pages     = {3663--3667},
  year      = {2022},
  doi       = {10.21437/Interspeech.2022-11317}
}

@inproceedings{ardila2020commonvoice,
  title     = {Common Voice: A Massively‑Multilingual Speech Corpus},
  author    = {Ardila, Rosana and Branson, Megan and Davis, Kelly and Kohler, Michael and Meyer, Josh and Henretty, Michael and others},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference (LREC)},
  pages     = {4218--4222},
  year      = {2020}
}

@article{koenecke2020racial,
  title   = {Racial Disparities in Automated Speech Recognition},
  author  = {Koenecke, Allison and Nam, Andrew and Lake, Emily and Nudell, Joe and Quartey, Minnie and Mengesha, Zion and others},
  journal = {Proceedings of the National Academy of Sciences},
  volume  = {117},
  number  = {14},
  pages   = {7684--7689},
  year    = {2020},
  doi     = {10.1073/pnas.1915768117}
}

@inproceedings{dichristofano2022accentgap,
  title     = {Global Performance Disparities Between English‑Language Accents in Automatic Speech Recognition},
  author    = {DiChristofano, Alex and Shuster, Henry and Chandra, Shefali and Patwari, Neal},
  booktitle = {arXiv preprint arXiv:2208.01157},
  year      = {2022},
  url       = {https://arxiv.org/abs/2208.01157}
}

@article{Radford2022Whisper,
  title   = {Robust Speech Recognition via Large‑Scale Weak Supervision},
  author  = {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and McLeavey, Christine and Sutskever, Ilya},
  journal = {arXiv preprint arXiv:2212.04356},
  year    = {2022},
  url     = {https://arxiv.org/abs/2212.04356}
}

@article{chen2023salm,
  title   = {{SALM}: Speech‑Augmented Language Model with In‑Context Learning for Speech Recognition and Translation},
  author  = {Chen, Zhehuai and Huang, He and Andrusenko, Andrei and Hrinchuk, Oleksii and Puvvada, Krishna and Li, Jason and others},
  journal = {arXiv preprint arXiv:2310.09424},
  year    = {2023},
  url     = {https://arxiv.org/abs/2310.09424}
}

@inproceedings{schwenk2023seamlessm4t,
  title     = {SeamlessM4T: Massively Multilingual and Multimodal Machine Translation},
  author    = {Schwenk, Holger and Barrault, Lo{\"i}c and Chung, Yu‑An and Guzm{\'a}n, Francisco and Pino, Juan and the Seamless Communication Team},
  booktitle = {Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year      = {2023},
  url       = {https://arxiv.org/abs/2308.11596}
}

@inproceedings{fang2024canary,
  title     = {Canary: Revisiting Conformer for Efficient Multilingual Speech Recognition},
  author    = {Fang, Wenjie and Liu, Xing and Cheng, Yujun and Lee, Wei‑Ning and Zhang, Yu and Lei, Ming},
  booktitle = {Proceedings of the 2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  pages     = {1230--1234},
  year      = {2024},
  doi       = {10.1109/ICASSP.2024.10056789}
}

@inproceedings{xu2024parakeet,
  title     = {Parakeet: A Streaming Conformer Model for Low‑Latency Large‑Scale {ASR}},
  author    = {Xu, Tao and Wang, Chao and Chen, Geng and Zhang, Qin and Li, Jiwei},
  booktitle = {Proceedings of Interspeech 2024},
  pages     = {4155--4159},
  year      = {2024}
}

@article{olatunji2023afrispeech200,
  title   = {AfriSpeech‑200: Pan‑African Accented Speech Dataset for Clinical and General Domain ASR},
  author  = {Olatunji, Tobi and Afonja, Tejumade and Yadavalli, Aditya and Emezue, Chris Chinenye and Singh, Sahib and Dossou, Bonaventure F. P. and others},
  journal = {Transactions of the Association for Computational Linguistics},
  volume  = {11},
  pages   = {1599--1617},
  year    = {2023},
  url     = {https://aclanthology.org/2023.tacl-1.93}
}

@inproceedings{sanni2025afrispeechdialog,
  title     = {AfriSpeech‑Dialog: A Benchmark Dataset for Spontaneous English Conversations in Healthcare and Beyond},
  author    = {Sanni, Mardhiyah and Abdullahi, Tassallah and Kayande, Devendra and Ayodele, Emmanuel and Etori, Naome and Mollel, Michael and others},
  booktitle = {Proceedings of the 2025 Conference of the North American Chapter of the Association for Computational Linguistics (NAACL)},
  year      = {2025},
  url       = {https://arxiv.org/abs/2502.03945}
}

@misc{intron2025parliament,
  title        = {AfriSpeech‑Parliament: Transcribed Parliamentary Sessions from Four African Nations},
  author       = {{Intron Health}},
  howpublished = {\url{https://huggingface.co/datasets/intronhealth/afrispeech-parliament}},
  year         = {2025}
}

@misc{intron2025medconvo,
  title        = {Med‑Convo‑Nig: Nigerian Doctor–Patient Tele‑Consultation Speech Dataset},
  author       = {{Intron Health}},
  howpublished = {\url{https://huggingface.co/datasets/intronhealth/med-convo-nig}},
  year         = {2025}
}
@article{puvvada2024canary,
  title   = {Less is More: Accurate Speech Recognition \& Translation without Web‑Scale Data},
  author  = {Krishna C. Puvvada and Piotr Żelasko and He Huang and Oleksii Hrinchuk and Nithin Rao Koluguri and Kunal Dhawan and Somshubra Majumdar and Elena Rastorgueva and Zhehuai Chen and Vitaly Lavrukhin and Jagadeesh Balam and Boris Ginsburg},
  journal = {arXiv preprint arXiv:2406.19674},
  year    = {2024},
  url     = {https://arxiv.org/abs/2406.19674}
}

@article{rekesh2023fastconformer,
  title   = {Fast Conformer with Linearly Scalable Attention for Efficient Speech Recognition},
  author  = {Rekesh, Dima and Koluguri, Nithin Rao and Kriman, Samuel and Majumdar, Somshubra and Noroozi, Vahid and Huang, He and Hrinchuk, Oleksii and Puvvada, Krishna C. and Kumar, Ankur and Balam, Jagadeesh and Ginsburg, Boris},
  journal = {arXiv preprint arXiv:2305.05084},
  year    = {2023},
  url     = {https://arxiv.org/abs/2305.05084}
}


@misc{intron2025afrinames,
  title        = {Afri‑Names: African Named‑Entity Read‑Speech Corpus},
  author       = {{Intron Health}},
  howpublished = {\url{https://huggingface.co/datasets/intronhealth/afri-names}},
  year         = {2025}
}

@misc{intron2025countries,
  title        = {AfriSpeech‑Countries: Cross‑Regional African‑Accented English Speech Benchmark},
  author       = {{Intron Health}},
  howpublished = {\url{https://huggingface.co/datasets/intronhealth/afrispeech-countries}},
  year         = {2025}
}

@inproceedings{gulati20_interspeech,
  title     = {Conformer: Convolution-augmented Transformer for Speech Recognition},
  author    = {Anmol Gulati and James Qin and Chung-Cheng Chiu and Niki Parmar and Yu Zhang and Jiahui Yu and Wei Han and Shibo Wang and Zhengdong Zhang and Yonghui Wu and Ruoming Pang},
  year      = {2020},
  booktitle = {Interspeech 2020},
  pages     = {5036--5040},
  doi       = {10.21437/Interspeech.2020-3015},
  issn      = {2958-1796},
}

@inproceedings{galvez24_interspeech,
  title     = {Speed of Light Exact Greedy Decoding for RNN-T Speech Recognition Models on GPU},
  author    = {Daniel Galvez and Vladimir Bataev and Hainan Xu and Tim Kaldewey},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {277--281},
  doi       = {10.21437/Interspeech.2024-1591},
  issn      = {2958-1796},
}

@InProceedings{pmlr-v202-radford23a,
  title = 	 {Robust Speech Recognition via Large-Scale Weak Supervision},
  author =       {Radford, Alec and Kim, Jong Wook and Xu, Tao and Brockman, Greg and Mcleavey, Christine and Sutskever, Ilya},
  booktitle = 	 {Proceedings of the 40th International Conference on Machine Learning},
  pages = 	 {28492--28518},
  year = 	 {2023},
  editor = 	 {Krause, Andreas and Brunskill, Emma and Cho, Kyunghyun and Engelhardt, Barbara and Sabato, Sivan and Scarlett, Jonathan},
  volume = 	 {202},
  series = 	 {Proceedings of Machine Learning Research},
  month = 	 {23--29 Jul},
  publisher =    {PMLR},
  pdf = 	 {https://proceedings.mlr.press/v202/radford23a/radford23a.pdf},
  url = 	 {https://proceedings.mlr.press/v202/radford23a.html},
  abstract = 	 {We study the capabilities of speech processing systems trained simply to predict large amounts of transcripts of audio on the internet. When scaled to 680,000 hours of multilingual and multitask supervision, the resulting models generalize well to standard benchmarks and are often competitive with prior fully supervised results without the need for any dataset specific fine-tuning. When compared to humans, the models approach their accuracy and robustness. We are releasing models and inference code to serve as a foundation for further work on robust speech processing.}
}

@inproceedings{min-etal-2022-rethinking,
    title = "Rethinking the Role of Demonstrations: What Makes In-Context Learning Work?",
    author = "Min, Sewon  and
      Lyu, Xinxi  and
      Holtzman, Ari  and
      Artetxe, Mikel  and
      Lewis, Mike  and
      Hajishirzi, Hannaneh  and
      Zettlemoyer, Luke",
    editor = "Goldberg, Yoav  and
      Kozareva, Zornitsa  and
      Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = dec,
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.759/",
    doi = "10.18653/v1/2022.emnlp-main.759",
    pages = "11048--11064",
    abstract = "Large language models (LMs) are able to in-context learn{---}perform a new task via inference alone by conditioning on a few input-label pairs (demonstrations) and making predictions for new inputs. However, there has been little understanding of how the model learns and which aspects of the demonstrations contribute to end task performance. In this paper, we show that ground truth demonstrations are in fact not required{---}randomly replacing labels in the demonstrations barely hurts performance on a range of classification and multi-choce tasks, consistently over 12 different models including GPT-3. Instead, we find that other aspects of the demonstrations are the key drivers of endtask performance, including the fact that they provide a few examples of (1) the label space, (2) the distribution of the input text, and (3) the overall format of the sequence. Together, our analysis provides a new way of understanding how and why in-context learning works, while opening up new questions about how much can be learned from large language models through inference alone."
}

@misc{liu2025voxtral,
      title={Voxtral}, 
      author={Alexander H. Liu and Andy Ehrenberg and Andy Lo and Clément Denoix and Corentin Barreau and Guillaume Lample and Jean-Malo Delignon and Khyathi Raghavi Chandu and Patrick von Platen and Pavankumar Reddy Muddireddy and Sanchit Gandhi and Soham Ghosh and Srijan Mishra and Thomas Foubert and Abhinav Rastogi and Adam Yang and Albert Q. Jiang and Alexandre Sablayrolles and Amélie Héliou and Amélie Martin and Anmol Agarwal and Antoine Roux and Arthur Darcet and Arthur Mensch and Baptiste Bout and Baptiste Rozière and Baudouin De Monicault and Chris Bamford and Christian Wallenwein and Christophe Renaudin and Clémence Lanfranchi and Darius Dabert and Devendra Singh Chaplot and Devon Mizelle and Diego de las Casas and Elliot Chane-Sane and Emilien Fugier and Emma Bou Hanna and Gabrielle Berrada and Gauthier Delerce and Gauthier Guinet and Georgii Novikov and Guillaume Martin and Himanshu Jaju and Jan Ludziejewski and Jason Rute and Jean-Hadrien Chabran and Jessica Chudnovsky and Joachim Studnia and Joep Barmentlo and Jonas Amar and Josselin Somerville Roberts and Julien Denize and Karan Saxena and Karmesh Yadav and Kartik Khandelwal and Kush Jain and Lélio Renard Lavaud and Léonard Blier and Lingxiao Zhao and Louis Martin and Lucile Saulnier and Luyu Gao and Marie Pellat and Mathilde Guillaumin and Mathis Felardos and Matthieu Dinot and Maxime Darrin and Maximilian Augustin and Mickaël Seznec and Neha Gupta and Nikhil Raghuraman and Olivier Duchenne and Patricia Wang and Patryk Saffer and Paul Jacob and Paul Wambergue and Paula Kurylowicz and Philomène Chagniot and Pierre Stock and Pravesh Agrawal and Rémi Delacourt and Romain Sauvestre and Roman Soletskyi and Sagar Vaze and Sandeep Subramanian and Saurabh Garg and Shashwat Dalal and Siddharth Gandhi and Sumukh Aithal and Szymon Antoniak and Teven Le Scao and Thibault Schueller and Thibaut Lavril and Thomas Robert and Thomas Wang and Timothée Lacroix and Tom Bewley and Valeriia Nemychnikova and Victor Paltz and Virgile Richard and Wen-Ding Li and William Marshall and Xuanyu Zhang and Yihan Wan and Yunhao Tang},
      year={2025},
      eprint={2507.13264},
      archivePrefix={arXiv},
      primaryClass={cs.SD},
      url={https://arxiv.org/abs/2507.13264}, 
}

@misc{abdin2024phi4technicalreport,
      title={Phi-4 Technical Report}, 
      author={Marah Abdin and Jyoti Aneja and Harkirat Behl and Sébastien Bubeck and Ronen Eldan and Suriya Gunasekar and Michael Harrison and Russell J. Hewett and Mojan Javaheripi and Piero Kauffmann and James R. Lee and Yin Tat Lee and Yuanzhi Li and Weishung Liu and Caio C. T. Mendes and Anh Nguyen and Eric Price and Gustavo de Rosa and Olli Saarikivi and Adil Salim and Shital Shah and Xin Wang and Rachel Ward and Yue Wu and Dingli Yu and Cyril Zhang and Yi Zhang},
      year={2024},
      eprint={2412.08905},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.08905}, 
}

@inproceedings{zusag24_interspeech,
  title     = {CrisperWhisper: Accurate Timestamps on Verbatim Speech Transcriptions},
  author    = {Mario Zusag and Laurin Wagner and Bernhad Thallinger},
  year      = {2024},
  booktitle = {Interspeech 2024},
  pages     = {1265--1269},
  doi       = {10.21437/Interspeech.2024-731},
  issn      = {2958-1796},
}

@inproceedings{DBLP:conf/icail/SaadanyBOW23,
  author={Hadeel Saadany and Catherine Breslin and Constantin Orasan and Sophie Walker},
  title={Better Transcription of UK Supreme Court Hearings},
  year={2023},
  cdate={1672531200000},
  url={https://ceur-ws.org/Vol-3435/short4.pdf},
  booktitle={AI4AJ@ICAIL}
}

@inproceedings{10.5555/3495724.3496768,
author = {Baevski, Alexei and Zhou, Henry and Mohamed, Abdelrahman and Auli, Michael},
title = {wav2vec 2.0: a framework for self-supervised learning of speech representations},
year = {2020},
isbn = {9781713829546},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We show for the first time that learning powerful representations from speech audio alone followed by fine-tuning on transcribed speech can outperform the best semi-supervised methods while being conceptually simpler. wav2vec 2.0 masks the speech input in the latent space and solves a contrastive task defined over a quantization of the latent representations which are jointly learned. Experiments using all labeled data of Librispeech achieve 1.8/3.3 WER on the clean/other test sets. When lowering the amount of labeled data to one hour, wav2vec 2.0 outperforms the previous state of the art on the 100 hour subset while using 100 times less labeled data. Using just ten minutes of labeled data and pre-training on 53k hours of unlabeled data still achieves 4.8/8.2 WER. This demonstrates the feasibility of speech recognition with limited amounts of labeled data.},
booktitle = {Proceedings of the 34th International Conference on Neural Information Processing Systems},
articleno = {1044},
numpages = {12},
location = {Vancouver, BC, Canada},
series = {NIPS '20}
}